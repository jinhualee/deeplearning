{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 15 自然语言处理：应用\n",
    "- **目录**\n",
    "  - 15.1 情感分析及数据集\n",
    "  - 15.2 情感分析：使用递归神经网络\n",
    "  - 15.3 情感分析：使用卷积神经网络\n",
    "  - 15.4 自然语言推断与数据集\n",
    "  - 15.5 自然语言推断：使用注意力\n",
    "  - 15.6 针对序列级和词元级应用程序微调BERT\n",
    "  - 15.7 自然语言推断：微调BERT\n",
    "<br><br>\n",
    "<center><img src='../img/nlp-map-app.svg'></center>\n",
    "<center>图15.1 预训练文本表示可以通过不同模型架构，放入不同的下游自然语言处理应用（本章重点介绍如何为不同的下游应用设计模型）</center><br>\n",
    "<br><br>\n",
    "- **要点：**\n",
    "  - 本章的重点是如何应用深度语言表征学习来解决自然语言处理问题。\n",
    "  - 之前的章节提及了一些没有预训练的自然语言处理应用，如基于循环神经网络的语言模型以及基于循环神经网络和注意力机制的机器翻译模型。\n",
    "  - 本章将重点介绍两种下游自然语言处理任务：情感分析和自然语言推断。\n",
    "  - 本章将使用不同的深度学习架构，如多层感知机、卷积神经网络、循环神经网络和注意力机制来设计自然语言处理模型。\n",
    "  - 针对情感分析，将探索基于循环神经网络和卷积神经网络的模型设计。\n",
    "  - 对于自然语言推断，将使用注意力机制和多层感知机来分析文本对。\n",
    "  - 介绍如何对预训练的BERT模型进行微调，以适应各种自然语言处理应用，包括单文本分类、文本对分类、文本标注和问答。\n",
    "  - BERT为各种自然语言处理应用提供了很少的架构更改，但需要微调大量的BERT参数。当有限的资源时，其他模型架构可能更具可行性。\n",
    "  - 本章的后续部分将从情感分析开始，探讨基于循环神经网络和卷积神经网络的模型设计。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
