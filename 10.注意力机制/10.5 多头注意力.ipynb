{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 10.5 多头注意力\n",
    "- **目录**\n",
    "  - 10.5.1 多头注意力模型\n",
    "  - 10.5.2 多头注意力实现\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "在实践中，当给定相同的查询、键和值的集合时，\n",
    "我们希望模型可以**基于相同的注意力机制学习到不同的行为**，\n",
    "然后将不同的行为作为知识组合起来，\n",
    "**捕获序列内各种范围的依赖关系**\n",
    "（例如，短距离依赖和长距离依赖关系）。\n",
    "因此，允许**注意力机制组合使用查询、键和值的不同\n",
    "子空间表示（representation subspaces）可能是有益的。**\n",
    "\n",
    "为此，与其只使用单独一个注意力池化，\n",
    "我们可以用独立学习得到的$h$组不同的\n",
    "**线性投影（linear projections）**来变换查询、键和值。\n",
    "然后，这$h$组变换后的查询、键和值将并行地送到注意力池化中。\n",
    "最后，将这$h$个注意力池化的输出拼接在一起，\n",
    "并且通过另一个可以学习的线性投影进行变换，\n",
    "以产生最终输出。\n",
    "这种设计被称为**多头注意力（multihead attention）**。\n",
    "对于$h$个注意力池化输出，每一个注意力池化都被称作一个**头（head）**。\n",
    "图10.5.1\n",
    "展示了使用全连接层来实现可学习的线性变换的多头注意力。\n",
    "\n",
    "<center><img src='../img/multi-head-attention.svg'/></center>\n",
    "<center>图10.5.1 多头注意力：多个头连结然后线性变换</center>\n",
    "\n",
    "- **要点：**\n",
    "  - **多头注意力**：实践中，我们希望模型可以基于相同的注意力机制**学习到不同的行为**，然后将不同的行为作为知识**组合**起来，捕获序列内**各种范围的依赖关系**。为此，我们可以使用多头注意力，它允许注意力机制组合使用查询、键和值的**不同子空间表示**。\n",
    "  - **子空间表示**：在多头注意力中，我们通过独立学习得到的$h$组不同的线性投影来变换查询、键和值。然后，这$h$组变换后的查询、键和值**并行**地送到注意力池化中。\n",
    "  - **多头**：在多头注意力中，对于$h$个注意力池化输出，每一个注意力池化都被称作一个\"头\"。\n",
    "  - **多头注意力的实现**：多头注意力的实现包括：使用全连接层来实现**可学习的线性变换**，将$h$个注意力池化的输出拼接在一起，并且通过另一个**可以学习的线性投影**进行变换，以产生最终输出。\n",
    "  - **线性投影**是一种线性变换，它将数据从**一个向量空间映射到另一个向量空间**。在深度学习中，这种线性投影通常由一个全连接层（或线性层）执行，**没有激活函数**。\n",
    "    - 线性投影的主要作用是改变数据的维度或表示形式，以便更好地进行后续的计算和处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "## 10.5.1 多头注意力模型\n",
    "\n",
    "- 首先用数学语言将多头注意力模型形式化地描述出来。\n",
    "- 给定查询$\\mathbf{q} \\in \\mathbb{R}^{d_q}$、\n",
    "键$\\mathbf{k} \\in \\mathbb{R}^{d_k}$和\n",
    "值$\\mathbf{v} \\in \\mathbb{R}^{d_v}$，\n",
    "每个注意力头$\\mathbf{h}_i$（$i = 1, \\ldots, h$）的计算方法为：\n",
    "$$\\mathbf{h}_i = f(\\mathbf W_i^{(q)}\\mathbf q, \\mathbf W_i^{(k)}\\mathbf k,\\mathbf W_i^{(v)}\\mathbf v) \\in \\mathbb R^{p_v} \\tag{10.5.1}$$\n",
    "  其中，可学习的参数包括：\n",
    "  - $\\mathbf W_i^{(q)}\\in\\mathbb R^{p_q\\times d_q}$、\n",
    "  - $\\mathbf W_i^{(k)}\\in\\mathbb R^{p_k\\times d_k}$和\n",
    "  - $\\mathbf W_i^{(v)}\\in\\mathbb R^{p_v\\times d_v}$，\n",
    "  - 以及代表注意力池化的函数$f$（实际上$f$中所包含的权重矩阵可以被学习）。\n",
    "- $f$可以是 10.3节中加性注意力和缩放点积注意力。\n",
    "- 多头注意力的输出需要经过另一个线性转换，\n",
    "它对应着$h$个头连结后的结果，因此其可学习参数是\n",
    "$\\mathbf W_o\\in\\mathbb R^{p_o\\times h p_v}$：\n",
    "$$\\mathbf W_o \\begin{bmatrix}\\mathbf h_1\\\\\\vdots\\\\\\mathbf h_h\\end{bmatrix} \\in \\mathbb{R}^{p_o}\\tag{10.5.2}$$\n",
    "- 基于这种设计，每个头都可能会关注输入的不同部分，可以表示比简单加权平均值更复杂的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "## 10.5.2 多头注意力实现\n",
    "\n",
    "- 在实现过程中，我们选择缩放点积注意力作为每一个注意力头。\n",
    "- 为了避免计算代价和参数代价的大幅增长，\n",
    "我们设定$p_q = p_k = p_v = p_o / h$。\n",
    "- 值得注意的是，如果我们将查询、键和值的线性变换的输出数量设置为\n",
    "$p_q h = p_k h = p_v h = p_o$，\n",
    "则可以并行计算$h$个头。\n",
    "- 在下面的实现中，$p_o$是通过参数`num_hiddens`指定的。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"多头注意力\"\"\"\n",
    "    '''\n",
    "     key_size, query_size, value_size, num_hiddens都是100；\n",
    "     num_heads：5\n",
    "    '''\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens,\n",
    "                 num_heads, dropout, bias=False, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = d2l.DotProductAttention(dropout)\n",
    "        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias) # (100, 100)\n",
    "        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias) # (100, 100)\n",
    "        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias) # (100, 100)\n",
    "        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias) # 100, 100)\n",
    "\n",
    "    '''\n",
    "    quereies: (2, 4, 100)\n",
    "    keys: (2, 6, 100)\n",
    "    values:(2, 6, 100)\n",
    "    \n",
    "    查询有4个，键-值对有6个。\n",
    "    有5个注意力头。\n",
    "    '''\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        # queries，keys，values的形状:\n",
    "        # (batch_size，查询或者“键－值”对的个数，num_hiddens)\n",
    "        # valid_lens　的形状:\n",
    "        # (batch_size，)或(batch_size，查询的个数)\n",
    "        # 经过变换后，输出的queries，keys，values　的形状:\n",
    "        # (batch_size*num_heads，查询或者“键－值”对的个数，\n",
    "        # num_hiddens/num_heads)\n",
    "        '''\n",
    "        queries的形状会变形为：(2, 4, 100)->(2*5, 4, 100/5)=(10, 4, 20)\n",
    "        keys的形状会变形为：(2, 6, 100)->(2*5, 6, 100/5)=(10, 6, 20)\n",
    "        values的形状会变形为：(2, 6, 100)->(2*5, 6, 100/5)=(10, 6, 20)\n",
    "        '''\n",
    "        queries = transpose_qkv(self.W_q(queries), self.num_heads)\n",
    "        keys = transpose_qkv(self.W_k(keys), self.num_heads)\n",
    "        values = transpose_qkv(self.W_v(values), self.num_heads)\n",
    "        #print('q,k,v的各自形状：', queries.shape, keys.shape, values.shape)\n",
    "        \n",
    "        if valid_lens is not None:\n",
    "            # 在轴0，将第一项（标量或者矢量）复制num_heads次，\n",
    "            # 然后如此复制第二项\n",
    "            valid_lens = torch.repeat_interleave(\n",
    "                valid_lens, repeats=self.num_heads, dim=0) #本例中应该是长度为10的向量，5个2和5个3\n",
    "            #print('复制后的valid_lens形状：', valid_lens.shape)\n",
    "\n",
    "        # output的形状:(batch_size*num_heads，查询的个数，num_hiddens/num_heads)\n",
    "        ## 此处为(2*5, 4, 100/5)\n",
    "        output = self.attention(queries, keys, values, valid_lens)\n",
    "\n",
    "        # output_concat的形状:(batch_size，查询的个数，num_hiddens)\n",
    "        output_concat = transpose_output(output, self.num_heads)\n",
    "        return self.W_o(output_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10]), tensor([2, 2, 2, 2, 2, 3, 3, 3, 3, 3]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如何间隔复制的对象是向量\n",
    "h = torch.tensor([2,3])\n",
    "h0 = torch.repeat_interleave(h, 5, dim=0)\n",
    "h0.shape, h0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "- 为了能够使多个头并行计算，\n",
    "上面的`MultiHeadAttention`类将使用下面定义的两个转置函数。\n",
    "- 具体来说，`transpose_output`函数反转了`transpose_qkv`函数的操作。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "origin_pos": 10,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "'''\n",
    "X形状：(2,4,100)或(2,6,100)\n",
    "num_heads:5\n",
    "'''\n",
    "def transpose_qkv(X, num_heads):\n",
    "    \"\"\"为了多注意力头的并行计算而变换形状\"\"\"\n",
    "    # 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens)\n",
    "    # 输出X的形状:(batch_size，查询或者“键－值”对的个数，num_heads，\n",
    "    # num_hiddens/num_heads)\n",
    "    '''\n",
    "    本步骤的变形\n",
    "    queries：(2,4,100)->(2,4,5,20)\n",
    "    keys,values: (2,6,100)->(2,6,5,20)\n",
    "    '''\n",
    "    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)\n",
    "\n",
    "    # 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数,\n",
    "    # num_hiddens/num_heads)\n",
    "    '''\n",
    "    本步骤的变形\n",
    "    queries：(2,4,5,20)->(2,5,4,20)\n",
    "    keys,values: (2,6,5,20)->(2,5,6,20)\n",
    "    '''\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "\n",
    "    # 最终输出的形状:(batch_size*num_heads,查询或者“键－值”对的个数,\n",
    "    # num_hiddens/num_heads)\n",
    "    '''\n",
    "    本步骤的变形\n",
    "    queries：(2,5,4,20)->(10,4,20)\n",
    "    keys,values: (2,5,6,20)->(10,6,20)\n",
    "    \n",
    "    可不可以这样解释：其实就是将注意力头个数的维度变形到了第1维，\n",
    "    便于后面进行张量计算。计算完之后，再通过transpose_output\n",
    "    变回来就是多头注意力的计算结果了。\n",
    "    '''\n",
    "    \n",
    "    return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "\n",
    "\n",
    "#@save\n",
    "'''\n",
    "以queries为例，多头注意力的计算值(非最终值)X是(10,4,20),然后要变回查询queries的形状(2,4,100)。\n",
    "'''\n",
    "def transpose_output(X, num_heads):\n",
    "    \n",
    "    \"\"\"逆转transpose_qkv函数的操作\"\"\"\n",
    "    '''\n",
    "    (10,4,20)->(2,5,4,20)，此处是将注意力头数从第1维中分离出来成为单独的第2维。\n",
    "    '''\n",
    "    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])\n",
    "    \n",
    "    '''\n",
    "    (2,5,4,20)->(2,4,5,20),将注意力头数重排到第3维，便于后面合并多头注意力的输出值。\n",
    "    '''\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    \n",
    "    '''\n",
    "    (2,4,5,20)->(2,4,100),合并第3,4维，作为多头注意力的值进行输出。\n",
    "    可以这样理解，在4个查询中，每个都有100个注意力，然后又分为5个头，每个头20个注意力值。\n",
    "    这5个分别捕获序列内各种范围的依赖关系，比如长距离依赖和短距离依赖 。\n",
    "    '''\n",
    "    return X.reshape(X.shape[0], X.shape[1], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "- 下面使用键和值相同的小例子来测试编写的`MultiHeadAttention`类。\n",
    "- 多头注意力输出的形状是（`batch_size`，`num_queries`，`num_hiddens`）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "origin_pos": 14,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHeadAttention(\n",
       "  (attention): DotProductAttention(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (W_q): Linear(in_features=100, out_features=100, bias=False)\n",
       "  (W_k): Linear(in_features=100, out_features=100, bias=False)\n",
       "  (W_v): Linear(in_features=100, out_features=100, bias=False)\n",
       "  (W_o): Linear(in_features=100, out_features=100, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_hiddens, num_heads = 100, 5\n",
    "attention = MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,\n",
    "                               num_hiddens, num_heads, 0.5)\n",
    "attention.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "origin_pos": 16,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 100])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, num_queries = 2, 4\n",
    "num_kvpairs, valid_lens =  6, torch.tensor([3, 2])\n",
    "X = torch.ones((batch_size, num_queries, num_hiddens))\n",
    "Y = torch.ones((batch_size, num_kvpairs, num_hiddens))\n",
    "## queries:X, keys:Y, values:Y\n",
    "attention(X, Y, Y, valid_lens).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2]), torch.Size([2, 4, 100]), torch.Size([2, 6, 100]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_lens.shape, X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 18
   },
   "source": [
    "## 小结\n",
    "\n",
    "* 多头注意力融合了来自于多个注意力池化的不同知识，这些知识的不同来源于相同的查询、键和值的不同的子空间表示。\n",
    "* 基于适当的张量操作，可以实现多头注意力的并行计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "- **说明：多头注意力形成多头的3种方式**\n",
    "  - **线性投影分头法**（最常用）：\n",
    "```python\n",
    "# 假设输入 x 的形状是 [batch_size, seq_len, d_model]\n",
    "# num_heads 是头数\n",
    "# d_k 是每个头的维度\n",
    "\n",
    "# 通过线性变换得到 Q、K、V\n",
    "Q = linear_q(x)  # [batch_size, seq_len, d_model]\n",
    "K = linear_k(x)  # [batch_size, seq_len, d_model]\n",
    "V = linear_v(x)  # [batch_size, seq_len, d_model]\n",
    "\n",
    "# 重塑为多头形式\n",
    "Q = Q.view(batch_size, seq_len, num_heads, d_k).transpose(1, 2)\n",
    "K = K.view(batch_size, seq_len, num_heads, d_k).transpose(1, 2)\n",
    "V = V.view(batch_size, seq_len, num_heads, d_k).transpose(1, 2)\n",
    "# 最终形状：[batch_size, num_heads, seq_len, d_k]\n",
    "```\n",
    "\n",
    "  - **直接分割法**：\n",
    "```python\n",
    "# 直接将输入特征维度分成多头\n",
    "x = x.view(batch_size, seq_len, num_heads, -1).transpose(1, 2)\n",
    "```\n",
    "\n",
    "  - **卷积分头法**：\n",
    "```python\n",
    "# 使用分组卷积来实现多头\n",
    "conv = nn.Conv1d(in_channels, out_channels, \n",
    "                 kernel_size=1, groups=num_heads)\n",
    "```\n",
    "\n",
    "  - **区别和特点**：\n",
    "    - 线性投影分头法：\n",
    "      - 最常用的方法\n",
    "      - 每个头可以学习不同的特征表示\n",
    "      - 通过可学习的参数进行特征变换\n",
    "      - 计算开销较大但效果更好\n",
    "    - 直接分割法：\n",
    "      - 实现简单，计算开销小\n",
    "      - 没有额外的参数学习\n",
    "      - 可能会限制特征学习能力\n",
    "      - 各个头之间的特征相关性可能较强\n",
    "    - 卷积分头法：\n",
    "      - 结合了卷积的特点\n",
    "      - 可以捕获局部特征\n",
    "      - 参数量适中\n",
    "      - 适合某些特定任务\n",
    "     \n",
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
