{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 8.2 文本预处理\n",
    "- **目录**\n",
    "  - 8.2.1 读取数据集\n",
    "  - 8.2.2 词元化\n",
    "  - 8.2.3 词表\n",
    "  - 8.2.4 整合所有功能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "- 对于**序列数据**处理问题，我们在 8.1节中\n",
    "评估了所需的统计工具和预测时面临的挑战。\n",
    "- 这样的数据存在许多种形式，**文本是最常见例子之一**。\n",
    "  - 例如，一篇文章可以被简单地看作是一串单词序列，甚至是一串字符序列。\n",
    "- 本节中，我们将解析文本的常见预处理步骤，通常包括：\n",
    "  - （1）将文本作为字符串加载到内存中。\n",
    "  - （2）将字符串拆分为**词元**（如单词和字符）。\n",
    "  - （3）建立一个词表，将拆分的**词元映射到数字索引**。\n",
    "  - （4）将**文本转换为数字索引序列**，方便模型操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import collections\n",
    "import re\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "## 8.2.1 读取数据集\n",
    "\n",
    "- 首先从H.G.Well的[时光机器](https://www.gutenberg.org/ebooks/35)中加载文本。\n",
    "  - 这是一个相当小的语料库，只有30000多个单词,可供试用，\n",
    "- 现实中的文档集合可能会包含数十亿个单词。\n",
    "- 下面的函数**将数据集读取到由多条文本行组成的列表中**，其中每条文本行都是一个字符串。\n",
    "  - 为简单起见，这里忽略了标点符号和字母大写。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "origin_pos": 5,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 文本总行数: 3221\n",
      "the time machine by h g wells\n",
      "twinkled and his usually pale face was flushed and animated the\n"
     ]
    }
   ],
   "source": [
    "#@save\n",
    "d2l.DATA_HUB['time_machine'] = (d2l.DATA_URL + 'timemachine.txt',\n",
    "                                '090b5e7e70c295757f55df93cb0a180b9691891a')\n",
    "\n",
    "def read_time_machine():  #@save\n",
    "    \"\"\"将时间机器数据集加载到文本行的列表中\"\"\"\n",
    "    with open(d2l.download('time_machine'), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    ## 将非字母的字符全部替换成空格\n",
    "    ## 消除两端空格，转换成小写\n",
    "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
    "\n",
    "lines = read_time_machine()\n",
    "print(f'# 文本总行数: {len(lines)}')\n",
    "print(lines[0])\n",
    "print(lines[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://d2l-data.s3-accelerate.amazonaws.com/'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d2l中相关数据的下载地址，数据集保存在亚马逊云服务平台上\n",
    "d2l.DATA_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello,world'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## re正则表达式的用法\n",
    "## pattern表示非字母,逗号，句点，句号的字符替换成空（注意不是空格）\n",
    "s='He  llo,w13or45ld!'\n",
    "re.sub('[^A-Za-z,.。]+','',s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "## 8.2.2 词元化\n",
    "\n",
    "- 下面的`tokenize`函数将文本行列表（`lines`）作为输入，\n",
    "- 列表中的每个元素是一个文本序列（如一条文本行）。\n",
    "- **每个文本序列又被拆分成一个词元列表**\n",
    "  - **词元（token）**是文本的**基本单位**。\n",
    "- 最后，返回一个由词元列表组成的列表，其中的每个词元都是一个字符串（string）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['i']\n",
      "[]\n",
      "[]\n",
      "['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him']\n",
      "['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']\n",
      "['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(lines, token='word'):  #@save\n",
    "    \"\"\"将文本行拆分为单词或字符词元\"\"\"\n",
    "    if token == 'word':\n",
    "        return [line.split() for line in lines] ## 将一行文字切割成单词\n",
    "    elif token == 'char':\n",
    "        return [list(line) for line in lines] ## list可以将字符串转换成字符列表\n",
    "    else:\n",
    "        print('错误：未知词元类型：' + token)\n",
    "        \n",
    "\n",
    "## tokens是一个二维list，内层list是某行文本的词元列表\n",
    "tokens = tokenize(lines)\n",
    "for i in range(11):\n",
    "    print(tokens[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'time', 'machine', 'by', 'h', 'g', 'wells'],\n",
       " ['i'],\n",
       " ['the',\n",
       "  'time',\n",
       "  'traveller',\n",
       "  'for',\n",
       "  'so',\n",
       "  'it',\n",
       "  'will',\n",
       "  'be',\n",
       "  'convenient',\n",
       "  'to',\n",
       "  'speak',\n",
       "  'of',\n",
       "  'him'],\n",
       " ['was',\n",
       "  'expounding',\n",
       "  'a',\n",
       "  'recondite',\n",
       "  'matter',\n",
       "  'to',\n",
       "  'us',\n",
       "  'his',\n",
       "  'grey',\n",
       "  'eyes',\n",
       "  'shone',\n",
       "  'and']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 还可以去掉空字符（不是空格字符，这是两码事）\n",
    "[line.split() for line in lines[0:10] if line != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['H', 'e', 'l', 'l', 'o', ',', 'w', 'o', 'r', 'l', 'd', '!'],\n",
       " ['你', '好', '，', '世', '界', '！']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 直接使用list将字符串转换成字符列表，这种方式很常用\n",
    "[list(line) for line in ['Hello,world!','你好，世界！']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 8,
    "tags": []
   },
   "source": [
    "## 8.2.3 词表 \n",
    "\n",
    "- 词元的类型是字符串，而模型需要的输入是数字，因此这种类型不方便模型使用。\n",
    "现在，让我们构建一个字典，通常也叫做**词表（vocabulary）**，\n",
    "用来将字符串类型的词元映射到从$0$开始的数字索引中。\n",
    "- 我们先将训练集中的所有文档合并在一起，对它们的**唯一词元**进行统计，得到的统计结果称之为**语料（corpus）**。\n",
    "- 然后根据每个唯一词元的出现频率，为其分配一个数字索引。\n",
    "**很少出现的词元通常被移除**，这可以降低复杂性。\n",
    "- 另外，语料库中不存在或已删除的任何词元都将映射到一个特定的**未知词元“&lt;unk&gt;”**。\n",
    "- 我们可以选择增加一个列表，用于保存那些被保留的词元，\n",
    "例如：**填充词元（“&lt;pad&gt;”）**；\n",
    "**序列开始词元（“&lt;bos&gt;”）**；\n",
    "**序列结束词元（“&lt;eos&gt;”）**。\n",
    "- **词表是词元与索引对应；语料就是原文档中的词元按顺序全部对应转换成索引**。\n",
    "\n",
    "- 构造语料库的**基本步骤**总结如下:\n",
    "  - **构建词表（vocabulary）**：将字符串类型的词元映射到从0开始的数字索引中。\n",
    "  - **生成语料（corpus）**：合并训练集中的所有文档，统计唯一词元。\n",
    "  - **为每个唯一词元分配数字索引**：根据词元的出现频率。\n",
    "  - **移除低频词元**：降低模型复杂性。\n",
    "  - **未知词元（\"&lt;unk&gt;\"）**：映射不存在或已删除的词元。\n",
    "  - **保留特殊词元**：例如填充词元（“&lt;pad&gt;”）、序列开始词元（“&lt;bos&gt;”）、序列结束词元（“&lt;eos&gt;”）。\n",
    "  - **词表与索引对应**：将词元转换为数字索引。\n",
    "  - **语料库转换**：将原文档中的词元按顺序全部对应转换成索引。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "- **说明：词频在词元索引分配中有何作用？**\n",
    "  - **降低计算复杂度**：根据出现频率为词元分配索引，使得高频词元具有较小的索引值，即排序靠前。在实际应用中，这有助于提高计算效率。例如，在一些压缩算法与神经网络模型中，较小的数字可以用较少的比特表示，从而降低存储和计算复杂度。\n",
    "  - **提高模型泛化能力**：高频词元在语料库中更具代表性，而低频词元可能是噪声、拼写错误或非常特殊的词汇。通过根据词频分配索引并移除低频词元，可以使模型关注更具代表性的词元，提高模型在处理新数据时的泛化能力。\n",
    "  - **有利于处理稀疏性问题**：在自然语言处理任务中，词表通常非常庞大，而大多数词元在给定文本中出现的频率较低。根据词频分配索引，有助于减少稀疏性问题，从而提高模型的性能。\n",
    "  - **简化模型训练**：在训练词嵌入（例如 Word2Vec 或 GloVe）时，根据词频对词元进行排序并分配索引有助于在负采样或其他优化策略中利用词频信息。例如，可以通过将词频转换为概率分布，从而更容易地从词汇表中采样词元。\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "origin_pos": 9,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class Vocab:  #@save\n",
    "    \"\"\"文本词表\"\"\"\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        ## 初始化tokens和reserved_tokens两个变量为list对象\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        # 按出现频率排序\n",
    "        counter = count_corpus(tokens)\n",
    "        ## 按照items的第二个元素排序，即字典的值，也就是按词频由高到低排序\n",
    "        ## reverse=True表示由高到底\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                   reverse=True)\n",
    "        # 未知词元的索引为0\n",
    "        ## 保存在reserved_tokens的第一个元素，因此其索引为0        \n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
    "        \n",
    "        ## 初始化token_to_idx，是一个字典类型数据：\n",
    "        ## \"词元:索引\"键值对\n",
    "        self.token_to_idx = {token: idx\n",
    "                             for idx, token in enumerate(self.idx_to_token)}\n",
    "        \n",
    "        ## 下面正式开始为idx_to_token和token_to_idx赋值\n",
    "        for token, freq in self._token_freqs:\n",
    "            ## 设定词频阈值，低于该阈值就忽略这个词元，不保存在字典中\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            '''\n",
    "            构造词表的关键步骤：\n",
    "            （1）idx_to_token本身就是一个list，直接将词元添加进去，自动和索引按顺序对应上了，\n",
    "            注意索引是从0开始，而不是从1开始。\n",
    "            （2）token_to_idx则不同，它是一个字典类型，键是词元，值是词元索引，由于是依次存入idx_to_token的，\n",
    "            因此直接取出idx_to_token的长度减1，即是该词元的索引。\n",
    "            注意减1是因为list的索引是从0开始的，因此最后一个词元的索引是idx_to_token的长度减1。           \n",
    "            '''\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    '''\n",
    "    覆盖该函数，本类的对象可以通过[]操作符取值或切片\n",
    "    按照后面的操作，返回指定词元的索引。  \n",
    "    \n",
    "    下属代码初看让人十分困惑，但是仔细看就明白了：\n",
    "    （1）if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        这两行代码首先判定tokens是不是list或tuple对象，如果不是list或tuple，那么就是单个token，然后从token_to_idx\n",
    "        以该token作为键取其值，即该词元的索引，然后返回给调用者，下面还有一个return就不执行了。\n",
    "    （2）return [self.__getitem__(token) for token in tokens]\n",
    "         这一行实际上就是获取类型为list或tuple的tokens里的单个token，然后再调用__getitem__函数，\n",
    "         那么就会执行判断，然后再执行self.token_to_idx.get(tokens, self.unk)。\n",
    "    （3）其实就是确保执行或最后执行的是return self.token_to_idx.get(tokens, self.unk)这一行代码。\n",
    "    '''\n",
    "    def __getitem__(self, tokens):\n",
    "        ## 确保tokens是单个的词元，然后通过词元获取相应索引\n",
    "        ## 如果词元不存在，返回unk的索引0。注意get的用法        \n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk) ## \n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        ## 确保indeces是单个索引，然后通过索引获取相应词元\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "    \n",
    "    ## 返回<unk>词元的索引，即0\n",
    "    ## 在__getitem__函数里调用\n",
    "    @property\n",
    "    def unk(self):  # 未知词元的索引为0\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self):\n",
    "        return self._token_freqs\n",
    "'''\n",
    "计算所有词元的词频，返回的是一个字典：\n",
    "键是词元，值是词频。\n",
    "'''\n",
    "def count_corpus(tokens):  #@save\n",
    "    \"\"\"统计词元的频率\"\"\"\n",
    "    # 这里的tokens是1D列表或2D列表\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        # 将词元列表展平成一个列表\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **说明：Vocab类功能详解**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 2,\n",
       "         'time': 2,\n",
       "         'to': 2,\n",
       "         'machine': 1,\n",
       "         'by': 1,\n",
       "         'h': 1,\n",
       "         'g': 1,\n",
       "         'wells': 1,\n",
       "         'i': 1,\n",
       "         'traveller': 1,\n",
       "         'for': 1,\n",
       "         'so': 1,\n",
       "         'it': 1,\n",
       "         'will': 1,\n",
       "         'be': 1,\n",
       "         'convenient': 1,\n",
       "         'speak': 1,\n",
       "         'of': 1,\n",
       "         'him': 1,\n",
       "         'was': 1,\n",
       "         'expounding': 1,\n",
       "         'a': 1,\n",
       "         'recondite': 1,\n",
       "         'matter': 1,\n",
       "         'us': 1,\n",
       "         'his': 1,\n",
       "         'grey': 1,\n",
       "         'eyes': 1,\n",
       "         'shone': 1,\n",
       "         'and': 1})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 计算每个token的个数\n",
    "## Counter本质上就是一个dict类型\n",
    "t = [t for line in lines[0:10] for t in line.split()]\n",
    "tf = collections.Counter(t)\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 2,\n",
       "         'time': 2,\n",
       "         'to': 2,\n",
       "         'machine': 1,\n",
       "         'by': 1,\n",
       "         'h': 1,\n",
       "         'g': 1,\n",
       "         'wells': 1,\n",
       "         'i': 1,\n",
       "         'traveller': 1,\n",
       "         'for': 1,\n",
       "         'so': 1,\n",
       "         'it': 1,\n",
       "         'will': 1,\n",
       "         'be': 1,\n",
       "         'convenient': 1,\n",
       "         'speak': 1,\n",
       "         'of': 1,\n",
       "         'him': 1,\n",
       "         'was': 1,\n",
       "         'expounding': 1,\n",
       "         'a': 1,\n",
       "         'recondite': 1,\n",
       "         'matter': 1,\n",
       "         'us': 1,\n",
       "         'his': 1,\n",
       "         'grey': 1,\n",
       "         'eyes': 1,\n",
       "         'shone': 1,\n",
       "         'and': 1})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "[t for line in lines[0:10] for t in line.split()]改成普通嵌套for循环.\n",
    "for line in lines[0:10]是外层循环；for t in line.split()是内层循环\n",
    "即前面for循环是外层循环，后面for循环是内层循环。\n",
    "'''\n",
    "t = []\n",
    "for line in lines[0:10]:\n",
    "    for token in line.split():\n",
    "        t.append(token)\n",
    "tf = collections.Counter(t)\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'time',\n",
       " 'machine',\n",
       " 'by',\n",
       " 'h',\n",
       " 'g',\n",
       " 'wells',\n",
       " 'i',\n",
       " 'the',\n",
       " 'time',\n",
       " 'traveller',\n",
       " 'for',\n",
       " 'so',\n",
       " 'it',\n",
       " 'will',\n",
       " 'be',\n",
       " 'convenient',\n",
       " 'to',\n",
       " 'speak',\n",
       " 'of',\n",
       " 'him',\n",
       " 'was',\n",
       " 'expounding',\n",
       " 'a',\n",
       " 'recondite',\n",
       " 'matter',\n",
       " 'to',\n",
       " 'us',\n",
       " 'his',\n",
       " 'grey',\n",
       " 'eyes',\n",
       " 'shone',\n",
       " 'and']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 2),\n",
       " ('time', 2),\n",
       " ('to', 2),\n",
       " ('machine', 1),\n",
       " ('by', 1),\n",
       " ('h', 1),\n",
       " ('g', 1),\n",
       " ('wells', 1),\n",
       " ('i', 1),\n",
       " ('traveller', 1),\n",
       " ('for', 1),\n",
       " ('so', 1),\n",
       " ('it', 1),\n",
       " ('will', 1),\n",
       " ('be', 1),\n",
       " ('convenient', 1),\n",
       " ('speak', 1),\n",
       " ('of', 1),\n",
       " ('him', 1),\n",
       " ('was', 1),\n",
       " ('expounding', 1),\n",
       " ('a', 1),\n",
       " ('recondite', 1),\n",
       " ('matter', 1),\n",
       " ('us', 1),\n",
       " ('his', 1),\n",
       " ('grey', 1),\n",
       " ('eyes', 1),\n",
       " ('shone', 1),\n",
       " ('and', 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "注意sorted排序函数后两个参数的用法。\n",
    "sorted函数是非就地排序，即不改变tf本身的排序，而是返回一个排序后的对象。\n",
    "key参数指定排序的依据，此处是依据元组的第二个元素即词频作为排序依据。\n",
    "reverse=True表示根据词频从高到低对元组进行排序。\n",
    "'''\n",
    "sorted(tf.items(), key=lambda x:x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('the', 2), ('time', 2), ('machine', 1), ('by', 1), ('h', 1), ('g', 1), ('wells', 1), ('i', 1), ('traveller', 1), ('for', 1), ('so', 1), ('it', 1), ('will', 1), ('be', 1), ('convenient', 1), ('to', 2), ('speak', 1), ('of', 1), ('him', 1), ('was', 1), ('expounding', 1), ('a', 1), ('recondite', 1), ('matter', 1), ('us', 1), ('his', 1), ('grey', 1), ('eyes', 1), ('shone', 1), ('and', 1)])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## tf是一个字典\n",
    "tf.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 2),\n",
       " ('time', 2),\n",
       " ('machine', 1),\n",
       " ('by', 1),\n",
       " ('h', 1),\n",
       " ('g', 1),\n",
       " ('wells', 1),\n",
       " ('i', 1),\n",
       " ('traveller', 1),\n",
       " ('for', 1),\n",
       " ('so', 1),\n",
       " ('it', 1),\n",
       " ('will', 1),\n",
       " ('be', 1),\n",
       " ('convenient', 1),\n",
       " ('to', 2),\n",
       " ('speak', 1),\n",
       " ('of', 1),\n",
       " ('him', 1),\n",
       " ('was', 1),\n",
       " ('expounding', 1),\n",
       " ('a', 1),\n",
       " ('recondite', 1),\n",
       " ('matter', 1),\n",
       " ('us', 1),\n",
       " ('his', 1),\n",
       " ('grey', 1),\n",
       " ('eyes', 1),\n",
       " ('shone', 1),\n",
       " ('and', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "items函数返回字典的键值对，是以元组形式保存，前键后值，然后将元组保存到一个list里。\n",
    "这样的确便于迭代，尤其是通过lambda函数操作。\n",
    "还不是一个纯list，而是封装在dict_items里的list，当然也可以直接转换成list使用。\n",
    "'''\n",
    "list(tf.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zhangsan'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## __getitem__函数的用法，既可切片也可进行字典操作\n",
    "class A: #@save\n",
    "    def __init__(self):\n",
    "        self.l={'name':'zhangsan','age':21}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._l)\n",
    "    \n",
    "    def __getitem__(self,k):\n",
    "        #print('get item')\n",
    "        return self.l[k]\n",
    "    \n",
    "a=A()\n",
    "a['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice\n",
      "Unknown\n"
     ]
    }
   ],
   "source": [
    "## 字典数据结构的get方法应用\n",
    "dictionary = {'name': 'Alice', 'age': 25, 'city': 'Beijing'}\n",
    "\n",
    "# 获取键'name'对应的值\n",
    "name_value = dictionary.get('name')\n",
    "print(name_value)  # 输出：Alice\n",
    "\n",
    "# 获取键'gender'对应的值，此键不存在，返回默认值'Unknown'\n",
    "gender_value = dictionary.get('gender', 'Unknown')\n",
    "print(gender_value)  # 输出：Unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "- 首先使用时光机器数据集作为语料库来**构建词表**。\n",
    "- 然后打印前几个高频词元及其索引。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "origin_pos": 11,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<unk>', 0), ('the', 1), ('i', 2), ('and', 3), ('of', 4), ('a', 5), ('to', 6), ('was', 7), ('in', 8), ('that', 9)]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(tokens)\n",
    "print(list(vocab.token_to_idx.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "- 现在可以**将每一条文本行转换成一个数字索引列表**。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "origin_pos": 13,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本: ['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "索引: [1, 19, 50, 40, 2183, 2184, 400]\n",
      "文本: ['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n",
      "索引: [2186, 3, 25, 1044, 362, 113, 7, 1421, 3, 1045, 1]\n"
     ]
    }
   ],
   "source": [
    "for i in [0, 10]:\n",
    "    print('文本:', tokens[i])\n",
    "    print('索引:', vocab[tokens[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **说明：继续解析Vocab类**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, [3, 113], [7, 1421])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Vocab对象既可传递单个词元，又可以传递词元列表或元组，以便获取词元索引\n",
    "## []功能通过覆盖__getitem__方法实现\n",
    "vocab.__getitem__('and'),vocab['and'],vocab[['and','face']],vocab[('was','flushed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 2261),\n",
       " ('i', 1267),\n",
       " ('and', 1245),\n",
       " ('of', 1155),\n",
       " ('a', 816),\n",
       " ('to', 695),\n",
       " ('was', 552),\n",
       " ('in', 541),\n",
       " ('that', 443),\n",
       " ('my', 440)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 词频，由高到低排列\n",
    "vocab.token_freqs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.to_tokens(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 14
   },
   "source": [
    "## 8.2.4 整合所有功能\n",
    "\n",
    "- 在使用上述函数时，**将所有功能打包到`load_corpus_time_machine`函数**。\n",
    "  - 该函数返回`corpus`（词元索引列表，即语料库）和`vocab`（时光机器语料库的词表）。\n",
    "- 此处的改变是：\n",
    "  - 为了简化后面章节中的训练，我们使用字符（而不是单词）实现文本词元化。\n",
    "    - **注：** 本视频课程将之设置为单词进行词元化。\n",
    "  - 时光机器数据集中的每个**文本行**不一定是一个句子或一个段落，还可能是一个单词，因此返回的`corpus`仅处理为单个列表，而不是使用多词元列表构成的一个列表。\n",
    "    - **注：** 即corpus是一维列表，而不是二维列表，即不是列表的列表。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "origin_pos": 15,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_corpus_time_machine(max_tokens=-1):  #@save\n",
    "    \"\"\"返回时光机器数据集的词元索引列表和词表\"\"\"\n",
    "    lines = read_time_machine()\n",
    "    ## 原代码的第二个参数是'char'\n",
    "    tokens = tokenize(lines, 'char')\n",
    "    #tokens = tokenize(lines, 'word')\n",
    "    vocab = Vocab(tokens)\n",
    "    # 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，\n",
    "    # 所以将所有文本行展平到一个列表中，即一维列表，而不是列表的列表\n",
    "    corpus = [vocab[token] for line in tokens for token in line]\n",
    "    ## 截取前10000个词元索引作为语料库\n",
    "    if max_tokens > 0:\n",
    "        corpus = corpus[:max_tokens]\n",
    "    return corpus, vocab\n",
    "\n",
    "## corpus将文本中的词元转换成词表中的该词元所对应的索引\n",
    "corpus, vocab = load_corpus_time_machine(10000)\n",
    "len(corpus), len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<unk>': 0,\n",
       " ' ': 1,\n",
       " 'e': 2,\n",
       " 't': 3,\n",
       " 'a': 4,\n",
       " 'i': 5,\n",
       " 'n': 6,\n",
       " 'o': 7,\n",
       " 's': 8,\n",
       " 'h': 9,\n",
       " 'r': 10,\n",
       " 'd': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'u': 14,\n",
       " 'c': 15,\n",
       " 'f': 16,\n",
       " 'w': 17,\n",
       " 'g': 18,\n",
       " 'y': 19,\n",
       " 'p': 20,\n",
       " 'b': 21,\n",
       " 'v': 22,\n",
       " 'k': 23,\n",
       " 'x': 24,\n",
       " 'z': 25,\n",
       " 'j': 26,\n",
       " 'q': 27}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.token_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **说明：语料库和词表的简单用法**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "time\n",
      "machine\n",
      "by\n",
      "h\n",
      "g\n",
      "wells\n",
      "i\n",
      "the\n",
      "time\n"
     ]
    }
   ],
   "source": [
    "## 比如下面代码反过来将语料中的索引转换成对应的词元，就是一个完整的文本\n",
    "for i in corpus[0:10]:\n",
    "    print(vocab.idx_to_token[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 19, 50, 40, 2183, 2184, 400, 2, 1, 19]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 语料库的前10个词元索引\n",
    "corpus[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 16
   },
   "source": [
    "## 小结\n",
    "\n",
    "* 文本是序列数据的一种最常见的形式之一。\n",
    "* 为了对文本进行预处理，我们通常将文本拆分为词元，构建词表将词元字符串映射为数字索引，并将文本数据转换为词元索引以供模型操作。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
