{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 14 自然语言处理：预训练\n",
    "- **目录**\n",
    "  - 14.1 词嵌入（Word2vec）\n",
    "  - 14.2 词嵌入近似训练\n",
    "  - 14.3 用于预训练词嵌入的数据集\n",
    "  - 14.4 预训练word2vec\n",
    "  - 14.5 全局向量的词嵌入（GloVe）\n",
    "  - 14.6 子词嵌入\n",
    "  - 14.7 词的相似性和类比任务\n",
    "  - 14.8 来自Transformers的双向编码器表示（BERT）\n",
    "  - 14.9 用于预训练BERT的数据集\n",
    "  - 14.10 预训练BERT\n",
    "<br><br>\n",
    "<center><img src='../img/nlp-map-pretrain.svg'></center>\n",
    "<center>图14.1 预训练好的<b>文本表示</b>可以放入各种深度学习架构，应用于不同自然语言处理任务（本章主要研究上游文本的预训练）</center><br>\n",
    "\n",
    "\n",
    "- **要点：**\n",
    "  - **人与人的交流**：由于人类的交流需求，每天都会产生大量的书面文本，如社交媒体、聊天应用、电子邮件、产品评论、新闻文章、研究论文和书籍中的文本。\n",
    "  - **自然语言处理**：这是一种研究计算机与人之间使用自然语言进行交互的技术。\n",
    "    - 实际应用中，它用于处理和分析文本数据。\n",
    "  - **学习文本表示**：为了理解文本，可以从学习其**表示**开始。\n",
    "  - **自监督学习**：这是一种预训练文本表示的方法，**利用文本的其它部分来预测文本的隐藏部分**。\n",
    "    - 这样，模型可以在没有昂贵标签的情况下从大量文本数据中学习。\n",
    "  - **词元表示的预训练**：可以使用word2vec、GloVe或子词嵌入模型在大型语料库上预训练每个词元。\n",
    "    - 预训练后，**每个词元的表示是一个向量**。\n",
    "  - **上下文敏感的表示**：传统的预训练模型，如word2vec和GloVe，为每个单词提供固定的**向量表示**，不考虑上下文。\n",
    "    - 例如，“bank”在不同上下文中的表示是相同的。\n",
    "    - 为了解决这个问题，新的预训练模型，如BERT，考虑了**上下文**来为单词**生成表示**。\n",
    "  - **预训练模型的应用**：图14.1显示预训练的文本表示可以被**整合**到多种深度学习架构中，并应用于各种自然语言处理任务。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
