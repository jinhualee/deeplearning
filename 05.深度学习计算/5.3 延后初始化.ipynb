{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 5.3 延后初始化\n",
    "- **目录**\n",
    "  - 5.3.1 延后初始化网络参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "- 到目前为止，我们忽略了建立网络时需要做的以下这些事情：\n",
    "  * 我们定义了网络架构，但**没有指定输入维度**。\n",
    "  * 我们添加层时**没有指定前一层的输出维度**。\n",
    "  * 我们在初始化参数时，甚至没有足够的信息来确定模型应该包含多少参数。\n",
    "- 诀窍是框架的**延后初始化**（defers initialization），即直到数据第一次通过模型传递时，框架才会**动态地推断出每个层的大小**。\n",
    "- 在以后，当使用卷积神经网络时，由于输入维度（即图像的分辨率）将影响每个后续层的维数，有了该技术将更加方便。\n",
    "- 现在我们在编写代码时无须知道维度是什么就可以设置参数，这种能力可以大大简化定义和修改模型的任务。\n",
    "- 接下来将更深入地研究初始化机制。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "## 5.3.1 延后初始化网络参数\n",
    "- 自定义一个延迟初始化网络参数的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前向传播前的参数值： None\n",
      "\n",
      "前向传播后的参数形状及其值：\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 20]),\n",
       " Parameter containing:\n",
       " tensor([[-0.2010, -0.3867,  0.1535,  0.3602,  0.4944, -0.1069,  0.2682,  0.4317,\n",
       "          -0.1254,  0.0297, -0.1777,  0.2044, -0.5152,  0.3057, -0.0886,  0.4759,\n",
       "           0.3039, -0.5366,  0.4115, -0.4545],\n",
       "         [ 0.1527,  0.3749, -0.3625, -0.0119,  0.3781, -0.3943, -0.0286, -0.0266,\n",
       "          -0.3532,  0.5460, -0.0886,  0.1037, -0.4048, -0.1733,  0.4334,  0.3845,\n",
       "           0.1415, -0.3082,  0.1364, -0.4234],\n",
       "         [ 0.1889, -0.0147, -0.0064,  0.4771, -0.0843, -0.3223, -0.4457,  0.5387,\n",
       "          -0.3342, -0.2725, -0.3292,  0.0328,  0.3403,  0.3548, -0.3319, -0.3659,\n",
       "           0.3398,  0.4780,  0.5352,  0.4583],\n",
       "         [ 0.1622,  0.0610,  0.1710, -0.2607, -0.1237,  0.2866,  0.5278, -0.0436,\n",
       "          -0.1756, -0.2497,  0.3399,  0.2321, -0.2702,  0.4515, -0.2746, -0.1913,\n",
       "          -0.2582,  0.0438, -0.3915, -0.3052],\n",
       "         [ 0.4998,  0.3642, -0.1349, -0.2395, -0.0197, -0.0591,  0.0085,  0.4109,\n",
       "           0.4158,  0.2711,  0.0870, -0.4338,  0.4711, -0.1826, -0.0910, -0.3672,\n",
       "          -0.1747, -0.4725,  0.4975,  0.4653],\n",
       "         [ 0.0900, -0.3903, -0.4608,  0.2885,  0.2808,  0.4111,  0.3944, -0.4949,\n",
       "          -0.1892,  0.1595,  0.0141, -0.1684,  0.2641, -0.3330, -0.5016, -0.4602,\n",
       "          -0.2697,  0.5109, -0.3470,  0.0760],\n",
       "         [-0.0887, -0.2933,  0.2742, -0.4114, -0.2169, -0.4597,  0.0162,  0.4093,\n",
       "          -0.1332, -0.2573, -0.4814, -0.3120,  0.0990,  0.1593,  0.4143, -0.4351,\n",
       "           0.2241, -0.4764,  0.1398,  0.2288],\n",
       "         [ 0.2011, -0.3555,  0.4965,  0.4351, -0.1530, -0.0325,  0.3974, -0.1918,\n",
       "          -0.4509,  0.2038,  0.4741,  0.2097,  0.3232,  0.3079,  0.1022,  0.4221,\n",
       "           0.5387,  0.4633,  0.3050, -0.3435],\n",
       "         [-0.5214,  0.4975, -0.5454, -0.3231, -0.3102,  0.0668,  0.1766, -0.1538,\n",
       "           0.4463, -0.2313,  0.5182, -0.4660,  0.2919,  0.0534, -0.0901, -0.3865,\n",
       "          -0.0141,  0.3846,  0.4118,  0.0612],\n",
       "         [ 0.1255,  0.5277, -0.1730, -0.2627,  0.1378,  0.5048, -0.0396,  0.0355,\n",
       "           0.4807, -0.3193, -0.2801,  0.3643, -0.4621,  0.0265, -0.1002,  0.5073,\n",
       "           0.3598,  0.5104,  0.0017, -0.0810]], requires_grad=True))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomLinear(nn.Module):\n",
    "    def __init__(self, out_features):\n",
    "        super(CustomLinear, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.weight = None\n",
    "        self.bias = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 延后初始化参数\n",
    "        if self.weight is None or self.bias is None:\n",
    "            in_features = x.shape[-1]\n",
    "            self.weight = nn.Parameter(torch.Tensor(self.out_features, in_features))\n",
    "            self.bias = nn.Parameter(torch.Tensor(self.out_features))\n",
    "\n",
    "            # 初始化权重和偏置，即在前向传播时初始化参数\n",
    "            nn.init.kaiming_uniform_(self.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
    "            nn.init.zeros_(self.bias)\n",
    "\n",
    "        return x @ self.weight.t() + self.bias\n",
    "\n",
    "# 创建一个自定义线性层实例\n",
    "custom_linear = CustomLinear(10)\n",
    "print('前向传播前的参数值：', custom_linear.weight)\n",
    "# 使用随机输入张量\n",
    "X = torch.rand(2, 20)\n",
    "output = custom_linear(X)\n",
    "print('\\n前向传播后的参数形状及其值：')\n",
    "custom_linear.weight.shape,custom_linear.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 13
   },
   "source": [
    "- **说明：**\n",
    "  - 在此示例中，我们定义了一个名为`CustomLinear`的自定义线性层类。它继承自`nn.Module`。在类的构造函数中，我们将输入权重和偏置设为`None`。这意味着权重和偏置参数在实例化时不会被初始化。\n",
    "\n",
    "  - 然后，在`forward`方法中，我们检查权重和偏置是否为`None`。如果它们尚未初始化，我们根据输入张量`x`的形状来初始化它们。请注意，当我们第一次调用`forward`方法并传递输入数据时，延后初始化发生。\n",
    "\n",
    "  - 在本示例中，我们使用了`Kaiming`均匀分布对权重进行初始化，并将偏置初始化为零。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "- **说明：延后初始化补充解释**\n",
    "  - 概念：\n",
    "    - 延后初始化是一种策略，它允许模型在没有具体输入维度信息时构建。\n",
    "    - 参数的实际初始化推迟到收到第一批数据时自动进行。\n",
    "  - 工作原理：\n",
    "     - 模型首次接收到数据时，根据数据的具体形状推断每一层的参数维度。\n",
    "     - 框架动态地分配必要的内存并初始化参数。\n",
    "  - 动态调整优势：\n",
    "    - 延后初始化允许模型动态适应不同的输入尺寸。\n",
    "    - 减少了模型定义时的工作量和出错可能性。\n",
    "  - 举例：\n",
    "    - 假设我们构建一个简单的全连接神经网络，其中包含输入层、隐藏层和输出层。\n",
    "    - 在不使用延后初始化的情况下，如果我们想要处理28x28和32x32两种大小的图像，我们需要为每种情况分别计算并指定隐藏层的输入维度。\n",
    "    - 使用延后初始化，我们只需定义层的连接方式，不需要指定具体的输入维度。\n",
    "    - 当我们第一次向网络输入28x28的图像时，框架自动推断出隐藏层的参数应该是多少。\n",
    "    - 如果我们稍后决定将图像尺寸改变为32x32，框架也会在新的数据第一次通过时自动调整参数维度。\n",
    "  - 框架支持：\n",
    "    - 许多现代深度学习框架，如PyTorch和MXNet，支持延后初始化。这使得研究者和工程师可以更加灵活地设计和实验不同的网络结构。\n",
    "  - 延后初始化是一种允许深度学习模型在不具备完整输入维度信息时构建的策略，它通过在模型第一次接收数据时自动推断参数维度，从而简化模型设计和参数初始化过程。\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 13
   },
   "source": [
    "## 小结\n",
    "\n",
    "* 延后初始化使框架能够自动推断参数形状，使修改模型架构变得容易，避免了一些常见的错误。\n",
    "* 我们可以通过模型传递数据，使框架最终初始化参数。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
