{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 5.2 参数管理\n",
    "- **目录**\n",
    "  - 5.2.1 参数访问\n",
    "    - 5.2.1.1 目标参数\n",
    "    - 5.2.1.2 一次性访问所有参数\n",
    "    - 5.2.1.3 从嵌套块收集参数\n",
    "  - 5.2.2 参数初始化\n",
    "    - 5.2.2.1 内置初始化\n",
    "    - 5.2.2.2 自定义初始化\n",
    "  - 5.2.3 参数绑定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "- 在**选择了架构**并**设置了超参数**后，模型将进入**训练阶段**。\n",
    "- 此时的目标是找到**使损失函数最小化的模型参数值。**\n",
    "- 经过训练后，我们将需要使用这些参数来做出未来的预测。\n",
    "- 此外，有时希望**提取参数**，以便在其他环境中**复用**它们，将模型保存下来，以便它可以在其他软件中执行，或者为了获得科学的理解而进行检查。\n",
    "- 本节将介绍操作参数的具体细节，包括如下内容：\n",
    "  * 访问参数，用于调试、诊断和可视化。\n",
    "  * 参数初始化。\n",
    "  * 在不同模型组件间共享参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4158],\n",
       "        [0.4272]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch import nn\n",
    "##torch有对权重和偏置进行自动初始化的功能\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "## 5.2.1 参数访问\n",
    "- 当通过`Sequential`类定义模型时，可以**通过索引来访问模型的任意层。**\n",
    "- 这就像模型是一个列表一样，每层的参数都在其属性中。\n",
    "- 如下所示可以检查第二个全连接层的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[ 0.1722,  0.3258, -0.2437,  0.3485,  0.3230,  0.0465,  0.2103,  0.2977]])), ('bias', tensor([-0.0523]))])\n"
     ]
    }
   ],
   "source": [
    "print(net[2].state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.1722,  0.3258, -0.2437,  0.3485,  0.3230,  0.0465,  0.2103,  0.2977]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0523], requires_grad=True)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(net[2].parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].state_dict()['weight'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "- **说明：序列模块前向传播的手工实现**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4158],\n",
       "        [0.4272]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "序列模块前向传播的手工实现：\n",
    "测试参数与输入值之间的计算关系\n",
    "尤其是权重的形状有点令人困惑：很显然权重是按照行向量进行保存的\n",
    "'''\n",
    "from torch.nn import functional as F\n",
    "w0=net[0].state_dict()['weight']\n",
    "b0=net[0].state_dict()['bias']\n",
    "o1=F.relu(X@w0.T+b0)\n",
    "w2=net[2].state_dict()['weight']\n",
    "b2=net[2].state_dict()['bias']\n",
    "o2=o1@w2.T+b2\n",
    "o2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4158],\n",
       "        [0.4272]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##下述取值方式与上述方式一致\n",
    "w0=net[0].weight.data\n",
    "b0=net[0].bias.data\n",
    "o1=F.relu(X@w0.T+b0)\n",
    "w2=net[2].weight.data\n",
    "b2=net[2].bias.data\n",
    "o2=o1@w2.T+b2\n",
    "o2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "- 输出的结果告诉我们一些重要的事情：\n",
    "  - 首先，这个**全连接层包含两个参数，分别是该层的权重和偏置。**\n",
    "  - 两者都存储为单精度浮点数（float32）。\n",
    "  - 注意，参数名称允许唯一标识每个参数，即使在包含数百个层的网络中也是如此。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "### 5.2.1.1 目标参数\n",
    "\n",
    "- 注意，每个参数都表示为参数类的一个实例。\n",
    "- 要对参数执行任何操作，首先我们需要访问底层的数值。\n",
    "- 下面的代码**从第二个全连接层（即第三个神经网络层）提取偏置，\n",
    "提取后返回的是一个参数类实例，并进一步访问该参数的值**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "origin_pos": 10,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([-0.0523], requires_grad=True)\n",
      "tensor([-0.0523])\n",
      "tensor([-0.0523])\n"
     ]
    }
   ],
   "source": [
    "print(type(net[2].bias))\n",
    "print(net[2].bias)\n",
    "print(net[2].state_dict()['bias']) ##也可以通过字典属性访问\n",
    "print(net[2].bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "- 参数是复合的对象，包含**值**、**梯度**和**额外信息**。\n",
    "这就是我们需要显式参数值的原因。\n",
    "- 除了值之外，我们还可以访问每个参数的梯度。\n",
    "在上面这个网络中，**由于我们还没有调用反向传播，所以参数的梯度处于初始状态。**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "origin_pos": 14,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight.grad == None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 15
   },
   "source": [
    "### 5.2.1.2 一次性访问所有参数\n",
    "\n",
    "- 当我们需要对所有参数执行操作时，逐个访问它们可能会很麻烦。\n",
    "- 当我们处理更复杂的块（例如，嵌套块）时，情况可能会变得特别复杂，\n",
    "因为我们需要**递归整个树**来提取每个子块的参数。\n",
    "- 下面，我们将通过演示来比较访问第一个全连接层的参数和访问所有层。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "origin_pos": 17,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "## 访问第一个全连接层的参数\n",
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()])\n",
    "## 访问所有层的参数\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 19
   },
   "source": [
    "- 另一种访问网络参数的方式，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "origin_pos": 21,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.4384,  0.1339, -0.0263,  0.1567,  0.1603,  0.4208,  0.2504, -0.3191]),\n",
       " tensor([-0.0523]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 注意字典中的键\n",
    "net.state_dict()['0.bias'], net.state_dict()['2.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_parameters of Sequential(\n",
       "  (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=8, out_features=1, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.named_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 23
   },
   "source": [
    "### 5.2.1.3 从嵌套块收集参数\n",
    "\n",
    "- 如果将多个块相互嵌套，参数命名约定是如何工作的?\n",
    "- 首先定义一个生成块的函数（可以说是“块工厂”），然后将这些块组合到更大的块中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "origin_pos": 25,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3582],\n",
       "        [0.3582]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## block1中有两个全连接层\n",
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                         nn.Linear(8, 4), nn.ReLU())\n",
    "## block2将block1嵌套4次，相当于8个全连接层\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        # 在这里嵌套\n",
    "        net.add_module(f'block {i}', block1())\n",
    "    return net\n",
    "\n",
    "## 最后一个输出层，输出一个标量作为结果\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 27
   },
   "source": [
    "- 设计了网络后，在看工作方式：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "origin_pos": 29,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 31
   },
   "source": [
    "- 因为层是分层嵌套的，所以也可以像通过嵌套列表索引一样访问它们。\n",
    "- 下面访问第一个主要的块中、第二个子块的第一层的偏置项。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "origin_pos": 33,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4556,  0.0797, -0.2463,  0.3337,  0.0246, -0.3351,  0.4367,  0.0798])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 取出某层的偏置参数的值\n",
    "rgnet[0][1][0].bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 35
   },
   "source": [
    "## 5.2.2 参数初始化\n",
    "\n",
    "- 第4.8节中讨论了良好初始化的必要性。\n",
    "- 深度学习框架提供**默认随机初始化**方法，也允许我们创建**自定义初始化方法**。\n",
    "- 默认情况下，PyTorch会根据一个范围均匀地初始化权重和偏置矩阵，这个范围是根据输入和输出维度计算出的。\n",
    "- 除此之外，PyTorch的`nn.init`模块提供了多种预置初始化方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 39
   },
   "source": [
    "### 5.2.2.1 内置初始化\n",
    "- 首先调用内置的初始化器。\n",
    "- 下面的代码将所有权重参数初始化为标准差为0.01的高斯随机变量，且将偏置参数设置为0。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "origin_pos": 41,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0201, -0.0038, -0.0204, -0.0055]), tensor(0.))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        ## 使用正态分布随机数初始化权重\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        ## 偏置全部初始化为0\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_normal)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 43
   },
   "source": [
    "- 还可以将所有参数初始化为给定的常数，比如初始化为1。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "origin_pos": 45,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1.]), tensor(0.))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_constant)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 47
   },
   "source": [
    "- 还可以对某些块应用不同的初始化方法：\n",
    "  - 例如，下面我们使用Xavier初始化方法初始化第一个神经网络层。\n",
    "  - 然后将第三个神经网络层初始化为常量值42。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "origin_pos": 49,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2908, -0.3863,  0.2172, -0.0157])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "def xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 42)\n",
    "        \n",
    "'''\n",
    "注意初始化参数一般是调用net的apply函数，以初始化函数作为参数\n",
    "'''\n",
    "net[0].apply(xavier)\n",
    "net[2].apply(init_42)\n",
    "print(net[0].weight.data[0])\n",
    "print(net[2].weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tensor',\n",
       " '_Optional',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_calculate_correct_fan',\n",
       " '_calculate_fan_in_and_fan_out',\n",
       " '_make_deprecate',\n",
       " '_no_grad_fill_',\n",
       " '_no_grad_normal_',\n",
       " '_no_grad_trunc_normal_',\n",
       " '_no_grad_uniform_',\n",
       " '_no_grad_zero_',\n",
       " 'calculate_gain',\n",
       " 'constant',\n",
       " 'constant_',\n",
       " 'dirac',\n",
       " 'dirac_',\n",
       " 'eye',\n",
       " 'eye_',\n",
       " 'kaiming_normal',\n",
       " 'kaiming_normal_',\n",
       " 'kaiming_uniform',\n",
       " 'kaiming_uniform_',\n",
       " 'math',\n",
       " 'normal',\n",
       " 'normal_',\n",
       " 'ones_',\n",
       " 'orthogonal',\n",
       " 'orthogonal_',\n",
       " 'sparse',\n",
       " 'sparse_',\n",
       " 'torch',\n",
       " 'trunc_normal_',\n",
       " 'uniform',\n",
       " 'uniform_',\n",
       " 'warnings',\n",
       " 'xavier_normal',\n",
       " 'xavier_normal_',\n",
       " 'xavier_uniform',\n",
       " 'xavier_uniform_',\n",
       " 'zeros_']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "nn.init包中的初始化函数列表\n",
    "注意不带下划线的同名初始化函数将废除\n",
    "'''\n",
    "dir(nn.init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 51
   },
   "source": [
    "### 5.2.2.2 自定义初始化\n",
    "\n",
    "- 有时，深度学习框架没有提供所需初始化方法。\n",
    "在下面的例子中，使用以下的分布为任意权重参数$w$定义初始化方法：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    w \\sim \\begin{cases}\n",
    "        U(5, 10) & \\text{ 可能性 } \\frac{1}{4} \\\\\n",
    "            0    & \\text{ 可能性 } \\frac{1}{2} \\\\\n",
    "        U(-10, -5) & \\text{ 可能性 } \\frac{1}{4}\n",
    "    \\end{cases}\n",
    "\\end{aligned} \\tag{5.2.1}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 53,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "- 同样，我们实现了一个`my_init`函数来应用到`net`：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "origin_pos": 56,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        '''\n",
    "        此处显示自动初始化后的值,第一次运行时按照torch的规则进行自动初始化，使用了均匀分布。\n",
    "        均匀分布函数使用的参数可以查看torch文档：\n",
    "        https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear\n",
    "        '''\n",
    "        print(\"Init:\", *[(name, param.shape)\n",
    "                        for name, param in m.named_parameters()][0])\n",
    "        ## 然后再调用均匀分布函数后的初始值\n",
    "        nn.init.uniform_(m.weight, -10, 10)\n",
    "        ## 调整均匀分布的参数值，符合5.2.1的分段初始化\n",
    "        m.weight.data *= m.weight.data.abs() >= 5\n",
    "        print('>=5设置后:',m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "origin_pos": 56,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init: weight torch.Size([8, 4])\n",
      ">=5设置后: tensor([[-0.0000,  0.0000, -6.7173, -6.1001],\n",
      "        [-0.0000,  0.0000,  5.8280,  8.7185],\n",
      "        [-6.5796,  0.0000, -9.2288,  9.7044],\n",
      "        [ 6.5312,  0.0000, -0.0000,  0.0000],\n",
      "        [-0.0000, -9.9691, -5.9393, -0.0000],\n",
      "        [ 0.0000,  0.0000,  8.8339,  5.7148],\n",
      "        [ 0.0000, -0.0000,  0.0000,  0.0000],\n",
      "        [ 7.9076, -9.9015,  7.7125,  0.0000]])\n",
      "Init: weight torch.Size([1, 8])\n",
      ">=5设置后: tensor([[-8.9822, -5.7516,  7.1305, -8.9823,  0.0000, -0.0000, -8.2990,  7.7230]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000,  0.0000, -6.7173, -6.1001],\n",
       "        [-0.0000,  0.0000,  5.8280,  8.7185]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.apply(my_init)\n",
    "net[0].weight[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 58
   },
   "source": [
    "- 注意，我们始终可以**直接设置参数**。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "origin_pos": 60,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42.0000,  1.0000, -5.7173, -5.1001])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data[:] += 1\n",
    "net[0].weight.data[0, 0] = 42\n",
    "net[0].weight.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 63
   },
   "source": [
    "## 5.2.3 参数绑定\n",
    "\n",
    "- 有时希望在多个层间共享参数：\n",
    "  - 先定义一个稠密层。\n",
    "  - 然后使用它的参数来设置另一个层的参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "origin_pos": 65,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2879],\n",
       "        [0.2987]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们需要给共享层一个名称，以便可以引用它的参数\n",
    "shared = nn.Linear(8, 8)\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                    shared, nn.ReLU(),##是一个新层，与上面一层参数完全相同\n",
    "                    shared, nn.ReLU(),##同上\n",
    "                    nn.Linear(8, 1))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "origin_pos": 65,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# 检查参数是否相同\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "net[2].weight.data[0, 0] = 100\n",
    "# 确保它们实际上是同一个对象，而不只是有相同的值\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 68,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "- 这个例子表明第三个和第五个神经网络层的**参数是绑定**的。\n",
    "- 它们不仅值相等，而且由相同的张量表示。\n",
    "- 因此，如果我们改变其中一个参数，另一个参数也会改变。\n",
    "- 当参数绑定时，梯度会发生什么情况？\n",
    "  - 答案是由于模型参数包含梯度，因此在反向传播期间第二个隐藏层即第三个神经网络层和第三个隐藏层即第五个神经网络层的梯度会加在一起。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **说明：共享参数**\n",
    "  - 上述代码将`shared`层（即nn.Linear(8, 8)）添加到神经网络中作为第三个和第五个神经网络层。由于这两个层实际上引用的是同一个`nn.Linear`对象，因此它们共享相同的权重和偏置参数。\n",
    "  - 当进行反向传播计算梯度时，梯度会在每一层上传递，并更新与之相关的权重参数。\n",
    "    - 对于这个特定的示例，第三个神经网络层（第二个隐藏层）的梯度与第五个神经网络层（第三个隐藏层）的梯度实际上是针对相同的权重和偏置参数计算的。\n",
    "    - 因此，它们的梯度会被累加在一起以更新这些共享参数。\n",
    "  - 这里的关键点是，由于`shared`层在多个位置使用（第三个和第五个神经网络层），所以在反向传播过程中，梯度会叠加，从而同时影响这两个位置的参数更新。\n",
    "  - 这意味着在训练期间，这两个层的参数始终保持相同，因为它们实际上是**同一个`nn.Linear`对象的实例**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1950369138832, 1950369138832)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 第3和第5个神经网络层实际上是同一个对象\n",
    "id(net[2]), id(net[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 激活层虽然也是一个层，但是其参数为空\n",
    "list(net[1].named_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 69
   },
   "source": [
    "## 小结\n",
    "\n",
    "* 我们有几种方法可以访问、初始化和绑定模型参数。\n",
    "* 我们可以使用自定义初始化方法。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
