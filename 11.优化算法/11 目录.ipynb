{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "origin_pos": 0
   },
   "source": [
    "# 11 优化算法\n",
    "\n",
    "\n",
    "- **目录**\n",
    "  - 11.1 优化和深度学习\n",
    "  - 11.2 凸性\n",
    "  - 11.3 梯度下降\n",
    "  - 11.4 随机梯度下降\n",
    "  - 11.5 小批量随机梯度下降\n",
    "  - 11.6 动量法\n",
    "  - 11.7 AdaGrad算法\n",
    "  - 11.8 RMSProp算法\n",
    "  - 11.9 Adadelta\n",
    "  - 11.10 Adam算法\n",
    "  - 11.11 学习率调度器\n",
    "<br><br>\n",
    "- **要点：**\n",
    "  - 在深度学习训练过程中，已经使用了各种优化算法来调整模型参数，并最小化损失函数。\n",
    "  - 优化算法被视为实现目标函数最优化的工具，常见算法包括**SGD（随机梯度下降）**和**Adam**。\n",
    "  - 优化算法对深度学习训练效率和模型性能至关重要，了解它们的工作原理可以帮助更有针对性地**调整超参数**。\n",
    "  - 深度学习优化问题大多是**非凸的**，但在凸问题上设计和分析优化算法有助于理解深度学习的优化问题。\n",
    "  - 本章将详细探讨常见的深度学习优化算法，并介绍凸优化基础及随机梯度下降在凸目标函数上的应用。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
