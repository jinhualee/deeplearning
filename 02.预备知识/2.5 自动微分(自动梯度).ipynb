{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 2.5 自动微分\n",
    "- **目录**\n",
    "  - 2.5.1 一个简单的例子\n",
    "  - 2.5.2 非标量变量的反向传播\n",
    "  - 2.5.3 分离计算\n",
    "  - 2.5.4 Python控制流的梯度计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "- 求导是几乎所有深度学习优化算法的关键步骤。\n",
    "- 深度学习框架通过自动计算导数，即**自动微分（automatic differentiation）** 来加快求导。\n",
    "- 在深度学习中，尤其在Pytorch等框架中，系统会根据用户设计的模型构建一个**计算图（computational graph）**，用于跟踪计算是哪些数据通过哪些操作组合起来并产生输出。\n",
    "\n",
    "- **反向传播（backpropagate）** 意味着跟踪整个计算图，填充关于每个参数的偏导数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "- **说明：何为计算图？**\n",
    "  - 计算图是一个有向无环图（DAG, Directed Acyclic Graph），用于表示计算过程。\n",
    "    - 图中的节点表示操作（如加法、乘法）或变量（如张量、权重），边表示操作的输入和输出关系。\n",
    "    - 在深度学习和自动微分框架中，计算图是理解模型计算以及梯度计算的核心概念。\n",
    "  - 计算图的基本概念：\n",
    "    - **节点（Nodes）**：\n",
    "      - **操作节点（Operation Nodes）**：表示各种数学操作，如加法、乘法、矩阵乘法、激活函数等。\n",
    "      - **变量节点（Variable Nodes）**：表示输入数据、模型参数、常数等。\n",
    "    - **边（Edges）**：\n",
    "      - 表示数据在节点之间的流动，即操作的输入和输出关系。\n",
    "  - 计算图的构建过程：\n",
    "    - **定义输入**：\n",
    "      - 将输入变量定义为计算图中的节点。例如，输入数据和模型参数。\n",
    "    - **定义操作**：\n",
    "      - 通过对输入变量进行各种操作（如加法、乘法等），构建计算图。这些操作节点会连接到相应的输入变量节点上。\n",
    "    - **计算输出**：\n",
    "      - 最终的输出节点通过一系列操作节点和输入节点相连，形成一个完整的计算图。\n",
    "  - 计算图的功能\n",
    "    - **前向传播（Forward Pass）**：\n",
    "      - 通过计算图从输入节点开始依次计算每个操作节点，直到得到最终的输出。前向传播用于计算模型的预测结果或损失。\n",
    "    - **反向传播（Backward Pass）**：\n",
    "      - 通过计算图从输出节点开始依次向前计算每个操作节点的梯度，直到得到输入节点的梯度。\n",
    "      - 反向传播用于计算模型参数的梯度，以便进行梯度下降优化。\n",
    "\n",
    "  - 计算图的类型\n",
    "    - **静态计算图（Static Computation Graph）**：\n",
    "      - 在模型运行之前，计算图是固定的，不能动态改变。例如，TensorFlow的静态图模式。\n",
    "    - **动态计算图（Dynamic Computation Graph）**：\n",
    "      - 计算图在每次前向传播时动态生成，可以根据需要进行修改。例如，PyTorch和TensorFlow的Eager Execution模式。\n",
    "\n",
    "  - 一个简单的计算图示例，用于计算以下表达式的前向传播和反向传播过程：\n",
    "    $$ z = (x + y) \\times w $$\n",
    "  - 前向传播：\n",
    "    - **定义输入**：$x$, $y$, $w$ 为输入变量节点。\n",
    "    - **定义操作**：\n",
    "      - 加法操作：$a = x + y$\n",
    "      - 乘法操作：$z = a \\times w$\n",
    "  - 计算图如下：\n",
    "```\n",
    "  x   y  w\n",
    "   \\ /   |\n",
    "    +    |\n",
    "   / \\   |\n",
    "  a   \\  |\n",
    "       \\ |\n",
    "        * \n",
    "        |\n",
    "        z\n",
    "```\n",
    "  - 反向传播，计算每个变量的梯度：\n",
    "    - **输出节点的梯度**：\n",
    "      - $\\frac{\\partial z}{\\partial z} = 1$\n",
    "    - **乘法节点的梯度**：\n",
    "      - $\\frac{\\partial z}{\\partial a} = w$\n",
    "      - $\\frac{\\partial z}{\\partial w} = a$\n",
    "    - **加法节点的梯度**：\n",
    "      - $\\frac{\\partial a}{\\partial x} = 1$\n",
    "      - $\\frac{\\partial a}{\\partial y} = 1$\n",
    "    - 通过链式法则，计算每个输入变量的梯度：\n",
    "      - $\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial a} \\cdot \\frac{\\partial a}{\\partial x} = w \\cdot 1 = w$\n",
    "      -  $\\frac{\\partial z}{\\partial y} = \\frac{\\partial z}{\\partial a} \\cdot \\frac{\\partial a}{\\partial y} = w \\cdot 1 = w$\n",
    "      - $\\frac{\\partial z}{\\partial w} = a = x + y$\n",
    "\n",
    "-------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "## 2.5.1 一个简单的例子\n",
    "\n",
    "- **对函数$y=2\\mathbf{x}^{\\top}\\mathbf{x}$关于列向量$\\mathbf{x}$求导**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "#创建变量`x`并为其分配一个初始值。\n",
    "x = torch.arange(4.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "- 深度学习一般会大规模更新相同的参数（GPT-3模型参数规模达1750亿）。\n",
    "- 每次都分配新的内存可能很快就会将内存耗尽。\n",
    "- 因此深度学习参数更新（尤其是权重更新）一般采用**就地操作（in-place）**。\n",
    "- 注意，一个**标量函数关于向量$\\mathbf{x}$的梯度是向量，并且与$\\mathbf{x}$具有相同的形状**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "x.requires_grad_(True)  # 等价于x=torch.arange(4.0,requires_grad=True)\n",
    "x.grad  # 默认值是None,即梯度为空"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在Tensor上调用`backward()`计算导数。\n",
    "- 如果 Tensor 是一个标量(即它包含一个元素的数据），则不需要为` backward() `指定任何参数。\n",
    "- 如果该Tensor有多个元素(比如向量或矩阵)，则需要指定一个`gradient` 参数，该参数是形状匹配的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "origin_pos": 10,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算y\n",
    "y = 2 * torch.dot(x, x)\n",
    "# 此时的y是一个标量，即只有一个值\n",
    "y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "- `x`是一个长度为4的向量，计算`x`和`x`的点积得到的`y`是一个的标量。\n",
    "- 然后**通过调用反向传播函数来自动计算`y`关于`x`每个分量的梯度**，并打印这些梯度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "origin_pos": 14,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "# 反向传播函数自动计算梯度，由于此时的y是一个标量，因此可以计算梯度\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 即y=2x^2,那么y'=4x，那么此时y关于x每个分量的梯度为4*[0,1,2,3]\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 16
   },
   "source": [
    "- 函数$y=2\\mathbf{x}^{\\top}\\mathbf{x}$关于$\\mathbf{x}$的梯度应为$4\\mathbf{x}$。\n",
    "- 根据此公式验证手工计算的梯度和上述反向传播计算后的梯度是否一致。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "origin_pos": 18,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 20
   },
   "source": [
    "- 计算`x`的另一个函数，然后计算梯度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "origin_pos": 22,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1.]), torch.Tensor, tensor(6., grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值\n",
    "x.grad.zero_() #清除梯度\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "x.grad,type(y), y # 此时的y也是一个标量，但是为何这么操作?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 2., 4., 6.]), tensor([0., 1., 4., 9.], grad_fn=<PowBackward0>))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#清除梯度\n",
    "x.grad.zero_()\n",
    "y=x**2\n",
    "##为何求和？主要是为了使y成为一个变量。可以参考这个帖子：https://blog.csdn.net/qq_39208832/article/details/117415229\n",
    "#解释更透彻的是这个帖子：https://zhuanlan.zhihu.com/p/27808095\n",
    "y.sum().backward()\n",
    "x.grad,y #仍是前面那个求和后的y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **说明**：在对函数求梯度时，**是否要求因变量是标量**？\n",
    "  - 在对函数求梯度时，**通常要求因变量是标量**。\n",
    "    - 梯度的定义和计算涉及到一个标量值函数（单一输出）关于其自变量的偏导数。\n",
    "    - 当因变量是标量时，可以计算该函数**在某个点处关于各个自变量分量的偏导数，并将它们组合成梯度向量**。\n",
    "  - 如果因变量不是标量（即函数有多个输出），那么**通常会针对每个输出分别计算梯度**。\n",
    "    - 在这种情况下，可以使用雅可比矩阵来表示所有输出关于自变量的梯度信息。\n",
    "    - 雅可比矩阵的第$i$ 行、第 $j$ 列元素表示第 $i$ 个输出关于第 $j$ 个自变量分量的偏导数：\n",
    "      $$J[i, j] = ∂f_i/∂x_j$$\n",
    "      其中 $f_i$ 是第 $i$ 个输出，$x_j$ 是第 $j$ 个自变量分量。\n",
    "  - 总结一下：在对函数求梯度时，通常要求因变量是标量。如果函数有多个输出，可以针对每个输出分别计算梯度，并使用雅可比矩阵表示所有梯度信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian matrix:\n",
      "tensor([[ 2.,  3.,  2.,  0.],\n",
      "        [ 0.,  4., 16., 26.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def f(u, v):\n",
    "    y = torch.zeros(2)\n",
    "    y[0] = u[0]**2 + u[1]*v[0]\n",
    "    y[1] = u[1]*v[1] + v[0]*v[1]**2\n",
    "    return y\n",
    "\n",
    "# 创建自变量张量\n",
    "u = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "v = torch.tensor([3.0, 4.0], requires_grad=True)\n",
    "\n",
    "# 计算输出向量，y本质上就是有个两个函数组成的向量而已\n",
    "# 计算梯度时分别针对y中的每个函数y[0],y[1]计算关于u,v的梯度\n",
    "y = f(u, v)\n",
    "\n",
    "# 初始化雅可比矩阵\n",
    "jacobian = torch.zeros(2, 4)\n",
    "\n",
    "# 计算雅可比矩阵的每个元素\n",
    "for i in range(2):\n",
    "    # 对 y[i] 进行反向传播以计算梯度\n",
    "    y[i].backward(retain_graph=True)\n",
    "    \n",
    "    # 提取梯度并存储在雅可比矩阵中\n",
    "    jacobian[i, :2] = u.grad\n",
    "    jacobian[i, 2:] = v.grad\n",
    "\n",
    "    # 清零梯度，以便进行下一次迭代\n",
    "    u.grad.zero_()\n",
    "    v.grad.zero_()\n",
    "\n",
    "print(\"Jacobian matrix:\")\n",
    "print(jacobian)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 上述代码的输出结果可解释为：\n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}{cccc} \n",
    "    ∂y[0]/∂u[0] = 2  & ∂y[0]/∂u[1] = 3  & ∂y[0]/∂v[0] = 2 &  ∂y[0]/∂v[1] = 0\\\\ \n",
    "    ∂y[1]/∂u[0] = 0  &  ∂y[1]/∂u[1] = 4   & ∂y[1]/∂v[0] = 16 & ∂y[1]/∂v[1] = 26\\\\ \n",
    "\\end{array}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 24
   },
   "source": [
    "## 2.5.2 非标量变量的反向传播\n",
    "\n",
    "- 当`y`不是标量时，向量`y`关于向量`x`的导数的最自然解释是一个矩阵，即**雅克比矩阵**。\n",
    "- 对于高阶和高维的`y`和`x`，求导的结果可以是一个<b>高阶张量。\n",
    "- 然而，虽然这些更奇特的对象(即作为求导结果的高阶张量)确实出现在高级机器学习中（包括深度学习中），但当调用向量的反向传播计算时，通常会试图计算一批训练样本中每个组成部分的损失函数的导数。\n",
    "- **这样做的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和**。\n",
    "- 对上述这句话以及下述代码注释的理解：\n",
    "  - 原代码中这段注释不好理解，而且例子也没有体现出为什么要求偏导数的和，因为此处的x只有一条样本数据。\n",
    "如果x是类似[[0, 1, 2, 3],[6, 7, 8, 9]]这样的数据，这样小批量样本中就有2条数据。\n",
    "  - 原注释中的\"只想求偏导数的和\"是指：\n",
    "    - 分别针对[0, 1, 2, 3]和[6, 7, 8, 9]两条数据计算偏导数，得出两个梯度向量：\n",
    "    - [0., 2., 4., 6.]和[12, 14, 16, 18]，然后再将二者按元素相加求和得到[12， 16， 20， 24]。\n",
    "    - 最后将这个向量作为某个小批量样本数据的总体梯度。\n",
    "    - 再将这个梯度作为后续工作的基础，比如用于更新权重等参数。\n",
    "  - 这种做法就是后面内容中的优化算法——小批量随机梯度下降的做法。\n",
    "  - 所谓“所以传递一个1的梯度是合适的.”这句话是指将梯度权重因子设为1，即将梯度乘以权重1，也就是不改变梯度的计算结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "origin_pos": 26,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。\n",
    "# 在我们的例子中，我们只想求偏导数的和，所以传递一个1的梯度是合适的.\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "# 注意此处没有调用y.sum(),那么需指定backward函数的gradient参数（即梯度权重因子）的值\n",
    "# 该参数形状与x相同\n",
    "y.backward(torch.ones(len(x)))\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y的值\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 2., 4., 6.]), tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 和上述代码的计算结果是一致的\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "# 等价于y.backward(torch.ones(len(x)))\n",
    "y.sum().backward()\n",
    "x.grad, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **说明**：`y.sum().backward()`与`y.backward(torch.ones(len(x)))`区别与联系\n",
    "  - $\\mathbf x$ 向量可以看成向量$[x_1, x_2, x_3, x_4]$，分别对应$[0., 1., 2., 3.]$，`y.sum()`对应$y = x_1^2 + x_2^2 + x_3^2 + x_4^2$，然后调用`backward()`计算梯度，本质上就是针对$y$分别计算关于$x_1, x_2, x_3, x_4$的偏导数，即$[2x_1,2x_2, 2x_3, 2x_4]$。代入$[0., 1., 2., 3.]$，偏导数值为：`[0., 2., 4., 6.]`\n",
    "  - `y.backward(torch.ones(len(x)))`函数如果不指定`gradient`参数则会报错。如果指定该参数比如此处的`torch.ones(len(x))`即`[1, 1, 1, 1]`，本质上是一个权重因子，用于调整每个$\\mathbf x$中每个分量的梯度值，即在计算了`y`关于每个分量的梯度之后，需要将梯度与对应的权重因子相乘，然后作为梯度保存。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- **示例：**\n",
    "  - `y.mean().backward()`：`y.mean()`对应$y = {\\frac14}{(x_1^2 + x_2^2 + x_3^2 + x_4^2)}$，然后调用`backward()`计算梯度，本质上就是针对$y$分别计算关于$x_1, x_2, x_3, x_4$的偏导数，即$[ {\\frac12}x_1, {\\frac12}x_2,  {\\frac12}x_3,  {\\frac12}x_4]$。代入$[0., 1., 2., 3.]$，那么偏导数值即为：`[0., 0.5, 1.,1.5]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.5000, 1.0000, 1.5000])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "y.mean().backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **示例：**\n",
    "  - `y.backward`指定`gradient`参数的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  6., 12., 18.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "# 设定x每个分量的梯度权重因子值为3\n",
    "y.backward(gradient=torch.tensor([3, 3, 3, 3]))\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- 注意：\n",
    "  - 再次强调，在`backward()`函数的上下文中，`gradient` 参数实际上是梯度因子，而不是梯度本身，在此处作为权重因子参与梯度计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 28
   },
   "source": [
    "## 2.5.3 分离计算\n",
    "\n",
    "- 所谓**分离计算**就是将某些计算移动到计算图之外，也就是在计算图中的某处**断开自动求导的传播**。\n",
    "- 例如，假设`y`是作为`x`的函数计算的，而`z`则是作为`y`和`x`的函数计算的。想计算`z`关于`x`的梯度，但由于某种原因希望将`y`视为一个常数，并且只考虑到`x`在`y`被计算后发挥的作用。\n",
    "- 此时，可以分离`y`来返回一个新变量`u`，该变量与`y`具有相同的值，但丢弃计算图中如何计算`y`的任何信息。\n",
    "- 换句话说，梯度不会向后流经`u`到`x`。\n",
    "- 下例就是利用反向传播函数计算`z=u*x`关于`x`的偏导数，同时将`u`作为常数处理，而不是`z=x*x*x`关于`x`的偏导数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "origin_pos": 30,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([True, True, True, True]),\n",
       " tensor([0., 1., 4., 9.]),\n",
       " tensor([0., 1., 4., 9.]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_() # 梯度清零\n",
    "y = x * x # y = x^2\n",
    "\n",
    "## 调用detach，将y剥离梯度计算，只是将其算术计算的结果赋给y\n",
    "# 也就是x*x的计算结果，此处u等于[0,1,2,3]*[0,1,2,3] = [0,1,4,9]\n",
    "u = y.detach() \n",
    "z = u * x # 如果对其求梯度，x的梯度即u\n",
    "\n",
    "z.sum().backward() #此处只求x的梯度，梯度就是u的值[0,1,4,9]\n",
    "x.grad == u,u,x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 36
   },
   "source": [
    "## 2.5.4 Python控制流的梯度计算\n",
    "\n",
    "- **即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），仍然可以计算得到的变量的梯度**。\n",
    "- **示例：**\n",
    "  - 在下面的代码中，`while`循环的迭代次数和`if`语句的结果都取决于输入`a`的值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "origin_pos": 38,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 40
   },
   "source": [
    "- 计算梯度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "origin_pos": 42,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2048.),\n",
       " tensor(0.8392, requires_grad=True),\n",
       " tensor(1718.6700, grad_fn=<MulBackward0>),\n",
       " tensor(2048., grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a是一个随机数，因此每次运行下列代码，a的值及其梯度可能不同\n",
    "a = torch.randn(size=(), requires_grad=True)\n",
    "d = f(a)\n",
    "d.backward()\n",
    "a.grad, a, d, d/a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-5.), tensor(5.))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 标量的范数\n",
    "t = torch.tensor(-5.0)\n",
    "t,t.norm() # 标量的范数等于其绝对值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 44
   },
   "source": [
    "- 上述代码中梯度的计算过程：\n",
    "  - 调用 `d.backward()`。这将触发关于 a 的梯度计算。根据链式法则，需要计算 $dc/da$。由于`c` 的值取决于 `b` 和条件语句，因此我们需要根据不同情况来计算梯度。\n",
    "  - 如果 `b.sum() > 0` 成立，那么 `c = b`，并且 $dc/db = 1$。在这种情况下，需要计算 $db/da$。由于 `b` 是通过重复乘以 `2` 计算的，可以表示为 $b = a * 2^n$（其中 $n$ 是满足`b.norm() >= 1000` 的最小整数）。因此，$db/da = 2^n$。最后，根据链式法则，$dc/da = dc/db * db/da = 1 * 2^n = 2^n$。\n",
    "  - 如果`b.sum() <= 0`成立，那么`c = 100 * b`，并且 $dc/db = 100$。在这种情况下，还是需要计算$db/da$，它的值仍然是 $2^n$。\n",
    "  - 最后，根据链式法则，$dc/da = dc/db * db/da = 100 * 2^n$。\n",
    "  - 计算完成后，`a.grad` 存储了关于`a` 的梯度值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 对于任何a，存在某个常量标量k，使得f(a)=k*a，其中k的值取决于输入a，因此可以用d/a验证梯度是否正确。\n",
    "  - 也就是说：不管是迭代还是if分支流程的控制，函数f(a)本质上就是关于a的乘法运算，即d=f(a)=k*a，那么其梯度就是k，即等于d/a。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "origin_pos": 46,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad == d / a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  计算z = (x + y)* w表达式的前向传播和反向传播过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = x + y: 5.0\n",
      "z = a * w: 20.0\n",
      "dz/dx: 4.0\n",
      "dz/dy: 4.0\n",
      "dz/dw: 5.0\n"
     ]
    }
   ],
   "source": [
    "# 计算z = (x + y)* w的计算图\n",
    "import torch\n",
    "\n",
    "# 定义输入变量，并设置requires_grad=True以便计算梯度\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "w = torch.tensor(4.0, requires_grad=True)\n",
    "\n",
    "# 前向传播\n",
    "a = x + y\n",
    "z = a * w\n",
    "\n",
    "# 打印前向传播的结果\n",
    "print(f\"a = x + y: {a.item()}\")\n",
    "print(f\"z = a * w: {z.item()}\")\n",
    "\n",
    "# 反向传播，计算梯度\n",
    "z.backward()\n",
    "\n",
    "# 打印各个变量的梯度\n",
    "print(f\"dz/dx: {x.grad.item()}\")\n",
    "print(f\"dz/dy: {y.grad.item()}\")\n",
    "print(f\"dz/dw: {w.grad.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 48
   },
   "source": [
    "## 小结\n",
    "\n",
    "* 深度学习框架可以自动计算导数：首先将梯度附加到想要对其计算偏导数的变量上。\n",
    "* 然后记录目标值的计算，执行它的反向传播函数，并访问得到的梯度。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
