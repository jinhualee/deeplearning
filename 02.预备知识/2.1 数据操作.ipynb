{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 2.1 数据操作\n",
    "- **目录**\n",
    "  - 2.1.1 入门\n",
    "  - 2.1.2 运算符\n",
    "  - 2.1.3 广播机制\n",
    "  - 2.1.4 索引和切片\n",
    "  - 2.1.5 节省内存\n",
    "  - 2.1.6 转换为其他Python对象\n",
    "  - 2.1.7 维度重新排列与轴变换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "- $n$维数组，也称为**张量（tensor）**。\n",
    "  - 在PyTorch和TensorFlow中为`Tensor`，二者都与Numpy的`ndarray`类似。\n",
    "\n",
    "  - GPU很好地支持PyTorch和TensorFlow张量的加速计算，而NumPy仅支持CPU计算；\n",
    "\n",
    "- 张量类支持<b>自动微分。\n",
    "  - 张量类的自动微分功能非常有利于深度学习建模。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "## 2.1.1 入门\n",
    "\n",
    "本教程所使用的基本数值计算工具。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "- **导入`torch`，代码中使用`torch`而不是`pytorch`。**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "origin_pos": 5,
    "scrolled": true,
    "tab": [
     "pytorch"
    ],
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.2+cu121'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 7
   },
   "source": [
    "- **张量表示由一个数值组成的数组，这个数组可能有多个维度**。\n",
    "  - 具有一个轴的张量对应数学上的**向量（vector）**。\n",
    "  - 具有两个轴的张量对应数学上的**矩阵（matrix）**。\n",
    "  - 具有两个轴以上的张量没有特殊的数学名称。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 9,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "- 使用 `arange` 创建一个行向量 `x`。\n",
    "  - 默认创建的数据类型为整数，可指定创建类型为其他类型比如：浮点数。\n",
    "  - 张量中的每个值都称为张量的 **元素（element）**。\n",
    "  - 张量创建后，**默认将存储在机器的内存中，使用CPU计算。**\n",
    "  - 可以通过参数指定存储在GPU中，并进行计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ],
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# arange函数创建维度为12的行向量 \n",
    "x = torch.arange(12)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 14
   },
   "source": [
    "- 可以通过张量的`shape`属性来访问张量（沿每个轴的长度）的**形状**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "origin_pos": 15,
    "tab": [
     "pytorch"
    ],
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 16
   },
   "source": [
    "- 如果只想知道张量中元素的总数可以使用numel()。\n",
    "- size函数得出张量的形状或尺寸，结果和shape属性一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "origin_pos": 18,
    "tab": [
     "pytorch"
    ],
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, torch.Size([12]), torch.Size([12]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numel(),x.shape,x.size() # shape属性与size函数的结果一致。注意：前者是张量属性；后者是pytorch函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 20
   },
   "source": [
    "- **改变张量的形状：reshape函数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "origin_pos": 21,
    "tab": [
     "pytorch"
    ],
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]]),\n",
       " torch.Size([3, 4]),\n",
       " torch.Size([3, 4]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 张量x从形状为（12,）的行向量转换为形状为（3,4）的矩阵\n",
    "X = x.reshape(3, 4)\n",
    "X,X.shape,X.size() # shape属性与size函数的结果一致"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 23
   },
   "source": [
    "- **使用参数`-1`自动计算张量维度**\n",
    "  - 可以用`x.reshape(-1,4)`或`x.reshape(3,-1)`来取代`x.reshape(3,4)`。\n",
    "  - (-1, 4)：-1所在轴的维度为12/4 = 3，reshape后的形状为(3, 4)。\n",
    "  - (3, -1)：-1所在轴的维度为12/3 = 4，reshape后的形状也是(3, 4)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 4]), torch.Size([3, 4]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1 = x.reshape(-1, 4)\n",
    "X2 = x.reshape(3, -1)\n",
    "X1.shape, X2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 23
   },
   "source": [
    "- **使用全0、全1、其他常量，或者从特定分布中随机采样的数字**来初始化矩阵。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "origin_pos": 25,
    "tab": [
     "pytorch"
    ],
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个形状为（2,3,4）的张量，其中所有元素都设置为0\n",
    "torch.zeros((2, 3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "origin_pos": 29,
    "tab": [
     "pytorch"
    ],
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个形状为(2,3,4)的张量，其中所有元素都设置为1\n",
    "torch.ones((2, 3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "origin_pos": 33,
    "tab": [
     "pytorch"
    ],
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8228, -0.5454,  0.0907, -0.7303],\n",
       "        [ 0.7852, -0.9472, -0.4773,  1.1415],\n",
       "        [-0.5561,  1.2983,  2.2317, -1.9276]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过从某个特定的概率分布中随机采样来得到张量中每个元素的值\n",
    "# 每个元素都从均值为0、标准差为1的标准高斯分布（正态分布）中随机采样\n",
    "torch.randn(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 35
   },
   "source": [
    "- **通过提供包含数值的Python列表（或嵌套列表），来为深度学习模型张量的每个元素赋予确定值**。\n",
    "  - 最外层的列表list对应于轴0（**垂直方向**，维度为3），内层的list对应于轴1（**水平方向**，维度为4）。\n",
    "  - 轴的方向很重要，后面很多深度学习模型的张量计算过程都涉及到，要牢固掌握。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "origin_pos": 37,
    "tab": [
     "pytorch"
    ],
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 1, 4, 3],\n",
       "        [1, 2, 3, 4],\n",
       "        [4, 3, 2, 1]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 个人感觉此种直接生成张量的方法比TensorFlow更方便\n",
    "torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8],\n",
       "         [ 9, 10, 11, 12]],\n",
       "\n",
       "        [[13, 14, 15, 16],\n",
       "         [17, 18, 19, 20],\n",
       "         [21, 22, 23, 24]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(1,25).reshape(2,3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- reshape函数的两种调用方式以及和numpy的reshape函数的比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1,  2,  3,  4],\n",
       "          [ 5,  6,  7,  8],\n",
       "          [ 9, 10, 11, 12]],\n",
       " \n",
       "         [[13, 14, 15, 16],\n",
       "          [17, 18, 19, 20],\n",
       "          [21, 22, 23, 24]]]),\n",
       " tensor([[[ 1,  2,  3,  4],\n",
       "          [ 5,  6,  7,  8],\n",
       "          [ 9, 10, 11, 12]],\n",
       " \n",
       "         [[13, 14, 15, 16],\n",
       "          [17, 18, 19, 20],\n",
       "          [21, 22, 23, 24]]]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = torch.arange(1,25)\n",
    "n.reshape(2,3,4), torch.reshape(n,(2,3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ 1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8],\n",
       "         [ 9, 10, 11, 12]],\n",
       " \n",
       "        [[13, 14, 15, 16],\n",
       "         [17, 18, 19, 20],\n",
       "         [21, 22, 23, 24]]]),\n",
       " array([[[ 1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8],\n",
       "         [ 9, 10, 11, 12]],\n",
       " \n",
       "        [[13, 14, 15, 16],\n",
       "         [17, 18, 19, 20],\n",
       "         [21, 22, 23, 24]]]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "t = np.arange(1,25)\n",
    "# 二者得出的结果一样，但是要注意t.reshape是就地修改t的形状，np.reshape是返回一个新对象\n",
    "t.reshape(2,3,4),np.reshape(t,(2,3,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 39
   },
   "source": [
    "## 2.1.2 运算符\n",
    "\n",
    "- 最简单且最有用的操作是**按元素（elementwise）** 运算。\n",
    "  - 按元素运算将二元运算符应用于两个数组中的每对位置对应的元素。\n",
    "  - 可以基于任何从标量到标量的函数来创建按元素函数。\n",
    "  - 常见的标准算术运算符 **（`+`、`-`、`*`、`/`和`**`）** 都可以被升级为按元素运算。\n",
    "  - 按元素操作**一般**要求两个张量的形状相同。\n",
    "  - 某些情况下可以通过**广播机制**对两个形状不同的张量进行按元素运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "origin_pos": 41,
    "tab": [
     "pytorch"
    ],
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3.,  4.,  6., 10.]),\n",
       " tensor([-1.,  0.,  2.,  6.]),\n",
       " tensor([ 2.,  4.,  8., 16.]),\n",
       " tensor([0.5000, 1.0000, 2.0000, 4.0000]),\n",
       " tensor([ 1.,  4., 16., 64.]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2, 4, 8])\n",
    "y = torch.tensor([2, 2, 2, 2])\n",
    "x + y, x - y, x * y, x / y, x ** y  # **运算符是幂运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "origin_pos": 45,
    "tab": [
     "pytorch"
    ],
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对自然对数的底数e求x次幂\n",
    "torch.exp(x) # 也是按元素计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 47
   },
   "source": [
    "- **多个张量的连结（concatenate）运算**\n",
    "  - 多个张量的端对端地叠起来形成一个更大的张量。\n",
    "  - 提供张量列表，并指定沿哪个轴连结。\n",
    "\n",
    "- **示例：**\n",
    "  - 第一个输出张量的轴-0长度（$6$）是两个输入张量轴-0长度的总和（$3 + 3$）；\n",
    "  - 第二个输出张量的轴-1长度（$8$）是两个输入张量轴-1长度的总和（$4 + 4$）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "origin_pos": 49,
    "tab": [
     "pytorch"
    ],
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [ 2.,  1.,  4.,  3.],\n",
       "         [ 1.,  2.,  3.,  4.],\n",
       "         [ 4.,  3.,  2.,  1.]]),\n",
       " tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
       "         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "# 分别沿行（轴-0，垂直方向）和按列（轴-1，水平方向）连结两个矩阵\n",
    "torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 51
   },
   "source": [
    "- **通过逻辑运算符构建二元张量**\n",
    "- 以`X == Y`为例：\n",
    "  - 对于每个位置，如果`X`和`Y`在该位置相等，则新张量中相应项的值为`True`或`1`。\n",
    "  - 否则，该位置为`False`或`0`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "origin_pos": 52,
    "tab": [
     "pytorch"
    ],
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True, False,  True],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 按元素进行逻辑运算，结果是布尔值\n",
    "X == Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, False, False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "True == 1, False == 0, True == 0, False == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(bool, int)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 很诡异的行为：True和1类型不同，但是==运算表明二者完全相等\n",
    "type(True), type(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 53
   },
   "source": [
    "- **对张量中的所有元素进行求和**\n",
    "  - `sum`函数如果不带参数，则对所有元素求和。\n",
    "  - 指定`dim`或`axis`参数则表示的对哪个轴或方向上求和。\n",
    "  - 个人感觉，Pytorch里使用`dim`作为参数名较多，Numpy里一般使用`axis`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "origin_pos": 54,
    "tab": [
     "pytorch"
    ],
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(66.), tensor([12., 15., 18., 21.]), tensor([ 6., 22., 38.]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 注意0和1表示求和的维度或方向，\n",
    "# “方向”理解起来要更直观一点。\n",
    "X.sum(),X.sum(dim = 0),X.sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **小测试**：现有一个3维张量，调用sum函数，当设置dim等于0,1,2时，各自的计算结果是什么？\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8],\n",
       "         [ 9, 10, 11, 12]],\n",
       "\n",
       "        [[13, 14, 15, 16],\n",
       "         [17, 18, 19, 20],\n",
       "         [21, 22, 23, 24]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_3 = torch.arange(1,25).reshape((2,3,4))\n",
    "X_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[14, 16, 18, 20],\n",
       "         [22, 24, 26, 28],\n",
       "         [30, 32, 34, 36]]),\n",
       " tensor([[15, 18, 21, 24],\n",
       "         [51, 54, 57, 60]]),\n",
       " tensor([[10, 26, 42],\n",
       "         [58, 74, 90]]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 思考下列张量运算\n",
    "X_3.sum(dim=0),X_3.sum(dim=1),X_3.sum(dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 56
   },
   "source": [
    "## 2.1.3 广播机制\n",
    "- 即使形状不同，仍可通过**广播机制（broadcasting mechanism）来执行按元素操作**:\n",
    "  - 首先，通过适当**复制元素**来**扩展**一个或两个数组，以便在转换之后，两个张量具有**相同**的形状。\n",
    "  - 其次，对生成的数组执行按元素操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **示例：**\n",
    "  - `x1`的形状为${(4, )}$, `y1`的形状为${(3, 4)}$。\n",
    "  - `x1`会在垂直方向进行广播，变成${[[1, 2, 3, 4],[1, 2, 3, 4],[1, 2, 3, 4]]}$。\n",
    "  - 广播后的`x1`与`y1`按元素相乘。\n",
    "  - `x2`的形状为${(3, 1)}$，然后进行水平方向广播，变成${[[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3]]}$,然后与`y1`按元素相乘。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 2, 3, 4]]),\n",
       " tensor([[1],\n",
       "         [2],\n",
       "         [3]]),\n",
       " tensor([[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]]),\n",
       " torch.Size([1, 4]),\n",
       " torch.Size([3, 1]),\n",
       " torch.Size([3, 4]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x1 = torch.tensor([1,2,3,4]) # 形状为(4,)，也可以进行纵向传播\n",
    "x1 = torch.tensor([1,2,3,4]).reshape(1, 4) # 形状(1, 4)\n",
    "x2 = torch.tensor([1,2,3]).reshape(3, -1) # 形状(3, 1)\n",
    "y1 = torch.arange(0,12).reshape(3, 4)\n",
    "\n",
    "x1, x2, y1, x1.shape, x2.shape, y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  2,  6, 12],\n",
       "         [ 4, 10, 18, 28],\n",
       "         [ 8, 18, 30, 44]]),\n",
       " tensor([[ 0,  1,  2,  3],\n",
       "         [ 8, 10, 12, 14],\n",
       "         [24, 27, 30, 33]]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 广播后按元素相乘\n",
    "x1*y1, x2*y1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 60
   },
   "source": [
    "- **示例：**  \n",
    "  - `a`和`b`分别是$3\\times1$和$1\\times2$矩阵。\n",
    "  - 矩阵`a`将复制列(按照`b`的列数复制)，矩阵`b`将复制行（按照`a`的行数复制）。\n",
    "  - 两个矩阵**广播**为一个更大的$3\\times2$矩阵。\n",
    "  - 再按元素相加。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "origin_pos": 58,
    "tab": [
     "pytorch"
    ],
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [1],\n",
       "         [2]]),\n",
       " tensor([[0, 1]]))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 另一个例子\n",
    "a = torch.arange(3).reshape((3, 1))\n",
    "b = torch.arange(2).reshape((1, 2))\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "origin_pos": 61,
    "tab": [
     "pytorch"
    ],
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [1, 2],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将a, b各自广播成(3, 2)的形状后按元素相加求和\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1]]),\n",
       " tensor([[0, 0],\n",
       "         [1, 1],\n",
       "         [2, 2]]),\n",
       " tensor([[0, 1],\n",
       "         [1, 2],\n",
       "         [2, 3]]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a, b张量被广播过程\n",
    "# cat函数的dim和axis似乎为同一个参数，功能相同\n",
    "a1 = torch.cat([b,b,b],axis = 0) # axis = 0表示垂直方向广播\n",
    "b1 = torch.cat([a,a],dim = 1)  # axis = 1表示水平方向广播\n",
    "a1, b1, a1 + b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **小测试：** a1的形状为(3,4,2), b1的形状为(4,1)，a1+b1运算如何广播？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1,  2],\n",
       "          [ 3,  4],\n",
       "          [ 5,  6],\n",
       "          [ 7,  8]],\n",
       " \n",
       "         [[ 9, 10],\n",
       "          [11, 12],\n",
       "          [13, 14],\n",
       "          [15, 16]],\n",
       " \n",
       "         [[17, 18],\n",
       "          [19, 20],\n",
       "          [21, 22],\n",
       "          [23, 24]]]),\n",
       " tensor([[4],\n",
       "         [5],\n",
       "         [6],\n",
       "         [7]]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1 = torch.arange(1,25).reshape(3,4,2)\n",
    "b1 = torch.arange(4,8).reshape(4,1)\n",
    "a1,b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 5,  6],\n",
       "         [ 8,  9],\n",
       "         [11, 12],\n",
       "         [14, 15]],\n",
       "\n",
       "        [[13, 14],\n",
       "         [16, 17],\n",
       "         [19, 20],\n",
       "         [22, 23]],\n",
       "\n",
       "        [[21, 22],\n",
       "         [24, 25],\n",
       "         [27, 28],\n",
       "         [30, 31]]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (3,4,2)形状的张量与(4,1)形状的张量进行运算如何广播？\n",
    "a1+b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 62
   },
   "source": [
    "## 2.1.4 索引和切片\n",
    "与Python的数组或list切片以及Numpy的切片机制基本一致：\n",
    "  - 第一个元素的索引是0，最后一个元素索引是-1；\n",
    "  - 指定范围以包含第一个元素和最后一个元素之前的元素。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "origin_pos": 63,
    "tab": [
     "pytorch"
    ],
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 8.,  9., 10., 11.]),\n",
       " tensor([[ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[-1], X[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 64,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "- **通过指定索引来将元素写入矩阵**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "origin_pos": 66,
    "tab": [
     "pytorch"
    ],
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  9.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1, 2] = 9\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 68
   },
   "source": [
    "- **索引指定多个元素，赋相同值**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "origin_pos": 69,
    "tab": [
     "pytorch"
    ],
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12., 12., 12., 12.],\n",
       "        [12., 12., 12., 12.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:2, :] = 12\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 71
   },
   "source": [
    "## 2.1.5 节省内存\n",
    "\n",
    "- **运行一些操作可能导致内存分配，从而增加内存消耗**。\n",
    "- 解决方法之一是使用**就地操作**(In-place operations) 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "origin_pos": 72,
    "tab": [
     "pytorch"
    ],
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140706189468744, 1322622047664, False)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用Y = X + Y，我们将取消引用Y指向的张量，而是指向新分配的内存处的张量。\n",
    "Y = 10\n",
    "before = id(Y)\n",
    "Y = Y + X\n",
    "# Y的内存地址发生变化，即分配了新的内存\n",
    "before, id(Y), id(Y) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 73
   },
   "source": [
    "- 注意：\n",
    "  - 尽量避免不必要地分配内存。在机器学习中，参数的size动辄数百兆，而且往往更新频繁，而且每次更新都是全部更新(dropout机制除外)。\n",
    "  - 通常情况下希望就地执行这些更新，比如深度学习模型**权重**的更新基本上都是就地操作。\n",
    "  - 即便不就地更新，仍有引用指向参数的原内存位置，这就会导致引用参数旧值的情况出现，这种情况不一定是用户希望的。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "origin_pos": 77,
    "tab": [
     "pytorch"
    ],
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(Z): 1322644398800\n",
      "id(Z): 1322644398800\n"
     ]
    }
   ],
   "source": [
    "# zeros_like生成一个与Y形状相同的张量，元素值皆为0\n",
    "Z = torch.zeros_like(Y)\n",
    "print('id(Z):', id(Z))\n",
    "\n",
    "# Z[:]注意切片方式可以保证就地操作\n",
    "Z[:] = X + Y\n",
    "print('id(Z):', id(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 79,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "- `X[:] = X + Y`或`X += Y`可以减少操作的内存开销\n",
    "  - 前提：没有其他程序需引用X的原值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "origin_pos": 81,
    "tab": [
     "pytorch"
    ],
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(X)\n",
    "# +=操作符也是就地操作\n",
    "X += Y\n",
    "id(X) == before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1322622050352, 1322644705680)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 此操作就不是就地操作\n",
    "X = X + Y \n",
    "id(X), before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 83
   },
   "source": [
    "## 2.1.6 转换为其他Python对象\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 85,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "- 张量和NumPy张量（`ndarray`）互换\n",
    "- torch张量和numpy数组将共享底层内存，因此就地操作更改一个张量也会同时更改另一个张量。\n",
    "- 要将**大小为1的张量**转换为Python标量，可调用`item`函数或Python的内置函数实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "origin_pos": 87,
    "tab": [
     "pytorch"
    ],
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, torch.Tensor)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = X.numpy()\n",
    "B = torch.tensor(A)\n",
    "type(A), type(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "origin_pos": 91,
    "tab": [
     "pytorch"
    ],
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.5000]), 3.5, 3.5, 3)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([3.5])\n",
    "# 使用张量的item函数或python的float或int等内置函数\n",
    "# 将单元素的张量转换成标量，此种操作在深度学习的编程中很常见\n",
    "a, a.item(), float(a), int(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.7 维度重新排列与轴变换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- 在卷积神经网络（Convolutional Neural Networks, CNNs）中，图像数据通常表示为多维张量。\n",
    "- 这些张量的结构如下所示：\n",
    "  - 通道优先 (Channel-first)：形状为 (C, H, W)。\n",
    "  - 通道末尾 (Channel-last)：形状为(H, W, C)。\n",
    "- C, H, W的意义如下：\n",
    "  - C：通道数量，对于彩色图像，通常有 3 个通道（红、绿、蓝），而对于灰度图像则只有 1 个通道。\n",
    "  - H：图像的高度，表示垂直方向上的像素数。\n",
    "  - W：图像的宽度，表示水平方向上的像素数。  \n",
    "- 不同的深度学习框架可能默认采用不同的表示方法：\n",
    "  - PyTorch 默认使用通道优先表示法。\n",
    "  - TensorFlow 默认使用通道末尾表示法。\n",
    "  - matplotlib 默认使用通道末尾表示法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4, 3])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模拟图像数据，通道维在第3维(2轴)，因此需要将之重排到第1维（0轴）\n",
    "imag = torch.arange(0, 60).reshape(5,4,3)\n",
    "imag.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "- **说明：permute函数**\n",
    "  - permute函数3个参数，代表期望的维度排序。\n",
    "  - 比如下例中：\n",
    "    - 将原来的2轴（第3维）变换到0轴（第1维）。\n",
    "    - 将原来的0轴重排到1轴，1轴重排到2轴。\n",
    "  - permute函数参数是一个元组：\n",
    "    - 元组索引代表变换后的轴顺序，比如本例中0,1,2索引分别代表新张量的0,1,2轴。\n",
    "    - 元组相应索引对应的元素值表示将原张量的哪个维度重排到此索引所代表的新张量的维度，(2,0,1)中的2就是将原张量的2轴重排到新张量的0轴。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 4])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 重排后的维度\n",
    "imag2 = imag.permute((2,0,1))\n",
    "imag2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0,  1,  2],\n",
       "          [ 3,  4,  5],\n",
       "          [ 6,  7,  8],\n",
       "          [ 9, 10, 11]],\n",
       " \n",
       "         [[12, 13, 14],\n",
       "          [15, 16, 17],\n",
       "          [18, 19, 20],\n",
       "          [21, 22, 23]],\n",
       " \n",
       "         [[24, 25, 26],\n",
       "          [27, 28, 29],\n",
       "          [30, 31, 32],\n",
       "          [33, 34, 35]],\n",
       " \n",
       "         [[36, 37, 38],\n",
       "          [39, 40, 41],\n",
       "          [42, 43, 44],\n",
       "          [45, 46, 47]],\n",
       " \n",
       "         [[48, 49, 50],\n",
       "          [51, 52, 53],\n",
       "          [54, 55, 56],\n",
       "          [57, 58, 59]]]),\n",
       " tensor([[[ 0,  3,  6,  9],\n",
       "          [12, 15, 18, 21],\n",
       "          [24, 27, 30, 33],\n",
       "          [36, 39, 42, 45],\n",
       "          [48, 51, 54, 57]],\n",
       " \n",
       "         [[ 1,  4,  7, 10],\n",
       "          [13, 16, 19, 22],\n",
       "          [25, 28, 31, 34],\n",
       "          [37, 40, 43, 46],\n",
       "          [49, 52, 55, 58]],\n",
       " \n",
       "         [[ 2,  5,  8, 11],\n",
       "          [14, 17, 20, 23],\n",
       "          [26, 29, 32, 35],\n",
       "          [38, 41, 44, 47],\n",
       "          [50, 53, 56, 59]]]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imag, imag2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 也可以通过`swapdims`函数实现：\n",
    "  - 首先将`imag`的0轴与2轴互换，结果为`imag3`新变量。\n",
    "  - 然后再将`imag3`的1轴和2轴互换，结果`imag4`张量和`imag2`完全一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 5, 4]),\n",
       " tensor([[[ 0,  3,  6,  9],\n",
       "          [12, 15, 18, 21],\n",
       "          [24, 27, 30, 33],\n",
       "          [36, 39, 42, 45],\n",
       "          [48, 51, 54, 57]],\n",
       " \n",
       "         [[ 1,  4,  7, 10],\n",
       "          [13, 16, 19, 22],\n",
       "          [25, 28, 31, 34],\n",
       "          [37, 40, 43, 46],\n",
       "          [49, 52, 55, 58]],\n",
       " \n",
       "         [[ 2,  5,  8, 11],\n",
       "          [14, 17, 20, 23],\n",
       "          [26, 29, 32, 35],\n",
       "          [38, 41, 44, 47],\n",
       "          [50, 53, 56, 59]]]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 此方法稍微麻烦一点\n",
    "imag3 = torch.swapdims(imag, 0, 2)\n",
    "imag4 = torch.swapdims(imag3, 1, 2)\n",
    "imag4.shape, imag4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 5, 4]),\n",
       " tensor([[[ 0,  3,  6,  9],\n",
       "          [12, 15, 18, 21],\n",
       "          [24, 27, 30, 33],\n",
       "          [36, 39, 42, 45],\n",
       "          [48, 51, 54, 57]],\n",
       " \n",
       "         [[ 1,  4,  7, 10],\n",
       "          [13, 16, 19, 22],\n",
       "          [25, 28, 31, 34],\n",
       "          [37, 40, 43, 46],\n",
       "          [49, 52, 55, 58]],\n",
       " \n",
       "         [[ 2,  5,  8, 11],\n",
       "          [14, 17, 20, 23],\n",
       "          [26, 29, 32, 35],\n",
       "          [38, 41, 44, 47],\n",
       "          [50, 53, 56, 59]]]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# swapaxes与swapdims函数功能一致\n",
    "imag5 = torch.swapaxes(imag, 0, 2)\n",
    "imag6 = torch.swapaxes(imag5, 1, 2)\n",
    "imag6.shape, imag6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 93
   },
   "source": [
    "## 小结\n",
    "\n",
    "* 深度学习存储和操作数据的主要接口是张量（$n$维数组）。\n",
    "* 张量提供了各种功能，包括基本数学运算、广播、索引、切片、内存节省和转换为其他Python对象。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
