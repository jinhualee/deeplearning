{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 5.5 读写文件\n",
    "- **目录**\n",
    "  - 5.5.1 加载和保存张量\n",
    "  - 5.5.2 加载和保存模型参数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "- 到目前为止，我们讨论了如何处理数据，以及如何构建、训练和测试深度学习模型。\n",
    "- 有时我们希望保存训练的模型，以备将来在各种环境中使用（比如在部署中进行预测）。\n",
    "- 此外，当运行一个耗时较长的训练过程时，最佳的做法是**定期保存中间结果**，以确保在服务器电源被不小心断掉时，我们不会损失几天的计算结果。\n",
    "- 因此掌握加载和存储权重向量和整个模型的技巧很重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "## 5.5.1 加载和保存张量\n",
    "\n",
    "- 对于单个张量，可以直接调用`load`和`save`函数分别读写它们。\n",
    "- 这两个函数都要求提供一个文件名称。\n",
    "- `save`要求将要保存的变量作为输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "x = torch.arange(4)\n",
    "torch.save(x, 'x-file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "- 现在可以将存储在文件中的数据读回内存。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = torch.load('x-file')\n",
    "x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "- 可以存储一个张量列表，然后把它们读回内存。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "origin_pos": 10,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 操作还是很简便的\n",
    "y = torch.zeros(4)\n",
    "torch.save([x, y],'x-files')\n",
    "x2, y2 = torch.load('x-files')\n",
    "(x2, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "- 可以写入或读取从字符串映射到张量的字典。\n",
    "- 当需要读取或写入模型中的所有权重时，该方法很方便。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "origin_pos": 14,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict = {'x': x, 'y': y}\n",
    "torch.save(mydict, 'mydict')\n",
    "mydict2 = torch.load('mydict')\n",
    "mydict2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 16
   },
   "source": [
    "## 5.5.2 加载和保存模型参数\n",
    "\n",
    "- 深度学习框架提供了内置函数来保存和加载整个网络。\n",
    "- 需要注意的一个重要细节是，这将保存模型的参数而不是保存整个模型。\n",
    "    - 例如，对于某个3层多层感知机，则需要单独指定架构。\n",
    "- 因为模型本身可以包含任意代码，所以模型本身难以序列化。\n",
    "- 因此，为了恢复模型，我们需要用代码生成架构，然后从磁盘加载参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "origin_pos": 18,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0692, -0.0820,  0.0069,  0.1195, -0.1927, -0.2309,  0.0661,  0.0667,\n",
       "          0.0261, -0.0405],\n",
       "        [ 0.0408,  0.0498,  0.0231,  0.0177, -0.0366, -0.0824, -0.0893,  0.0744,\n",
       "         -0.0338,  0.0079]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        '''\n",
    "          层的名字可以任意命名，当然要有一定意义，便于理解。\n",
    "          比如此处在原代码的基础上，增加了一个hidden2层，原hidden改成hidden1\n",
    "        '''\n",
    "        self.hidden1 = nn.Linear(20, 256)\n",
    "        self.hidden2 = nn.Linear(256, 128)##此层是我加的\n",
    "        self.output = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ##原代码\n",
    "        #return self.output(F.relu(self.hidden(x)))\n",
    "        \n",
    "        ## 对原来的forward函数进行了修改\n",
    "        ## 注意此处的嵌套调用方式\n",
    "        return self.output(F.relu(self.hidden2(F.relu(self.hidden1(x)))))\n",
    "\n",
    "net = MLP()\n",
    "X = torch.randn(size=(2, 20))\n",
    "Y = net(X)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 20
   },
   "source": [
    "- 将模型的参数存储在一个叫做“mlp.params”的文件中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "origin_pos": 22,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "## 模型的参数保存在state_dict中\n",
    "torch.save(net.state_dict(), 'mlp.params')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 24
   },
   "source": [
    "- 为了恢复模型，实例化了原始多层感知机模型的一个备份。\n",
    "- 不需要随机初始化模型参数，而是**直接读取文件中存储的参数。**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "origin_pos": 26,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (hidden1): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (hidden2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (output): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone = MLP()\n",
    "clone.load_state_dict(torch.load('mlp.params'))\n",
    "clone.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('hidden1.weight',\n",
       "              tensor([[ 0.1710, -0.0664, -0.0360,  ...,  0.1741, -0.0216,  0.1797],\n",
       "                      [ 0.1008, -0.1274,  0.1556,  ...,  0.0724, -0.2218, -0.0725],\n",
       "                      [ 0.1780,  0.0977, -0.1743,  ...,  0.1828,  0.0696, -0.0862],\n",
       "                      ...,\n",
       "                      [ 0.0681, -0.0513,  0.1119,  ..., -0.1003, -0.0724,  0.0553],\n",
       "                      [ 0.0176,  0.1683,  0.2015,  ..., -0.2133, -0.0777,  0.1066],\n",
       "                      [-0.0421, -0.0341, -0.1183,  ...,  0.0671,  0.2195, -0.1041]])),\n",
       "             ('hidden1.bias',\n",
       "              tensor([-0.1316,  0.0226, -0.0699, -0.1633, -0.1629,  0.0876,  0.0511,  0.0073,\n",
       "                      -0.0837, -0.1180,  0.1689, -0.1043, -0.1351,  0.0022,  0.1146,  0.0863,\n",
       "                      -0.0722,  0.1177, -0.1565, -0.1193,  0.1253, -0.0268, -0.1376,  0.2009,\n",
       "                       0.0311, -0.1901,  0.0541,  0.0132, -0.0219,  0.0113, -0.0717,  0.0252,\n",
       "                       0.0647, -0.1975,  0.0228, -0.0269, -0.2208,  0.0861, -0.1261,  0.1249,\n",
       "                      -0.0038, -0.0850, -0.0072, -0.1768, -0.2075,  0.2152,  0.1698, -0.1002,\n",
       "                      -0.0088,  0.1920, -0.1127, -0.1344,  0.1183, -0.1731, -0.0210,  0.0820,\n",
       "                      -0.1285,  0.0950,  0.1935,  0.0597,  0.0143,  0.1179, -0.1551, -0.1413,\n",
       "                      -0.1474,  0.1291,  0.0709, -0.0532, -0.1362,  0.1160, -0.1517,  0.0044,\n",
       "                      -0.1307,  0.1930,  0.0112,  0.1482, -0.2209,  0.0892, -0.0615,  0.0810,\n",
       "                       0.1001, -0.0994, -0.1681,  0.1138,  0.0652,  0.0290,  0.0064,  0.0925,\n",
       "                       0.1818, -0.0978, -0.0301, -0.1698,  0.0403, -0.0927, -0.1322, -0.0909,\n",
       "                       0.1483,  0.0047, -0.1994,  0.0534,  0.1369,  0.1854,  0.0793,  0.0953,\n",
       "                      -0.1938,  0.0433, -0.0419,  0.0049,  0.1398, -0.1271,  0.1367,  0.0449,\n",
       "                      -0.0961,  0.0542,  0.2031, -0.1943, -0.0157,  0.1544, -0.0552,  0.1319,\n",
       "                      -0.0675, -0.1575, -0.1149, -0.1042, -0.1941,  0.0271, -0.0422,  0.0158,\n",
       "                      -0.0845,  0.1966,  0.1033, -0.2137,  0.2011, -0.0597,  0.1267, -0.0556,\n",
       "                      -0.0937,  0.1042, -0.1006,  0.2089,  0.0235, -0.1030,  0.1018, -0.1236,\n",
       "                       0.1080, -0.0551, -0.1584, -0.1057, -0.0017, -0.0840, -0.1171, -0.1557,\n",
       "                      -0.0247, -0.1331,  0.1652, -0.1164,  0.1174, -0.2158, -0.1169,  0.0404,\n",
       "                       0.0646, -0.0837,  0.1106, -0.1912,  0.1522, -0.0793, -0.0318,  0.1267,\n",
       "                       0.1911,  0.0408,  0.1301,  0.1336, -0.0545, -0.1505,  0.1621, -0.0777,\n",
       "                       0.0671,  0.1398, -0.1332, -0.1616, -0.0011, -0.0923,  0.0543,  0.0553,\n",
       "                       0.0833,  0.1923,  0.0508, -0.2073, -0.2110,  0.0442,  0.2086, -0.2121,\n",
       "                      -0.1899, -0.1449,  0.0674, -0.0730, -0.0895,  0.1090,  0.2042,  0.0690,\n",
       "                      -0.1638,  0.1517,  0.0770,  0.1056, -0.1942,  0.0396,  0.1633, -0.1954,\n",
       "                      -0.0752,  0.0005,  0.2175,  0.2134, -0.0196, -0.2217, -0.1511,  0.1832,\n",
       "                      -0.0771,  0.0441, -0.1407, -0.0442, -0.0144, -0.0368, -0.0510, -0.0044,\n",
       "                      -0.0374, -0.1395,  0.1848,  0.0583, -0.0571,  0.0938, -0.0527,  0.1398,\n",
       "                      -0.2156,  0.0522,  0.0852,  0.1318,  0.0672,  0.2178,  0.1804,  0.1512,\n",
       "                      -0.1708,  0.0286,  0.0880,  0.1570,  0.1833,  0.0906,  0.1296, -0.0965,\n",
       "                      -0.1075, -0.0303, -0.2212, -0.0835, -0.1617,  0.1166, -0.0131,  0.0599])),\n",
       "             ('hidden2.weight',\n",
       "              tensor([[ 0.0399,  0.0442,  0.0467,  ..., -0.0204, -0.0477, -0.0059],\n",
       "                      [ 0.0142,  0.0474,  0.0261,  ..., -0.0496,  0.0451, -0.0621],\n",
       "                      [ 0.0500,  0.0259,  0.0280,  ...,  0.0197, -0.0566, -0.0452],\n",
       "                      ...,\n",
       "                      [ 0.0435, -0.0522, -0.0333,  ..., -0.0099,  0.0539, -0.0486],\n",
       "                      [ 0.0462, -0.0495, -0.0203,  ...,  0.0343,  0.0567,  0.0480],\n",
       "                      [-0.0183, -0.0491, -0.0375,  ..., -0.0207, -0.0266,  0.0093]])),\n",
       "             ('hidden2.bias',\n",
       "              tensor([ 6.0873e-03,  6.1686e-02, -1.3512e-02, -9.1330e-03, -1.6724e-03,\n",
       "                      -6.5903e-04,  4.0934e-02,  1.4499e-02,  5.8597e-03,  4.0546e-02,\n",
       "                       5.4088e-02,  2.5619e-02, -1.3243e-02,  3.5660e-03, -1.3929e-02,\n",
       "                       2.0278e-02,  4.4004e-02, -3.5149e-02,  1.0381e-02,  1.2849e-02,\n",
       "                       5.8694e-02, -4.9824e-02, -4.6877e-03, -3.6409e-02,  4.0775e-02,\n",
       "                       9.1913e-03, -2.0548e-02,  5.2767e-02,  4.8980e-02, -2.8285e-02,\n",
       "                       2.0454e-02, -5.7427e-02, -8.9902e-03,  5.1666e-02,  2.5195e-02,\n",
       "                      -3.7945e-02, -5.0459e-02, -1.2618e-02,  9.5095e-03, -4.8769e-02,\n",
       "                      -1.9450e-02, -5.3703e-02,  9.7638e-03,  5.3114e-02, -1.9393e-02,\n",
       "                      -4.3894e-02,  1.7314e-02, -5.7707e-02, -1.6840e-02,  5.3097e-03,\n",
       "                       4.7555e-02, -5.0977e-02, -3.1891e-02,  4.8798e-02, -4.2428e-02,\n",
       "                       5.5919e-02,  5.2982e-02,  3.3488e-02, -5.7486e-02,  4.0341e-02,\n",
       "                      -4.8873e-02, -6.1776e-02, -2.9291e-02,  1.0010e-02, -1.6936e-02,\n",
       "                       5.6576e-03, -3.8461e-02,  2.3304e-02, -4.8291e-02, -5.0460e-03,\n",
       "                      -7.1650e-04,  2.9983e-02,  2.5360e-02, -4.6043e-02,  1.8284e-02,\n",
       "                       3.1580e-02,  4.5568e-02, -2.8673e-03,  7.4078e-03,  4.7395e-02,\n",
       "                       5.9429e-03,  2.5948e-02, -1.6765e-02, -4.2592e-02, -5.1911e-02,\n",
       "                       4.8612e-02, -5.4619e-02,  1.6182e-02,  1.4437e-02,  1.5232e-02,\n",
       "                       2.5914e-02,  3.4861e-02, -5.9119e-02,  2.9842e-02,  3.7550e-02,\n",
       "                       4.1098e-02, -1.9574e-02,  3.8044e-02, -4.1694e-03,  3.5666e-02,\n",
       "                      -2.5553e-03,  4.8189e-02,  3.3860e-02, -3.3126e-02,  3.6763e-02,\n",
       "                      -1.4948e-02, -5.2501e-02,  2.7419e-03,  1.1456e-02, -5.5749e-03,\n",
       "                      -7.9900e-03, -5.8389e-02, -5.8982e-03,  5.0721e-02, -4.2766e-02,\n",
       "                      -5.2131e-02, -4.5335e-02,  8.6342e-03, -4.3156e-02, -1.2926e-02,\n",
       "                       1.8521e-02, -2.8116e-02, -4.5406e-02, -9.8467e-05,  5.9032e-02,\n",
       "                       5.7914e-02, -6.2342e-02, -2.5318e-02])),\n",
       "             ('output.weight',\n",
       "              tensor([[-1.6530e-02,  1.8227e-02,  6.1779e-02,  ..., -6.8865e-02,\n",
       "                       -1.8567e-02,  4.2113e-02],\n",
       "                      [ 6.9898e-02, -5.5982e-02, -8.4605e-02,  ...,  8.3857e-02,\n",
       "                       -2.6945e-02, -6.5492e-02],\n",
       "                      [ 7.5914e-02, -4.4848e-02, -4.2652e-02,  ..., -7.0765e-02,\n",
       "                       -3.4683e-02, -4.7460e-02],\n",
       "                      ...,\n",
       "                      [ 8.1188e-02, -6.7387e-02, -7.1414e-02,  ...,  7.2317e-02,\n",
       "                        5.2551e-02, -2.3896e-02],\n",
       "                      [-3.9279e-05, -3.8595e-02,  1.6832e-02,  ...,  3.9638e-02,\n",
       "                       -1.2866e-02, -5.5756e-02],\n",
       "                      [ 8.3369e-02,  3.1429e-02, -1.8898e-03,  ...,  8.6313e-02,\n",
       "                        7.7150e-02,  3.2218e-02]])),\n",
       "             ('output.bias',\n",
       "              tensor([ 0.0123,  0.0270, -0.0819, -0.0131, -0.0835, -0.0660,  0.0641, -0.0526,\n",
       "                       0.0307,  0.0171]))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 克隆模型的参数\n",
    "clone.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('hidden1.weight',\n",
       "              tensor([[ 0.1710, -0.0664, -0.0360,  ...,  0.1741, -0.0216,  0.1797],\n",
       "                      [ 0.1008, -0.1274,  0.1556,  ...,  0.0724, -0.2218, -0.0725],\n",
       "                      [ 0.1780,  0.0977, -0.1743,  ...,  0.1828,  0.0696, -0.0862],\n",
       "                      ...,\n",
       "                      [ 0.0681, -0.0513,  0.1119,  ..., -0.1003, -0.0724,  0.0553],\n",
       "                      [ 0.0176,  0.1683,  0.2015,  ..., -0.2133, -0.0777,  0.1066],\n",
       "                      [-0.0421, -0.0341, -0.1183,  ...,  0.0671,  0.2195, -0.1041]])),\n",
       "             ('hidden1.bias',\n",
       "              tensor([-0.1316,  0.0226, -0.0699, -0.1633, -0.1629,  0.0876,  0.0511,  0.0073,\n",
       "                      -0.0837, -0.1180,  0.1689, -0.1043, -0.1351,  0.0022,  0.1146,  0.0863,\n",
       "                      -0.0722,  0.1177, -0.1565, -0.1193,  0.1253, -0.0268, -0.1376,  0.2009,\n",
       "                       0.0311, -0.1901,  0.0541,  0.0132, -0.0219,  0.0113, -0.0717,  0.0252,\n",
       "                       0.0647, -0.1975,  0.0228, -0.0269, -0.2208,  0.0861, -0.1261,  0.1249,\n",
       "                      -0.0038, -0.0850, -0.0072, -0.1768, -0.2075,  0.2152,  0.1698, -0.1002,\n",
       "                      -0.0088,  0.1920, -0.1127, -0.1344,  0.1183, -0.1731, -0.0210,  0.0820,\n",
       "                      -0.1285,  0.0950,  0.1935,  0.0597,  0.0143,  0.1179, -0.1551, -0.1413,\n",
       "                      -0.1474,  0.1291,  0.0709, -0.0532, -0.1362,  0.1160, -0.1517,  0.0044,\n",
       "                      -0.1307,  0.1930,  0.0112,  0.1482, -0.2209,  0.0892, -0.0615,  0.0810,\n",
       "                       0.1001, -0.0994, -0.1681,  0.1138,  0.0652,  0.0290,  0.0064,  0.0925,\n",
       "                       0.1818, -0.0978, -0.0301, -0.1698,  0.0403, -0.0927, -0.1322, -0.0909,\n",
       "                       0.1483,  0.0047, -0.1994,  0.0534,  0.1369,  0.1854,  0.0793,  0.0953,\n",
       "                      -0.1938,  0.0433, -0.0419,  0.0049,  0.1398, -0.1271,  0.1367,  0.0449,\n",
       "                      -0.0961,  0.0542,  0.2031, -0.1943, -0.0157,  0.1544, -0.0552,  0.1319,\n",
       "                      -0.0675, -0.1575, -0.1149, -0.1042, -0.1941,  0.0271, -0.0422,  0.0158,\n",
       "                      -0.0845,  0.1966,  0.1033, -0.2137,  0.2011, -0.0597,  0.1267, -0.0556,\n",
       "                      -0.0937,  0.1042, -0.1006,  0.2089,  0.0235, -0.1030,  0.1018, -0.1236,\n",
       "                       0.1080, -0.0551, -0.1584, -0.1057, -0.0017, -0.0840, -0.1171, -0.1557,\n",
       "                      -0.0247, -0.1331,  0.1652, -0.1164,  0.1174, -0.2158, -0.1169,  0.0404,\n",
       "                       0.0646, -0.0837,  0.1106, -0.1912,  0.1522, -0.0793, -0.0318,  0.1267,\n",
       "                       0.1911,  0.0408,  0.1301,  0.1336, -0.0545, -0.1505,  0.1621, -0.0777,\n",
       "                       0.0671,  0.1398, -0.1332, -0.1616, -0.0011, -0.0923,  0.0543,  0.0553,\n",
       "                       0.0833,  0.1923,  0.0508, -0.2073, -0.2110,  0.0442,  0.2086, -0.2121,\n",
       "                      -0.1899, -0.1449,  0.0674, -0.0730, -0.0895,  0.1090,  0.2042,  0.0690,\n",
       "                      -0.1638,  0.1517,  0.0770,  0.1056, -0.1942,  0.0396,  0.1633, -0.1954,\n",
       "                      -0.0752,  0.0005,  0.2175,  0.2134, -0.0196, -0.2217, -0.1511,  0.1832,\n",
       "                      -0.0771,  0.0441, -0.1407, -0.0442, -0.0144, -0.0368, -0.0510, -0.0044,\n",
       "                      -0.0374, -0.1395,  0.1848,  0.0583, -0.0571,  0.0938, -0.0527,  0.1398,\n",
       "                      -0.2156,  0.0522,  0.0852,  0.1318,  0.0672,  0.2178,  0.1804,  0.1512,\n",
       "                      -0.1708,  0.0286,  0.0880,  0.1570,  0.1833,  0.0906,  0.1296, -0.0965,\n",
       "                      -0.1075, -0.0303, -0.2212, -0.0835, -0.1617,  0.1166, -0.0131,  0.0599])),\n",
       "             ('hidden2.weight',\n",
       "              tensor([[ 0.0399,  0.0442,  0.0467,  ..., -0.0204, -0.0477, -0.0059],\n",
       "                      [ 0.0142,  0.0474,  0.0261,  ..., -0.0496,  0.0451, -0.0621],\n",
       "                      [ 0.0500,  0.0259,  0.0280,  ...,  0.0197, -0.0566, -0.0452],\n",
       "                      ...,\n",
       "                      [ 0.0435, -0.0522, -0.0333,  ..., -0.0099,  0.0539, -0.0486],\n",
       "                      [ 0.0462, -0.0495, -0.0203,  ...,  0.0343,  0.0567,  0.0480],\n",
       "                      [-0.0183, -0.0491, -0.0375,  ..., -0.0207, -0.0266,  0.0093]])),\n",
       "             ('hidden2.bias',\n",
       "              tensor([ 6.0873e-03,  6.1686e-02, -1.3512e-02, -9.1330e-03, -1.6724e-03,\n",
       "                      -6.5903e-04,  4.0934e-02,  1.4499e-02,  5.8597e-03,  4.0546e-02,\n",
       "                       5.4088e-02,  2.5619e-02, -1.3243e-02,  3.5660e-03, -1.3929e-02,\n",
       "                       2.0278e-02,  4.4004e-02, -3.5149e-02,  1.0381e-02,  1.2849e-02,\n",
       "                       5.8694e-02, -4.9824e-02, -4.6877e-03, -3.6409e-02,  4.0775e-02,\n",
       "                       9.1913e-03, -2.0548e-02,  5.2767e-02,  4.8980e-02, -2.8285e-02,\n",
       "                       2.0454e-02, -5.7427e-02, -8.9902e-03,  5.1666e-02,  2.5195e-02,\n",
       "                      -3.7945e-02, -5.0459e-02, -1.2618e-02,  9.5095e-03, -4.8769e-02,\n",
       "                      -1.9450e-02, -5.3703e-02,  9.7638e-03,  5.3114e-02, -1.9393e-02,\n",
       "                      -4.3894e-02,  1.7314e-02, -5.7707e-02, -1.6840e-02,  5.3097e-03,\n",
       "                       4.7555e-02, -5.0977e-02, -3.1891e-02,  4.8798e-02, -4.2428e-02,\n",
       "                       5.5919e-02,  5.2982e-02,  3.3488e-02, -5.7486e-02,  4.0341e-02,\n",
       "                      -4.8873e-02, -6.1776e-02, -2.9291e-02,  1.0010e-02, -1.6936e-02,\n",
       "                       5.6576e-03, -3.8461e-02,  2.3304e-02, -4.8291e-02, -5.0460e-03,\n",
       "                      -7.1650e-04,  2.9983e-02,  2.5360e-02, -4.6043e-02,  1.8284e-02,\n",
       "                       3.1580e-02,  4.5568e-02, -2.8673e-03,  7.4078e-03,  4.7395e-02,\n",
       "                       5.9429e-03,  2.5948e-02, -1.6765e-02, -4.2592e-02, -5.1911e-02,\n",
       "                       4.8612e-02, -5.4619e-02,  1.6182e-02,  1.4437e-02,  1.5232e-02,\n",
       "                       2.5914e-02,  3.4861e-02, -5.9119e-02,  2.9842e-02,  3.7550e-02,\n",
       "                       4.1098e-02, -1.9574e-02,  3.8044e-02, -4.1694e-03,  3.5666e-02,\n",
       "                      -2.5553e-03,  4.8189e-02,  3.3860e-02, -3.3126e-02,  3.6763e-02,\n",
       "                      -1.4948e-02, -5.2501e-02,  2.7419e-03,  1.1456e-02, -5.5749e-03,\n",
       "                      -7.9900e-03, -5.8389e-02, -5.8982e-03,  5.0721e-02, -4.2766e-02,\n",
       "                      -5.2131e-02, -4.5335e-02,  8.6342e-03, -4.3156e-02, -1.2926e-02,\n",
       "                       1.8521e-02, -2.8116e-02, -4.5406e-02, -9.8467e-05,  5.9032e-02,\n",
       "                       5.7914e-02, -6.2342e-02, -2.5318e-02])),\n",
       "             ('output.weight',\n",
       "              tensor([[-1.6530e-02,  1.8227e-02,  6.1779e-02,  ..., -6.8865e-02,\n",
       "                       -1.8567e-02,  4.2113e-02],\n",
       "                      [ 6.9898e-02, -5.5982e-02, -8.4605e-02,  ...,  8.3857e-02,\n",
       "                       -2.6945e-02, -6.5492e-02],\n",
       "                      [ 7.5914e-02, -4.4848e-02, -4.2652e-02,  ..., -7.0765e-02,\n",
       "                       -3.4683e-02, -4.7460e-02],\n",
       "                      ...,\n",
       "                      [ 8.1188e-02, -6.7387e-02, -7.1414e-02,  ...,  7.2317e-02,\n",
       "                        5.2551e-02, -2.3896e-02],\n",
       "                      [-3.9279e-05, -3.8595e-02,  1.6832e-02,  ...,  3.9638e-02,\n",
       "                       -1.2866e-02, -5.5756e-02],\n",
       "                      [ 8.3369e-02,  3.1429e-02, -1.8898e-03,  ...,  8.6313e-02,\n",
       "                        7.7150e-02,  3.2218e-02]])),\n",
       "             ('output.bias',\n",
       "              tensor([ 0.0123,  0.0270, -0.0819, -0.0131, -0.0835, -0.0660,  0.0641, -0.0526,\n",
       "                       0.0307,  0.0171]))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 模型的参数\n",
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (hidden1): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (hidden2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (output): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net ## 原模型有两层，一层是隐藏层，一层是输出层，此处修改成了3层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 28
   },
   "source": [
    "- 由于两个实例具有相同的模型参数，在输入相同的`X`时，两个实例的计算结果应该相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "origin_pos": 30,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## clone是通过加载参数文件后生成的模型\n",
    "Y_clone = clone(X)\n",
    "Y_clone == Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 32
   },
   "source": [
    "## 小结\n",
    "\n",
    "* `save`和`load`函数可用于张量对象的文件读写。\n",
    "* 我们可以通过参数字典保存和加载网络的全部参数。\n",
    "* 保存架构必须在代码中完成，而不是在参数中完成。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
