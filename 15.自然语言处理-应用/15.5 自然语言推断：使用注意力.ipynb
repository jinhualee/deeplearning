{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 15.5 自然语言推断：使用注意力\n",
    "- **目录**\n",
    "  - 15.5.1 用于NLI的注意力模型\n",
    "    - 15.5.1.1 注意力计算（Attending）\n",
    "    - 15.5.1.2 比较\n",
    "    - 15.5.1.3 聚合\n",
    "    - 15.5.1.4 整合代码\n",
    "  - 15.5.2 NLI注意力模型训练和评估\n",
    "    - 15.5.2.1 读取数据集\n",
    "    - 15.5.2.2 创建模型\n",
    "    - 15.5.2.3 训练和评估模型\n",
    "    - 15.5.2.4 使用模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "- 在 15.4节中介绍了自然语言推断任务和SNLI数据集。\n",
    "- 鉴于许多模型都是基于复杂而深度的架构，Parikh等人提出用注意力机制解决自然语言推断问题，并称之为“可分解注意力模型”。\n",
    "- 这使得模型**没有循环层或卷积层**，在SNLI数据集上以更少的参数实现了当时的最佳结果。\n",
    "- 本节将描述并实现这种基于注意力的自然语言推断方法（使用MLP），如图15.5.1中所述。\n",
    "\n",
    "<center><img src='../img/nlp-map-nli-attention.svg'></center>\n",
    "<center>图15.5.1 将预训练GloVe送入基于<b>注意力</b>和<b>MLP</b>的<b>自然语言推断架构</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 本节函数与API列表：\n",
    "  - mlp：分解注意力计算的多层感知机。\n",
    "  - Attend：软对齐注意力计算，即计算双向软注意力。\n",
    "  - Compare：比较假设词元与前提词元以及反过来的比较，生成词元级交互特征。\n",
    "  - Aggregate：求和两组比较向量，然后通过一个多层感知机进行聚合，并获得分类结果，简而言之就是汇总特征并分类。\n",
    "  - DecomposableAttention：将注意力计算步骤、比较步骤和聚合步骤组合在一起。\n",
    "  - predict_snli：预测和输出一对前提和假设之间的逻辑关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "## 15.5.1 用于NLI的注意力模型\n",
    "\n",
    "- 与保留前提和假设中词元的顺序相比，我们可以 **将一个文本序列中的词元与另一个文本序列中的每个词元对齐，然后比较和聚合这些信息，以预测前提和假设之间的逻辑关系** 。\n",
    "- 与机器翻译中源句和目标句之间的词元对齐类似，前提和假设之间的词元对齐可以通过注意力机制灵活地完成。\n",
    "<center><img src='../img/nli-attention.svg'></center>\n",
    "<center>图15.5.2 利用注意力机制进行自然语言推断</center><br>\n",
    "- 图15.5.2描述了使用注意力机制的自然语言推断方法。\n",
    "- 从高层次上讲，它由三个联合训练的步骤组成：<b>对齐</b>、 <b>比较</b>和<b>汇总</b>。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 3
   },
   "source": [
    "### 15.5.1.1 注意力计算（Attending）\n",
    "\n",
    "- **第一步是将一个文本序列中的词元与另一个序列中的每个词元对齐**。\n",
    "  - 比如**前提**是“我确实需要睡眠”，**假设**是“我累了”。\n",
    "  - 由于语义上的相似性，我们不妨将假设中的“我”与前提中的“我”对齐，将假设中的“累”与前提中的“睡眠”对齐。\n",
    "  - 同样，我们可能希望将前提中的“我”与假设中的“我”对齐，将前提中的“需要”和“睡眠”与假设中的“累”对齐。\n",
    "  - 请注意，这种对齐是使用加权平均的“软”对齐，其中理想情况下较大的权重与要对齐的词元相关联。\n",
    "  - 为了便于演示， 图15.5.2以“硬”对齐的方式显示了这种对齐方式。\n",
    "- 现在，我们更详细地描述使用注意力机制的软对齐。\n",
    "  - 用$\\mathbf{A} = (\\mathbf{a}_1, \\ldots, \\mathbf{a}_m)$和$\\mathbf{B} = (\\mathbf{b}_1, \\ldots, \\mathbf{b}_n)$表示前提和假设，其词元数量分别为$m$和$n$，其中$\\mathbf{a}_i, \\mathbf{b}_j \\in \\mathbb{R}^{d}$（$i = 1, \\ldots, m, j = 1, \\ldots, n$）是$d$维的词向量。\n",
    "  - 对于软对齐，我们将注意力权重$e_{ij} \\in \\mathbb{R}$计算为：\n",
    "  $$e_{ij} = f(\\mathbf{a}_i)^\\top f(\\mathbf{b}_j), \\tag{15.5.1}$$\n",
    "  其中函数$f$是在下面的`mlp`函数中定义的多层感知机。输出维度$f$由`mlp`的`num_hiddens`参数指定。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "origin_pos": 5,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def mlp(num_inputs, num_hiddens, flatten):\n",
    "    net = []\n",
    "    net.append(nn.Dropout(0.2))\n",
    "    net.append(nn.Linear(num_inputs, num_hiddens))\n",
    "    net.append(nn.ReLU())\n",
    "    if flatten:\n",
    "        ## 从第二维开始展平，比如三维张量\n",
    "        ## 将其第二维的矩阵展平成一维数组\n",
    "        net.append(nn.Flatten(start_dim=1))\n",
    "    net.append(nn.Dropout(0.2))\n",
    "    #前面一层的输出特征数也是num_hiddens，无论是否展平\n",
    "    net.append(nn.Linear(num_hiddens, num_hiddens))\n",
    "    net.append(nn.ReLU())\n",
    "    if flatten:\n",
    "        net.append(nn.Flatten(start_dim=1))\n",
    "    ## Sequential里参数既不是数组也不是list，而是简单列举需要串联的模型对象\n",
    "    return nn.Sequential(*net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "- 值得注意的是，在公式 15.5.1中，$f$分别输入$\\mathbf{a}_i$和$\\mathbf{b}_j$，而不是将它们一对放在一起作为输入。\n",
    "- 这种**分解**技巧导致$f$只有$m + n$个次计算（线性复杂度），而不是$mn$次计算（二次复杂度）。\n",
    "- 对公式15.5.1中的注意力权重进行规范化，我们计算假设中所有词元向量的加权平均值，以获得假设的表示，该**假设与前提**中索引$i$的词元进行**软对齐**：\n",
    "$$\n",
    "\\boldsymbol{\\beta}_i = \\sum_{j=1}^{n}\\frac{\\exp(e_{ij})}{ \\sum_{k=1}^{n} \\exp(e_{ik})} \\mathbf{b}_j. \\tag{15.5.2}\n",
    "$$\n",
    "- 同样，我们计算**假设中索引为$j$的每个词元与前提词元**的**软对齐**：\n",
    "$$\n",
    "\\boldsymbol{\\alpha}_j = \\sum_{i=1}^{m}\\frac{\\exp(e_{ij})}{ \\sum_{k=1}^{m} \\exp(e_{kj})} \\mathbf{a}_i. \\tag{15.5.3}\n",
    "$$\n",
    "- 下面，我们定义`Attend`类来计算假设（`beta`）与输入前提`A`的软对齐以及前提（`alpha`）与输入假设`B`的软对齐。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "origin_pos": 8,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "## 注意力计算，即软对齐计算\n",
    "class Attend(nn.Module):\n",
    "    ## 100,200\n",
    "    def __init__(self, num_inputs, num_hiddens, **kwargs):\n",
    "        super(Attend, self).__init__(**kwargs)\n",
    "        self.f = mlp(num_inputs, num_hiddens, flatten=False)\n",
    "\n",
    "    '''\n",
    "    A和B的形状都是(256,50,100)，即批量大小、序列长度、词嵌入维度\n",
    "    '''\n",
    "    def forward(self, A, B):\n",
    "        # A/B的形状：（批量大小，序列A/B的词元数，embed_size）\n",
    "        # f_A/f_B的形状：（批量大小，序列A/B的词元数，num_hiddens）\n",
    "        ## f_A和f_B的形状(256,50,200)\n",
    "        f_A = self.f(A)\n",
    "        f_B = self.f(B)\n",
    "        \n",
    "        # e的形状：（批量大小，序列A的词元数，序列B的词元数）\n",
    "        ## 矩阵的批量乘法(256,50,200)@(256,200,50)=(256,50,50)\n",
    "        e = torch.bmm(f_A, f_B.permute(0, 2, 1))\n",
    "        \n",
    "        # beta的形状：（批量大小，序列A的词元数，embed_size），\n",
    "        # 意味着序列B被软对齐到序列A的每个词元(beta的第1个维度)\n",
    "        '''\n",
    "        beta表示a_j 对 b_k 的注意力权重（序列 B → 序列 A）,即将文本序列B的信息聚合于文本序列A，\n",
    "        在每个矩阵的行上求softmax,结果是(256,50,50)\n",
    "        张量的小批量乘积结果：(256,50,50)@(256,50,100)=(256,50,100)\n",
    "        beta实现公式15.5.2计算过程。\n",
    "        '''\n",
    "        beta = torch.bmm(F.softmax(e, dim=-1), B)\n",
    "        \n",
    "        # alpha的形状：（批量大小，序列B的词元数，embed_size），\n",
    "        # 意味着序列A被软对齐到序列B的每个词元(alpha的第1个维度)\n",
    "        '''\n",
    "        b_k 对 a_j 的注意力权重（序列 A → 序列 B），即将文本序列A的信息聚合于文本序列B，\n",
    "        本例中A和B的长度一样，所以即便对e进行转置,alpha的结果仍是(256,50,100)。\n",
    "        alpha实现15.5.3的计算过程。\n",
    "        '''\n",
    "        alpha = torch.bmm(F.softmax(e.permute(0, 2, 1), dim=-1), A)\n",
    "        ## 二者的返回结果：(256,50,100)\n",
    "        return beta, alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 9
   },
   "source": [
    "### 15.5.1.2 比较\n",
    "\n",
    "- 在下一步中，我们将一个序列中的词元与与该词元软对齐的另一个序列进行比较。\n",
    "  - 请注意，在**软对齐中，一个序列中的所有词元（尽管可能具有不同的注意力权重）将与另一个序列中的词元进行比较**。\n",
    "- 为便于演示， 图15.5.2对词元以**硬的**方式对齐。\n",
    "  - 例如，上述的“注意力计算”（attending）步骤确定前提中的“need”和“sleep”都与假设中的“tired”对齐，则将对“tired–need sleep”进行比较。\n",
    "- 在比较步骤中，我们将来自一个序列的词元的连结（运算符$[\\cdot, \\cdot]$）和来自另一序列的对齐的词元送入函数$g$（一个多层感知机）：\n",
    "$$\\mathbf{v}_{A,i} = g([\\mathbf{a}_i, \\boldsymbol{\\beta}_i]), i = 1, \\ldots, m $$\n",
    "  $$ \\mathbf{v}_{B,j} = g([\\mathbf{b}_j, \\boldsymbol{\\alpha}_j]), j = 1, \\ldots, n. \\tag{15.5.4}$$\n",
    "  - 在公式15.5.4中，$\\mathbf{v}_{A,i}$是指，所有假设中的词元与前提中词元$i$软对齐，再与词元$i$的比较；\n",
    "  - 而$\\mathbf{v}_{B,j}$是指，所有前提中的词元与假设中词元$i$软对齐，再与词元$i$的比较。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 9
   },
   "source": [
    "- `Compare`类的功能和目的是实现语义比较，将前提（Premise）和假设（Hypothesis）的词元表示与对方序列的对齐结果进行**深度交互**，生成**更具判别性的比较特征**。\n",
    "  - 将每个词元的原始表示与其软对齐后的上下文表示结合，通过**非线性**变换生成比较向量。\n",
    "  - 输入：\n",
    "    - 原始词向量序列 A（前提）和 B（假设）。\n",
    "    - 软对齐后的表示 $\\beta$（序列 B→A 的对齐结果）和 $\\alpha$（序列 A→B 的对齐结果）。\n",
    "  - 输出：\n",
    "    - 增强后的**比较向量**V_A和V_B，用于后续的**聚合**和**分类**。\n",
    "- 上述操作的目的在于：\n",
    "  - (1) 融合本地与全局信息\n",
    "    - 原始词向量（$\\mathbf{a}_i$ 或 $\\mathbf{b}_j$）：编码**词本身的语义**。\n",
    "    - 对齐向量（$\\beta_i$ 或 $\\alpha_j$）：编码另一序列中相关词元的上下文信息。\n",
    "    - 比较向量：将两者结合，同时捕捉词元**自身特征**和跨序列的**关联特征**。\n",
    "  - (2) 增强判别性，通过非线性变换（MLP），模型能够学习：\n",
    "    - 词元与其对齐上下文之间的语义兼容性（如“喜欢”和“爱好”是正向关联）。\n",
    "    - 词元之间的矛盾或中性关系（如“喜欢”和“讨厌”是矛盾关系）。\n",
    "  - (3) 为聚合步骤提供高阶特征，比较向量 V_A 和 V_B 是后续聚合（求和+分类）的直接输入，其质量直接影响分类性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "origin_pos": 11,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class Compare(nn.Module):\n",
    "    ## (200,200)\n",
    "    def __init__(self, num_inputs, num_hiddens, **kwargs):\n",
    "        super(Compare, self).__init__(**kwargs)\n",
    "        ## MLP的参数(200,200)\n",
    "        self.g = mlp(num_inputs, num_hiddens, flatten=False)\n",
    "\n",
    "    def forward(self, A, B, beta, alpha):\n",
    "        # cat的结果：(256,50,100),(256,50,100)=(256,50,200)\n",
    "        # MLP计算结果,即V_A和V_B的形状:(256,50,200)@(200,200)=(256,50,200)\n",
    "        V_A = self.g(torch.cat([A, beta], dim=2))\n",
    "        V_B = self.g(torch.cat([B, alpha], dim=2))\n",
    "        ## 二者的返回结果皆是：(256,50,200)\n",
    "        return V_A, V_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "### 15.5.1.3 聚合\n",
    "\n",
    "- 现在我们有有两组比较向量$\\mathbf{v}_{A,i}$（$i = 1, \\ldots, m$）和$\\mathbf{v}_{B,j}$（$j = 1, \\ldots, n$）。\n",
    "- 在最后一步中将聚合这些信息以推断逻辑关系。\n",
    "- 首先求和这两组比较向量：\n",
    "$$\n",
    "\\mathbf{v}_A = \\sum_{i=1}^{m} \\mathbf{v}_{A,i}, \\quad \\mathbf{v}_B = \\sum_{j=1}^{n}\\mathbf{v}_{B,j}.\\tag{15.5.5}\n",
    "$$\n",
    "- 接下来将两个求和结果的连结提供给函数$h$（一个多层感知机），以获得逻辑关系的分类结果：\n",
    "$$\n",
    "\\hat{\\mathbf{y}} = h([\\mathbf{v}_A, \\mathbf{v}_B]).\\tag{15.5.6}\n",
    "$$\n",
    "- 聚合步骤在以下`Aggregate`类中定义。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "origin_pos": 14,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class Aggregate(nn.Module):\n",
    "    ## 参数400,200,3\n",
    "    def __init__(self, num_inputs, num_hiddens, num_outputs, **kwargs):\n",
    "        super(Aggregate, self).__init__(**kwargs)\n",
    "        \n",
    "        ''' \n",
    "        其实此处flatten=True纯粹是防御作用，保证代码的健壮性和一致性。\n",
    "        也就是说，即便输入的三维张量，也将该数据通过从第二维进行展平后变成二维张量。\n",
    "        如果输入的是二维张量，则没任何影响，展平结果仍是二维张量，数据没发生任何变化。\n",
    "\n",
    "        num_inputs, num_hidden分别为：400,200\n",
    "        '''\n",
    "        self.h = mlp(num_inputs, num_hiddens, flatten=True)\n",
    "        ## (200,3)\n",
    "        self.linear = nn.Linear(num_hiddens, num_outputs)\n",
    "\n",
    "    def forward(self, V_A, V_B):\n",
    "        ''' \n",
    "        对两组比较向量分别求和。\n",
    "        V_A,V_B变成：从(256, 50, 200)变成(256,200)。\n",
    "        其实就是对矩阵纵向求和，也就是每列求和。\n",
    "        '''\n",
    "        V_A = V_A.sum(dim=1)\n",
    "        V_B = V_B.sum(dim=1)\n",
    "        # 将两个求和结果的连结送到多层感知机中\n",
    "        '''\n",
    "        (1) torch.cat([V_A, V_B], dim=1):(256,200);(256,200)=(256,400)\n",
    "        (2) self.h: (256,400)@(400,200)=(256,200)\n",
    "            flatten: \n",
    "        (3) self.linear: (256,200)@(200,3)=(256,3)\n",
    "            3就是分类结果。\n",
    "        '''\n",
    "        Y_hat = self.linear(self.h(torch.cat([V_A, V_B], dim=1)))\n",
    "        return Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "- **说明：Aggregate类代码分析**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0,  1,  2,  3],\n",
       "          [ 4,  5,  6,  7],\n",
       "          [ 8,  9, 10, 11]],\n",
       " \n",
       "         [[12, 13, 14, 15],\n",
       "          [16, 17, 18, 19],\n",
       "          [20, 21, 22, 23]]]),\n",
       " torch.Size([2, 3, 4]),\n",
       " tensor([[12, 15, 18, 21],\n",
       "         [48, 51, 54, 57]]),\n",
       " torch.Size([2, 4]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## dim=1时求三维张量的和\n",
    "v = torch.arange(24).reshape(2,3,4)\n",
    "v, v.shape,v.sum(dim=1),v.sum(dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0,  1,  2,  3],\n",
       "          [ 4,  5,  6,  7],\n",
       "          [ 8,  9, 10, 11]],\n",
       " \n",
       "         [[12, 13, 14, 15],\n",
       "          [16, 17, 18, 19],\n",
       "          [20, 21, 22, 23]]]),\n",
       " torch.Size([2, 12]),\n",
       " tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n",
       "         [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#v1 = torch.arange(24).reshape(3,8)\n",
    "flat = nn.Flatten(start_dim=1)\n",
    "v, flat(v).shape, flat(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 15
   },
   "source": [
    "### 5.5.1.4 整合代码\n",
    "\n",
    "- 通过将**注意步骤、比较步骤和聚合步骤**组合在一起，定义了可分解注意力模型来联合训练这三个步骤。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "origin_pos": 17,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class DecomposableAttention(nn.Module):\n",
    "    ## 前三个参数：18678，embed_size:100，num_hiddens:200\n",
    "    def __init__(self, vocab, embed_size, num_hiddens, num_inputs_attend=100,\n",
    "                 num_inputs_compare=200, num_inputs_agg=400, **kwargs):\n",
    "        super(DecomposableAttention, self).__init__(**kwargs)\n",
    "        ## 嵌入层：18678, 100\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_size)\n",
    "        ## 注意层：100, 200\n",
    "        self.attend = Attend(num_inputs_attend, num_hiddens)\n",
    "        ## 比较层: 200, 200\n",
    "        self.compare = Compare(num_inputs_compare, num_hiddens)\n",
    "        ## 有3种可能的输出：蕴涵、矛盾和中性\n",
    "        ## 聚合层：400,200,3 \n",
    "        self.aggregate = Aggregate(num_inputs_agg, num_hiddens, num_outputs=3)\n",
    "\n",
    "    def forward(self, X):\n",
    "        premises, hypotheses = X\n",
    "        ## 经过嵌入层的前提A:(256,50,100)\n",
    "        A = self.embedding(premises)\n",
    "        ## 经过嵌入层的假设B:(256,50,100)\n",
    "        B = self.embedding(hypotheses)\n",
    "        ## (256,50,100)\n",
    "        beta, alpha = self.attend(A, B)\n",
    "        ## (256,50,200)\n",
    "        V_A, V_B = self.compare(A, B, beta, alpha)\n",
    "        ## (256,3)\n",
    "        Y_hat = self.aggregate(V_A, V_B)\n",
    "        return Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "- **说明：DecomposableAttention分解注意力计算**\n",
    "  - 传统注意力机制（如Transformer中的缩放点积注意力）需要直接计算**所有词对**之间的交互（复杂度 $O(mn)$）。\n",
    "  - DecomposableAttention直接反映了该模型的核心设计思想，即通过**分解注意力计算**（Decomposing Attention）来降低计算复杂度，同时保持对序列间交互的高效建模。\n",
    "  - 可分解注意力通过以下分解技巧降低复杂度：\n",
    "    - （1）**分离计算路径**：\n",
    "      - 对前提（Premise）和假设（Hypothesis）的词向量**分别**通过MLP（函数 $f$）映射，再计算注意力权重。  \n",
    "      - 原始计算：$e_{ij} = \\text{MLP}([a_i; b_j])$ （需 $mn$ 次MLP计算）。  \n",
    "      - 分解后计算：$e_{ij} = f(a_i)^\\top f(b_j)$ （仅需 $m+n$ 次MLP计算）。  \n",
    "      - **复杂度优化**： 从二次复杂度（$O(mn)$）降至线性复杂度（$O(m+n)$）。\n",
    "    - **(2) 任务目标的分解**，模型将自然语言推断任务拆解为三个可独立优化的子步骤：\n",
    "      - **Attend（对齐）**：计算双向软注意力。\n",
    "      - **Compare（比较）**：生成词元级交互特征。\n",
    "      - **Aggregate（聚合）**：汇总特征并分类。  \n",
    "      - 每个步骤可单独调整或替换，体现模块化设计。\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 18
   },
   "source": [
    "## 15.5.2 NLI注意力模型训练和评估\n",
    "- 在SNLI数据集上对定义好的可分解注意力模型进行训练和评估。\n",
    "- 从读取数据集开始。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 18
   },
   "source": [
    "### 15.5.2.1 读取数据集\n",
    "- 使用15.4节中定义的函数下载并读取SNLI数据集。批量大小和序列长度分别设置为$256$和$50$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "origin_pos": 19,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 549367 examples\n",
      "read 9824 examples\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "（1）SNLI数据集可以先下载解压，\n",
    "然后将d2l包的torch.py修改即可正常读取。\n",
    "将d2l的torch.py文件2636行（具体行可能些微差别）修改成如下：\n",
    "#data_dir = d2l.download_extract('SNLI')\n",
    "data_dir = r'../data/snli_1.0'\n",
    "也就是data_dir改成语料数据所在的目录。可以先下载该文件然后解压即可。\n",
    "\n",
    "（2）训练和测试数据迭代器的形状为:\n",
    "[[[前提(256,50)],[假设(256,50)]],[标签(256,)]]\n",
    "最外层是小批量个数，训练数据有2146个，测试数据有39个小批量。\n",
    "'''\n",
    "batch_size, num_steps = 256, 50\n",
    "train_iter, test_iter, vocab = d2l.load_data_snli(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "- **说明：调用load_data_snli函数的路径问题**\n",
    "<img src='../img/15_5_1.png' width=800px />\n",
    "\n",
    "  - 问题：[Errno 22] Invalid argument: '..\\\\data\\\\snli_1.0\\\\Icon\\r'\n",
    "    - 报错的原因是SNLI数据集的压缩文件\"snli_1.0.zip\"里面有两个路径为“snli_1.0\\Icon\\r”和“’__MACOSX/snli_1.0/._Icon\\r’”的文件，导致无法解析此路径进而导致整个文件无法解压。\n",
    "  - 解决方法：\n",
    "    - 解压数据集\"snli_1.0.zip”，找到data文件夹，手动把数据集\"snli_1.0.zip\"解压到当前文件夹。\n",
    "    - 然后在d2l包源文件torch.py中定位load_data_snli函数。\n",
    "    - 将该函数的变量num_workers赋值0。\n",
    "    - 将变量data_dir赋值为数据集解压后的路径，即把data_dir = d2l.download_extract('SNLI')改为data_dir = r' ../data/snli_1.0'。\n",
    "<img src='../img/15_5_2.png' width=800px />\n",
    "\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2146"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 训练数据共有2146个小批量\n",
    "len(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 50]), torch.Size([256, 50]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 训练数据和测试数据都是由(256,50)的小批量数据组成。\n",
    "## 序列长度为50\n",
    "list(iter(test_iter))[0][0][0].shape,list(iter(test_iter))[0][0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 测试数据的小批量个数\n",
    "len(iter(test_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 标签\n",
    "list(iter(test_iter))[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18678"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 词表有18678个词元\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([[ 353,  617, 1314,  ...,    1,    1,    1],\n",
       "          [ 353,  617, 1314,  ...,    1,    1,    1],\n",
       "          [ 353,  617, 1314,  ...,    1,    1,    1],\n",
       "          ...,\n",
       "          [   4,   34,   36,  ...,    1,    1,    1],\n",
       "          [  59,  147,    8,  ...,    1,    1,    1],\n",
       "          [  59,  147,    8,  ...,    1,    1,    1]]),\n",
       "  tensor([[  14,  617,   69,  ...,    1,    1,    1],\n",
       "          [  14,  617,    6,  ...,    1,    1,    1],\n",
       "          [   4, 1314,  273,  ...,    1,    1,    1],\n",
       "          ...,\n",
       "          [  14,   34,    6,  ...,    1,    1,    1],\n",
       "          [  59,  147,    8,  ...,    1,    1,    1],\n",
       "          [   4,   26,    8,  ...,    1,    1,    1]])],\n",
       " tensor([2, 0, 1, 2, 0, 1, 0, 2, 1, 2, 0, 1, 0, 0, 1, 1, 0, 2, 0, 1, 2, 1, 0, 2,\n",
       "         2, 1, 0, 0, 2, 1, 1, 0, 0, 0, 2, 1, 2, 0, 1, 0, 2, 1, 1, 0, 2, 2, 1, 0,\n",
       "         2, 1, 2, 0, 1, 2, 1, 0, 0, 1, 2, 0, 1, 2, 2, 0, 1, 0, 2, 2, 1, 2, 0, 1,\n",
       "         0, 2, 1, 0, 2, 0, 0, 1, 1, 1, 0, 0, 1, 2, 1, 2, 0, 2, 1, 0, 2, 0, 1, 1,\n",
       "         0, 2, 2, 0, 1, 2, 0, 1, 0, 2, 0, 1, 0, 0, 0, 1, 2, 0, 1, 2, 0, 1, 1, 0,\n",
       "         2, 2, 1, 2, 0, 1, 2, 0, 1, 1, 2, 0, 2, 0, 1, 0, 1, 2, 0, 1, 2, 2, 0, 1,\n",
       "         2, 1, 0, 2, 2, 1, 0, 2, 1, 1, 2, 0, 2, 1, 0, 2, 0, 1, 2, 2, 1, 0, 1, 2,\n",
       "         1, 0, 0, 0, 1, 2, 0, 1, 2, 2, 1, 0, 2, 0, 2, 2, 0, 1, 1, 2, 0, 0, 2, 1,\n",
       "         0, 2, 1, 0, 1, 2, 1, 2, 0, 2, 1, 2, 0, 1, 0, 2, 1, 1, 0, 2, 1, 0, 2, 0,\n",
       "         2, 1, 0, 1, 2, 2, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 2, 2, 0, 1, 0, 1, 2, 1,\n",
       "         2, 0, 0, 0, 1, 0, 2, 1, 2, 1, 0, 1, 0, 2, 2, 1])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "训练和测试数据1个小批量的构成，是由二维list构成。\n",
    "第二维两个元素：一个list和一个tensor。\n",
    "第一个元素的list由两个tensor构成(前提和假设)。\n",
    "\n",
    "二维数组：[[tensor(256,50),tensor(256,50)],tensor(256)]。\n",
    "前提和假设放在list里两个张量。\n",
    "'''\n",
    "list(iter(test_iter))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 20
   },
   "source": [
    "### 15.5.2.2 创建模型\n",
    "\n",
    "- 此处使用预训练好的100维GloVe嵌入来表示输入词元。\n",
    "- 将向量$\\mathbf{a}_i$和$\\mathbf{b}_j$在公式15.5.1中的维数预定义为100。 \n",
    "- 公式15.5.1中的函数$f$和公式15.5.4中的函数$g$的输出维度被设置为200。\n",
    "- 然后创建一个模型实例，初始化它的参数，并加载GloVe嵌入来初始化输入词元的向量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "origin_pos": 22,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "embed_size, num_hiddens, devices = 100, 200, d2l.try_all_gpus()\n",
    "## 参数：18678，100，200\n",
    "net = DecomposableAttention(vocab, embed_size, num_hiddens)\n",
    "# 注意glove_embedding预训练词向量的词元个数为40万个，TokenEmbedding加载后会变成40万零1个\n",
    "glove_embedding = d2l.TokenEmbedding('glove.6b.100d')\n",
    "embeds = glove_embedding[vocab.idx_to_token]\n",
    "net.embedding.weight.data.copy_(embeds);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18678, 100])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 词表嵌入：(词表个数，嵌入长度)，嵌入长度取决于glove.6b.100d\n",
    "embeds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 23
   },
   "source": [
    "### 15.5.2.3 训练和评估模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 25
   },
   "source": [
    "- 现在可以在SNLI数据集上训练和评估模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "origin_pos": 27,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "lr, num_epochs = 0.001, 4\n",
    "trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 28
   },
   "source": [
    "### 15.5.2.4 使用模型\n",
    "- 最后，定义预测函数，输出一对前提和假设之间的逻辑关系。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "origin_pos": 30,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "def predict_snli(net, vocab, premise, hypothesis):\n",
    "    \"\"\"预测前提和假设之间的逻辑关系\"\"\"\n",
    "    net.eval()\n",
    "    premise = torch.tensor(vocab[premise], device=d2l.try_gpu())\n",
    "    hypothesis = torch.tensor(vocab[hypothesis], device=d2l.try_gpu())\n",
    "    \n",
    "    '''\n",
    "    使用包含两个张量的list输入进行预测，两个张量的第一维相同，第二维可以不同。\n",
    "    即表示前提假设对的1对1的句子，然后句子的长度可以不同。\n",
    "    但是在模型中会进行统一处理。\n",
    "    '''\n",
    "    label = torch.argmax(net([premise.reshape((1, -1)),\n",
    "                           hypothesis.reshape((1, -1))]), dim=1)\n",
    "    return 'entailment' if label == 0 else 'contradiction' if label == 1 \\\n",
    "            else 'neutral'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 31
   },
   "source": [
    "- 我们可以使用训练好的模型来获得对示例句子的自然语言推断结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "origin_pos": 32,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'contradiction'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_snli(net, vocab, ['he', 'is', 'good', '.'], ['he', 'is', 'very', 'bad', '.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 33
   },
   "source": [
    "## 小结\n",
    "\n",
    "* 可分解注意模型包括三个步骤来预测前提和假设之间的逻辑关系：注意力计算、比较和聚合。\n",
    "* 通过注意力机制，我们可以将一个文本序列中的词元与另一个文本序列中的每个词元对齐，反之亦然。这种对齐是使用加权平均的软对齐，其中理想情况下较大的权重与要对齐的词元相关联。\n",
    "* 在计算注意力权重时，分解技巧会带来比二次复杂度更理想的线性复杂度。\n",
    "* 我们可以使用预训练好的词向量作为下游自然语言处理任务（如自然语言推断）的输入表示。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
