{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 14.2 词嵌入近似训练\n",
    "- **目录**\n",
    "  - 14.2.1 负采样\n",
    "  - 14.2.2 层序Softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 近似训练（Approximate Training）指的是一类通过**简化**或**逼近计算密集型步骤**来提升训练效率的技术，尤其在处理**长序列**、**大规模词汇表**或**复杂模型**时。\n",
    "- 这些方法通过**牺牲**部分理论精确性来换取计算速度和内存效率的提升。\n",
    "- 近似训练的核心思想：\n",
    "  - **目标**：解决传统方法（如精确计算）在NLP任务中面临的**计算复杂度高**或**内存消耗大**的问题。\n",
    "  - **手段**：用数学或工程上的**近似替代**精确计算，使模型在可接受的误差范围内高效训练。\n",
    "- 近似训练技术一般包括：负采样（Negative Sampling）、层序Softmax（Hierarchical Softmax）、采样Softmax（Sampled Softmax）、掩码语言模型（Masked Language Model, MLM）的局部预测、低秩近似（Low-Rank Approximation）、梯度近似（Gradient Approximation）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "- 14.1节中的讨论：\n",
    "  - 跳元模型的主要思想是使用softmax运算来计算基于给定的中心词$w_c$生成上下文词$w_o$的条件概率（如公式14.1.4），对应的对数损失在公式14.1.7给出。\n",
    "  - 由于softmax操作的性质，上下文词可以是词表$\\mathcal{V}$中的任意项，公式14.1.7包含与整个词表大小一样多的项的求和。\n",
    "  - 因此， **公式14.1.8中跳元模型的梯度计算和公式14.1.15中的连续词袋模型的梯度计算都包含求和**。\n",
    "  - 不幸的是，在**一个词典上（通常有几十万或数百万个单词）求和的梯度的计算成本是巨大的**！\n",
    "- 为了降低上述计算复杂度，本节将介绍两种近似训练方法：**负采样**和**分层softmax**。\n",
    "- 由于跳元模型和连续词袋模型的相似性，我们将以跳元模型为例来描述这两种近似训练方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "## 14.2.1 负采样\n",
    "\n",
    "\n",
    "负采样修改了原目标函数。给定中心词$w_c$的上下文窗口，任意上下文词$w_o$来自该上下文窗口的被认为是由下式建模概率的事件：\n",
    "\n",
    "$$P(D=1\\mid w_c, w_o) = \\sigma(\\mathbf{u}_o^\\top \\mathbf{v}_c),\\tag{14.2.1}$$\n",
    "\n",
    "其中$\\sigma$使用了sigmoid激活函数的定义：\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+\\exp(-x)}.\\tag{14.2.2}$$\n",
    "\n",
    "\n",
    "让我们从最大化文本序列中所有这些事件的联合概率开始训练词嵌入。具体而言，给定长度为$T$的文本序列，以$w^{(t)}$表示时间步$t$的词，并使上下文窗口为$m$，考虑最大化联合概率：\n",
    "\n",
    "$$ \\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m,\\ j \\neq 0} P(D=1\\mid w^{(t)}, w^{(t+j)}).\\tag{14.2.3}$$\n",
    "\n",
    "\n",
    "然而， 公式14.2.3只考虑那些**正样本的事件**。仅当所有词向量都等于无穷大时，公式14.2.3中的联合概率才最大化为1。当然，这样的结果毫无意义。为了使目标函数更有意义，**负采样**添加从预定义分布中采样的负样本。\n",
    "\n",
    "用$S$表示上下文词$w_o$来自中心词$w_c$的上下文窗口的事件。对于这个涉及$w_o$的事件，从预定义分布$P(w)$中采样$K$个不是来自这个上下文窗口**噪声词**。用$N_k$表示噪声词$w_k$（$k=1, \\ldots, K$）不是来自$w_c$的上下文窗口的事件。假设正例和负例$S, N_1, \\ldots, N_K$的这些事件是相互独立的。负采样将公式14.2.3中的联合概率（仅涉及正例）重写为\n",
    "\n",
    "$$ \\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m,\\ j \\neq 0} P(w^{(t+j)} \\mid w^{(t)}),\\tag{14.2.4} $$\n",
    "\n",
    "通过事件$S, N_1, \\ldots, N_K$近似条件概率：\n",
    "\n",
    "$$ P(w^{(t+j)} \\mid w^{(t)}) =P(D=1\\mid w^{(t)}, w^{(t+j)})\\prod_{k=1,\\ w_k \\sim P(w)}^K P(D=0\\mid w^{(t)}, w_k).\\tag{14.2.5}$$\n",
    "\n",
    "\n",
    "分别用$i_t$和$h_k$表示词$w^{(t)}$和噪声词$w_k$在文本序列的时间步$t$处的索引。公式14.2.5中关于条件概率的对数损失为：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "-\\log P(w^{(t+j)} \\mid w^{(t)})\n",
    "=& -\\log P(D=1\\mid w^{(t)}, w^{(t+j)}) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log P(D=0\\mid w^{(t)}, w_k)\\\\\n",
    "=&-  \\log\\, \\sigma\\left(\\mathbf{u}_{i_{t+j}}^\\top \\mathbf{v}_{i_t}\\right) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log\\left(1-\\sigma\\left(\\mathbf{u}_{h_k}^\\top \\mathbf{v}_{i_t}\\right)\\right)\\\\\n",
    "=&-  \\log\\, \\sigma\\left(\\mathbf{u}_{i_{t+j}}^\\top \\mathbf{v}_{i_t}\\right) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log\\sigma\\left(-\\mathbf{u}_{h_k}^\\top \\mathbf{v}_{i_t}\\right).\n",
    "\\end{aligned} \\tag{14.2.6}\n",
    "$$\n",
    "\n",
    "我们可以看到，现在每个训练步的梯度计算成本与词表大小无关，而是线性依赖于$K$。当将超参数$K$设置为较小的值时，在负采样的每个训练步处的梯度的计算成本较小。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "- **要点：**\n",
    "  - 负采样是一种用于训练词嵌入的技术，旨在解决标准的词嵌入训练在大词汇量上计算成本过高的问题。\n",
    "  - 负采样通过引入负样本的采样来提高词嵌入模型的训练效率，使得模型的训练不再直接依赖于整个词表的大小，而是依赖于较小的负样本集合大小$K$，从而解决了大规模词汇集上的计算成本问题。\n",
    "  - **目标概率模型**：给定中心词$w_c$和上下文词$w_o$，通过sigmoid函数定义的概率$P(D=1\\mid w_c, w_o) = \\sigma(\\mathbf{u}_o^\\top \\mathbf{v}_c)$描述了$w_o$出现在$w_c$上下文中的概率。\n",
    "  - **最大化联合概率**：训练目标是最大化文本序列中所有正样本（即实际上下文词对）事件的联合概率$\\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m,\\ j \\neq 0} P(D=1\\mid w^{(t)}, w^{(t+j)})$。\n",
    "  - **负采样方法**：为了使目标函数更实用，引入负采样技术，即从预定义分布$P(w)$中采样$K$个负样本（噪声词），假设所有正样本和负样本的事件是相互独立的。\n",
    "  - **替代条件概率公式**：使用事件$S$（上下文词来自中心词的上下文窗口）和噪声词事件$N_k$，负采样将原有的条件概率公式14.2.3替换成$\\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m,\\ j \\neq 0} P(w^{(t+j)} \\mid w^{(t)})$，通过这些事件近似。\n",
    "  -  **对数损失函数**：条件概率的对数损失函数定义为$-\\log P(w^{(t+j)} \\mid w^{(t)}) = -  \\log\\, \\sigma\\left(\\mathbf{u}_{i_{t+j}}^\\top \\mathbf{v}_{i_t}\\right) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log\\sigma\\left(-\\mathbf{u}_{h_k}^\\top \\mathbf{v}_{i_t}\\right)$，涉及正样本和采样的负样本。\n",
    "  -  **计算复杂性**：负采样方法将每步训练的梯度计算复杂性从依赖于词表大小降低到线性依赖于$K$。通过适当选择较小的$K$值，每步训练的计算成本大大降低。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "- **说明：负采样的原理与步骤**\n",
    "  - **负采样的基本原理：**\n",
    "    - **问题背景**：在原始的跳元模型中，为了学习词向量，试图最大化正样本（即上下文窗口中的词对）的联合概率。为了达到这个目标，需要使用softmax来计算整个词汇表中每个词的概率，这在大型词汇表中是非常消耗计算资源的。\n",
    "    - **目标函数的问题**：只考虑正样本的情况下，模型的目标函数会试图最大化所有正样本的联合概率。然而，如果只考虑正样本，这个目标函数的最优解是使所有词向量的值都变得非常大，这样的结果是没有意义的。\n",
    "    - **负采样的引入**：为了解决上述问题，负采样被引入。除了考虑正样本，还随机选择一些负样本（或称为噪声词），这些词不在上下文窗口中。目标不再是最大化整个词汇表的联合概率，而是最大化正样本的概率，并最小化负样本的概率。\n",
    "  - **负采样实现的步骤：**\n",
    "    - **定义概率**：\n",
    "      - 对于一个正样本（中心词 $w_c$ 和上下文词 $w_o$），定义这个词对出现的概率为：\n",
    "    $$P(D=1\\mid w_c, w_o) = \\sigma(\\mathbf{u}_o^\\top \\mathbf{v}_c).$$\n",
    "        - 其中，$\\sigma(x)$ 是sigmoid函数。\n",
    "    - **定义目标函数**：\n",
    "      - 最初，只考虑正样本，目标函数试图最大化所有正样本的联合概率：\n",
    "      $$ \\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m,\\ j \\neq 0} P(D=1\\mid w^{(t)}, w^{(t+j)}).$$\n",
    "      - 为了引入负样本，我们重新定义目标函数为：\n",
    "      $$ \\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m,\\ j \\neq 0} P(w^{(t+j)} \\mid w^{(t)}),$$\n",
    "       - 其中条件概率是通过正样本和$K$个负样本来近似的：\n",
    "      $$ P(w^{(t+j)} \\mid w^{(t)}) =P(D=1\\mid w^{(t)}, w^{(t+j)})\\prod_{k=1,\\ w_k \\sim P(w)}^K P(D=0\\mid w^{(t)}, w_k).$$\n",
    "    - **计算损失函数**：\n",
    "      - 使用上述定义的条件概率，我们可以定义损失函数为：\n",
    "      $$-\\log P(w^{(t+j)} \\mid w^{(t)}).$$\n",
    "      - 这样的损失函数旨在最大化正样本的概率并最小化负样本的概率。\n",
    "    - **采样负样本**：\n",
    "      - 从预定义的分布 $P(w)$ 中随机选择 $K$ 个噪声词作为负样本。\n",
    "      - 这个分布通常根据单词频率的某种权重来选择，例如，使用单词频率的3/4次方作为权重。\n",
    "    - **梯度下降**：\n",
    "      - 使用上述损失函数进行梯度下降来更新词向量。\n",
    "      - 由于只考虑了一个正样本和$K$个负样本，每次更新的计算成本与$K$线性相关，而与词汇表大小无关。\n",
    "  - 负采样通过引入对抗性负样本，从根本上改变了优化问题的几何结构：\n",
    "    - 打破对称性：防止所有向量坍缩到同一方向\n",
    "    - 引入对比学习：迫使模型区分信号与噪声\n",
    "    - 稳定数值优化：平衡梯度的**推拉**作用\n",
    "    - 这种机制使得词向量能够收敛到具有**语义区分度**的稳定状态，而非趋向无穷大的病态解。\n",
    "    - 实验表明，当负采样数$K=5-20$之间 时，即可获得高质量的语义表示。\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "## 14.2.2 层序Softmax\n",
    "\n",
    "作为另一种近似训练方法，**层序Softmax（hierarchical softmax）** 使用二叉树（图14.2.1中说明的数据结构），其中树的每个叶节点表示词表$\\mathcal{V}$中的一个词。\n",
    "<center><img src='../img/hi-softmax.svg'></center>\n",
    "<center>图14.2.1 用于近似训练的分层softmax，其中树的每个叶节点表示词表中的一个词</center><br>\n",
    "\n",
    "用$L(w)$表示二叉树中表示字$w$的从根节点到叶节点的路径上的节点数（包括两端）。设$n(w,j)$为该路径上的$j^\\mathrm{th}$节点，其上下文字向量为$\\mathbf{u}_{n(w, j)}$。例如，图14.2.1中的$L(w_3) = 4$。层序softmax将公式14.1.4中的条件概率近似为\n",
    "\n",
    "$$P(w_o \\mid w_c) = \\prod_{j=1}^{L(w_o)-1} \\sigma\\left( [\\![  n(w_o, j+1) = \\text{leftChild}(n(w_o, j)) ]\\!] \\cdot \\mathbf{u}_{n(w_o, j)}^\\top \\mathbf{v}_c\\right),\\tag{14.2.7}$$\n",
    "\n",
    "其中函数$\\sigma$在公式14.2.2中定义，$\\text{leftChild}(n)$是节点$n$的左子节点：如果$x$为真，$[\\![x]\\!] = 1$;否则$[\\![x]\\!] = -1$。\n",
    "\n",
    "\n",
    "为了说明，让我们计算图14.2.1中给定词$w_c$生成词$w_3$的条件概率。这需要$w_c$的词向量$\\mathbf{v}_c$和从根到$w_3$的路径（图14.2.1中加粗的路径）上的非叶节点向量之间的点积，该路径依次向左、向右和向左遍历：\n",
    "\n",
    "$$P(w_3 \\mid w_c) = \\sigma(\\mathbf{u}_{n(w_3, 1)}^\\top \\mathbf{v}_c) \\cdot \\sigma(-\\mathbf{u}_{n(w_3, 2)}^\\top \\mathbf{v}_c) \\cdot \\sigma(\\mathbf{u}_{n(w_3, 3)}^\\top \\mathbf{v}_c).\\tag{14.2.8}$$\n",
    "\n",
    "由$\\sigma(x)+\\sigma(-x) = 1$，它认为基于任意词$w_c$生成词表$\\mathcal{V}$中所有词的条件概率总和为1：\n",
    "\n",
    "$$\\sum_{w \\in \\mathcal{V}} P(w \\mid w_c) = 1.\\tag{14.2.9}$$\n",
    "\n",
    "\n",
    "幸运的是，由于二叉树结构，$L(w_o)-1$大约与$\\mathcal{O}(\\text{log}_2|\\mathcal{V}|)$是一个数量级。当词表大小$\\mathcal{V}$很大时，与没有近似训练的相比，使用分层softmax的每个训练步的计算代价显著降低。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "-------------\n",
    "- **说明：公式14.2.7，14.2.8，14.2.9的说明**\n",
    "  - （1）**公式 14.2.7:** 此公式表示了给定中心词$w_c$下，生成外部词$w_o$的条件概率。  \n",
    "    - $L(w_o)$：从二叉树的根到词$w_o$的路径上的节点数。\n",
    "    - $n(w_o, j)$：从根到词$w_o$路径上的第j个节点。\n",
    "    - $\\mathbf{u}_{n(w_o, j)}$：第j个节点的向量表示。\n",
    "    - $\\mathbf{v}_c$：中心词$w_c$的向量表示。\n",
    "    - $\\sigma$：sigmoid函数，将输入值压缩到0到1之间。\n",
    "    - 核心思想是，层序softmax不是直接计算词$w_o$的概率，而是按照从根到这个词的路径进行计算。\n",
    "    - 对于路径上的每一个转向（向左或向右），都要计算一次概率。\n",
    "  - （2）**公式 14.2.8:** 此公式是14.2.7公式的一个实例化。\n",
    "    - 假设我们要计算生成词$w_3$的概率，该词在二叉树中的路径是左、右、左。因此需要计算三次概率。\n",
    "  - （3）**公式 14.2.9:**\n",
    "    - 该公式表示，给定中心词$w_c$，所有可能的外部词的条件概率之和为1。\n",
    "    - 这符合概率的基本定义，即一个事件的所有可能结果的概率之和应该为1。\n",
    "  - **举例:**\n",
    "    - 假设词汇表中只有五个词: \\{'a', 'b', 'c', 'd', 'e'\\}。而我们的Huffman树结构如下（只为说明，并非真实的Huffman树）:\n",
    "    ```\n",
    "       ROOT\n",
    "       /   \\\n",
    "      N1   'a'\n",
    "     /  \\\n",
    "   'b'  N2\n",
    "        /  \\\n",
    "      'c' 'd'\n",
    "    ```\n",
    "    - 'c'的路径是左、右、左，因此$P('c' \\mid w_c)$ 为:\n",
    "    $$\\sigma(\\mathbf{u}_{N1}^\\top \\mathbf{v}_c) \\cdot \\sigma(-\\mathbf{u}_{N2}^\\top \\mathbf{v}_c)$$  \n",
    "      其中，$\\mathbf{u}_{N1}$和$\\mathbf{u}_{N2}$是节点N1和N2的向量表示，$\\mathbf{v}_c$是中心词$w_c$的向量表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- **公式14.2.7中$[\\![ n(w_o, j+1) = \\text{leftChild}(n(w_o, j)) ]\\!]$的具体涵义**\n",
    "  - 这个记号 $[\\![ n(w_o, j+1) = \\text{leftChild}(n(w_o, j)) ]\\!]$是一个指示函数，代表了一个逻辑条件的真假值。\n",
    "  - 公式里的$[\\![ \\cdot ]\\!]$ 是一个**指示函数 (indicator function)** ：\n",
    "    - 如果里面的条件为真，它返回 1。\n",
    "    - 如果条件为假，它返回 -1。\n",
    "  - 在这个特定的上下文中，条件$n(w_o, j+1) = \\text{leftChild}(n(w_o, j))$ 检查路径上的下一个节点$n(w_o, j+1)$是否是当前节点$n(w_o, j)$的左子节点。\n",
    "  - 因此：\n",
    "    - 如果$n(w_o, j+1)$是$n(w_o, j)$的左子节点，这个函数返回 1。\n",
    "    - 否则，它返回 -1（意味着它是右子节点）。\n",
    "  - 这在层次Softmax中很有用，因为它帮助我们确定应该沿着二叉树的哪个方向（左或右）来计算特定词的概率。\n",
    "  - 用一个简单的Python代码表示：\n",
    "  ```python\n",
    "    def indicator_function(n_wo_jp1, n_wo_j, leftChild):\n",
    "        if n_wo_jp1 == leftChild(n_wo_j):\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "  ```\n",
    "    - 在这里，`leftChild`是一个函数，给定一个节点，返回它的左子节点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "## 小结\n",
    "\n",
    "* 负采样通过考虑相互独立的事件来构造损失函数，这些事件同时涉及正例和负例。训练的计算量与每一步的噪声词数成线性关系。\n",
    "* 分层softmax使用二叉树中从根节点到叶节点的路径构造损失函数。训练的计算成本取决于词表大小的对数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "- **附录：**\n",
    "- **（1）负采样训练词嵌入示例**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 243.70747685432434\n",
      "Epoch 10, Loss: 206.1396586894989\n",
      "Epoch 20, Loss: 167.03803253173828\n",
      "Epoch 30, Loss: 156.81356525421143\n",
      "Epoch 40, Loss: 151.44770860671997\n",
      "Epoch 50, Loss: 149.59182167053223\n",
      "Epoch 60, Loss: 146.5918196439743\n",
      "Epoch 70, Loss: 141.97285223007202\n",
      "Epoch 80, Loss: 142.299910902977\n",
      "Epoch 90, Loss: 139.7790914773941\n",
      "like: tensor([ 0.5667, -0.3364, -0.9349,  0.5797, -0.3512,  0.3652, -0.0976,  0.9614,\n",
      "        -0.1616,  0.4742])\n",
      ".: tensor([ 0.9286,  0.1652, -0.7736,  0.0022, -0.8984,  0.0215,  0.6049,  0.7349,\n",
      "         0.0325,  0.1382])\n",
      "I: tensor([-0.0333,  0.7369, -0.4710,  0.8120,  0.3424, -0.1277,  0.7740,  0.7851,\n",
      "         0.3807, -0.0899])\n",
      "language: tensor([ 0.3295,  0.4894,  0.0388,  0.9904,  0.9861,  0.5428, -0.0172,  1.3430,\n",
      "         0.4378, -0.0017])\n",
      "love: tensor([ 0.5455, -0.5593, -0.6580,  0.4329, -0.3569,  0.3043,  1.0169,  0.2203,\n",
      "         1.0975,  0.6139])\n",
      "processing: tensor([ 0.3267, -0.5965, -0.4262,  0.6374,  0.0799,  0.2888,  0.0458,  1.1187,\n",
      "        -0.0632,  0.5898])\n",
      "enjoy: tensor([0.9938, 0.2143, 0.0216, 0.1876, 0.2179, 0.1730, 0.7648, 0.4775, 1.0037,\n",
      "        0.5225])\n",
      "natural: tensor([ 1.1273,  0.1994, -0.1613,  0.7644, -0.2822, -0.0706, -0.4842,  1.4380,\n",
      "        -0.2151,  0.0543])\n",
      "deep: tensor([ 0.3400, -0.6532, -0.0876,  0.1564, -0.0040,  0.1676,  1.3227,  0.9567,\n",
      "         0.2573,  0.0175])\n",
      "learning: tensor([-0.0752, -0.0031, -0.0613,  0.6714, -0.7146, -0.5891,  0.9323,  0.7564,\n",
      "         0.9357,  0.2118])\n",
      "machine: tensor([ 0.0617,  0.0369, -0.3866,  0.6803,  0.0716, -0.2392,  0.8443,  0.5344,\n",
      "         0.3314,  1.1882])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# 示例文本数据\n",
    "text = \"I like natural language processing . I enjoy deep learning . I love machine learning .\"\n",
    "tokens = text.split()\n",
    "vocab = set(tokens)\n",
    "vocab_size = len(vocab)\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "# 计算词频\n",
    "word_counts = Counter(tokens)\n",
    "total_words = sum(word_counts.values())\n",
    "word_freqs = {word: count / total_words for word, count in word_counts.items()}\n",
    "\n",
    "# 负采样分布：P(w) = count(w)^(3/4) / sum(count(w)^(3/4))\n",
    "pow_freqs = {word: count**0.75 for word, count in word_counts.items()}\n",
    "sum_pow = sum(pow_freqs.values())\n",
    "neg_sample_dist = {word: pow_freq / sum_pow for word, pow_freq in pow_freqs.items()}\n",
    "neg_sample_words = list(neg_sample_dist.keys())\n",
    "neg_sample_probs = list(neg_sample_dist.values())\n",
    "\n",
    "# 超参数\n",
    "embedding_dim = 10\n",
    "window_size = 2\n",
    "K = 5  # 每个正样本的负样本数\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "# 初始化词向量\n",
    "center_embeddings = nn.Embedding(vocab_size, embedding_dim)  # v_c\n",
    "context_embeddings = nn.Embedding(vocab_size, embedding_dim)  # u_o\n",
    "nn.init.xavier_uniform_(center_embeddings.weight)\n",
    "nn.init.xavier_uniform_(context_embeddings.weight)\n",
    "\n",
    "# 优化器\n",
    "optimizer = optim.SGD(list(center_embeddings.parameters()) + list(context_embeddings.parameters()), lr=learning_rate)\n",
    "\n",
    "# 训练\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for center_pos in range(len(tokens)):\n",
    "        center_word = tokens[center_pos]\n",
    "        center_idx = word_to_idx[center_word]\n",
    "        \n",
    "        # 获取上下文窗口\n",
    "        context_window = range(max(0, center_pos - window_size), min(len(tokens), center_pos + window_size + 1))\n",
    "        context_indices = [word_to_idx[tokens[pos]] for pos in context_window if pos != center_pos]\n",
    "        \n",
    "        for context_idx in context_indices:\n",
    "            # 正样本损失: -log σ(u_o^T v_c)\n",
    "            # 公式14.2.6的前半部分\n",
    "            u_o = context_embeddings(torch.tensor(context_idx))\n",
    "            v_c = center_embeddings(torch.tensor(center_idx))\n",
    "            pos_score = torch.sigmoid(torch.dot(u_o, v_c))\n",
    "            pos_loss = -torch.log(pos_score + 1e-10)  # 避免 log(0)\n",
    "            \n",
    "            # 负采样\n",
    "            neg_samples = np.random.choice(neg_sample_words, size=K, p=neg_sample_probs, replace=True)\n",
    "            neg_indices = [word_to_idx[word] for word in neg_samples]\n",
    "            \n",
    "            # 负样本损失: -sum log σ(-u_k^T v_c)\n",
    "            # 公式14.2.6的后半部分\n",
    "            neg_loss = 0\n",
    "            for neg_idx in neg_indices:\n",
    "                u_k = context_embeddings(torch.tensor(neg_idx))\n",
    "                neg_score = torch.sigmoid(-torch.dot(u_k, v_c))\n",
    "                neg_loss += -torch.log(neg_score + 1e-10)\n",
    "            \n",
    "            # 总损失\n",
    "            loss = pos_loss + neg_loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # 反向传播\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss}\")\n",
    "\n",
    "# 获取训练后的词向量\n",
    "trained_embeddings = center_embeddings.weight.data\n",
    "for word, idx in word_to_idx.items():\n",
    "    print(f\"{word}: {trained_embeddings[idx]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **（2）层序Softmax训练词嵌入示例**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 136.1333657503128\n",
      "Epoch 10, Loss: 132.61492264270782\n",
      "Epoch 20, Loss: 129.64000010490417\n",
      "Epoch 30, Loss: 126.78521597385406\n",
      "Epoch 40, Loss: 123.95440828800201\n",
      "\n",
      "条件概率验证：\n",
      "P(learning|deep): 0.1252\n",
      "P(learning|love): 0.1218\n",
      "P(deep|learning): 0.091\n",
      "\n",
      "最终词嵌入：\n",
      "'like': [-0.1786  0.6203  0.1859  0.5665 -0.1942 -0.2948 -0.4107  0.4355  0.5674\n",
      "  0.1885]\n",
      "'machine': [-0.3554 -0.2893  0.1604 -0.266  -0.2376  0.0983 -0.2284 -0.533   0.2093\n",
      "  0.282 ]\n",
      "'deep': [ 0.2617 -0.4536 -0.108  -0.6072 -0.2735 -0.0053 -0.573   0.2474 -0.4212\n",
      " -0.002 ]\n",
      "'enjoy': [ 0.1784 -0.0188  0.1447  0.4478  0.2927 -0.3438 -0.2447 -0.1335 -0.0073\n",
      "  0.3231]\n",
      "'language': [-0.017   0.3465 -0.1796 -0.2872 -0.2294  0.0135  0.6566 -0.4332  0.8518\n",
      "  0.1279]\n",
      "'processing': [-0.5243  0.1001  0.1129  0.0735  0.0754  0.059  -0.1835  0.284   0.679\n",
      "  0.2856]\n",
      "'.': [-0.7736 -0.0764  1.0543 -0.6797  0.1605  0.4038  0.1273 -0.2054 -0.0908\n",
      "  0.0843]\n",
      "'love': [-0.1232  0.3277 -0.2041 -0.0485 -0.5394 -0.4037 -0.2045 -0.0509 -0.4663\n",
      "  0.4051]\n",
      "'learning': [ 0.4258 -0.3998  0.0592  0.3065 -0.6419 -0.1085 -0.1651  0.5407 -0.4519\n",
      " -0.5425]\n",
      "'natural': [-1.0037  0.0019 -0.4436  0.3182  0.1469 -0.248   0.0601  0.3721 -0.46\n",
      " -0.5146]\n",
      "'I': [-0.1973 -0.5521 -0.0533  0.2048 -0.1873  0.4085  0.4723  0.3929  0.4868\n",
      "  0.071 ]\n",
      "\n",
      "霍夫曼树路径示例：\n",
      "'deep': 路径节点=[20, 19, 16, 13], 方向编码=[0, 1, 0, 0]\n",
      "'learning': 路径节点=[20, 18, 15], 方向编码=[1, 0, 1]\n",
      "'machine': 路径节点=[20, 19, 17, 14], 方向编码=[0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "# 示例文本数据\n",
    "text = \"I like natural language processing . I enjoy deep learning . I love machine learning .\"\n",
    "tokens = text.split()\n",
    "vocab = list(set(tokens))\n",
    "vocab_size = len(vocab)\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for idx, word in enumerate(vocab)}\n",
    "\n",
    "# 构建霍夫曼树（频率越高路径越短）\n",
    "def build_huffman_tree(word_counts):\n",
    "    \"\"\"构建霍夫曼树，对应公式14.2.7中的二叉树结构\"\"\"\n",
    "    # 创建叶节点\n",
    "    nodes = [{'word': word, 'count': count, 'left': None, 'right': None, 'index': i} \n",
    "             for i, (word, count) in enumerate(word_counts.items())]\n",
    "    node_count = len(nodes)\n",
    "    \n",
    "    # 构建优先队列（最小堆）\n",
    "    heap = nodes.copy()\n",
    "    heap.sort(key=lambda x: x['count'])\n",
    "    \n",
    "    # 构建内部节点\n",
    "    while len(heap) > 1:\n",
    "        # 取出两个最小频率的节点\n",
    "        left = heap.pop(0)\n",
    "        right = heap.pop(0)\n",
    "        \n",
    "        # 创建新内部节点\n",
    "        new_node = {\n",
    "            'word': None,\n",
    "            'count': left['count'] + right['count'],\n",
    "            'left': left,\n",
    "            'right': right,\n",
    "            'index': node_count\n",
    "        }\n",
    "        node_count += 1\n",
    "        \n",
    "        # 插入回堆中\n",
    "        heap.append(new_node)\n",
    "        heap.sort(key=lambda x: x['count'])\n",
    "    \n",
    "    return heap[0], node_count\n",
    "\n",
    "# 计算词频\n",
    "word_counts = defaultdict(int)\n",
    "for word in tokens:\n",
    "    word_counts[word] += 1\n",
    "\n",
    "# 构建霍夫曼树\n",
    "huffman_tree, total_nodes = build_huffman_tree(word_counts)\n",
    "\n",
    "# 为每个词预计算路径和方向\n",
    "word_paths = {}\n",
    "word_codes = {}\n",
    "\n",
    "def traverse(node, path=[], code=[]):\n",
    "    \"\"\"遍历霍夫曼树，记录每个词的路径和方向编码\"\"\"\n",
    "    if node['word'] is not None:\n",
    "        word_paths[node['word']] = path.copy()  # 存储路径节点索引\n",
    "        word_codes[node['word']] = code.copy()  # 存储方向编码（1=左，0=右）\n",
    "        return\n",
    "    \n",
    "    traverse(node['left'], path + [node['index']], code + [1])   # 左为1\n",
    "    traverse(node['right'], path + [node['index']], code + [0])  # 右为0\n",
    "\n",
    "traverse(huffman_tree)\n",
    "\n",
    "# 超参数设置\n",
    "embedding_dim = 10\n",
    "window_size = 2\n",
    "learning_rate = 0.01\n",
    "epochs = 50\n",
    "\n",
    "# 初始化词向量\n",
    "center_embeddings = nn.Embedding(vocab_size, embedding_dim)  # 中心词向量v_c\n",
    "node_embeddings = nn.Embedding(total_nodes, embedding_dim)   # 树节点向量u_n\n",
    "\n",
    "# 初始化参数（Xavier初始化）\n",
    "nn.init.xavier_uniform_(center_embeddings.weight)\n",
    "nn.init.xavier_uniform_(node_embeddings.weight)\n",
    "\n",
    "# 优化器\n",
    "optimizer = optim.SGD(list(center_embeddings.parameters()) + list(node_embeddings.parameters()), lr=learning_rate)\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for center_pos in range(len(tokens)):\n",
    "        center_word = tokens[center_pos]\n",
    "        center_idx = torch.tensor([word_to_idx[center_word]])\n",
    "        \n",
    "        # 获取上下文窗口（公式14.2.7中的上下文词）\n",
    "        context_indices = []\n",
    "        start = max(0, center_pos - window_size)\n",
    "        end = min(len(tokens), center_pos + window_size + 1)\n",
    "        for pos in range(start, end):\n",
    "            if pos != center_pos:\n",
    "                context_word = tokens[pos]\n",
    "                context_indices.append(word_to_idx[context_word])\n",
    "        \n",
    "        # 处理每个上下文词\n",
    "        for context_idx in context_indices:\n",
    "            context_word = idx_to_word[context_idx]\n",
    "            \n",
    "            # 获取该词的霍夫曼编码路径（对应公式14.2.7中的n(w,j)）\n",
    "            path_indices = word_paths[context_word]  # 路径节点索引\n",
    "            path_codes = word_codes[context_word]    # 路径方向编码\n",
    "            \n",
    "            # 初始化概率\n",
    "            log_prob = 0.0\n",
    "            \n",
    "            # 获取中心词向量（公式14.2.7中的v_c）\n",
    "            v_c = center_embeddings(center_idx)  # shape: (1, embedding_dim)\n",
    "            \n",
    "            # 沿着路径计算概率（公式14.2.7的乘积实现）\n",
    "            for node_idx, code in zip(path_indices, path_codes):\n",
    "                # 获取当前节点向量（公式14.2.7中的u_n(w,j)）\n",
    "                u_n = node_embeddings(torch.tensor([node_idx]))  # shape: (1, embedding_dim)\n",
    "                \n",
    "                # 计算节点得分（公式14.2.7中的u_n(w,j)^T v_c）\n",
    "                score = torch.dot(u_n.squeeze(), v_c.squeeze())\n",
    "                \n",
    "                # 根据方向计算概率（公式14.2.7中的[[...]]项）\n",
    "                if code == 1:  # 左子节点\n",
    "                    # σ(u_n^T v_c)\n",
    "                    log_prob += torch.log(torch.sigmoid(score) + 1e-10)\n",
    "                else:  # 右子节点\n",
    "                    # σ(-u_n^T v_c) = 1 - σ(u_n^T v_c)\n",
    "                    log_prob += torch.log(torch.sigmoid(-score) + 1e-10)\n",
    "            \n",
    "            # 损失是负对数概率（公式14.2.7取负对数）\n",
    "            loss = -log_prob\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # 反向传播\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss}\")  \n",
    "\n",
    "# 验证：计算几个词的条件概率\n",
    "def calculate_prob(center_word, target_word):\n",
    "    \"\"\"计算P(target_word|center_word)，对应公式14.2.7的实现\"\"\"\n",
    "    if center_word not in word_to_idx or target_word not in word_to_idx:\n",
    "        return 0.0\n",
    "    \n",
    "    center_idx = torch.tensor([word_to_idx[center_word]])\n",
    "    v_c = center_embeddings(center_idx)\n",
    "    \n",
    "    path_indices = word_paths[target_word]  # 路径节点索引\n",
    "    path_codes = word_codes[target_word]    # 路径方向编码\n",
    "    \n",
    "    prob = 1.0\n",
    "    for node_idx, code in zip(path_indices, path_codes):\n",
    "        u_n = node_embeddings(torch.tensor([node_idx]))\n",
    "        score = torch.dot(u_n.squeeze(), v_c.squeeze())\n",
    "        if code == 1:\n",
    "            prob *= torch.sigmoid(score).item()  # 左子节点：σ(u_n^T v_c)\n",
    "        else:\n",
    "            prob *= torch.sigmoid(-score).item() # 右子节点：σ(-u_n^T v_c)\n",
    "    return prob\n",
    "\n",
    "# 示例验证（对应公式14.2.8的示例计算）\n",
    "print(\"\\n条件概率验证：\")\n",
    "print(\"P(learning|deep):\", np.round(calculate_prob(\"deep\", \"learning\"), 4))\n",
    "print(\"P(learning|love):\", np.round(calculate_prob(\"love\", \"learning\"), 4))\n",
    "print(\"P(deep|learning):\", np.round(calculate_prob(\"learning\", \"deep\"), 4))\n",
    "\n",
    "# 打印最终词嵌入\n",
    "print(\"\\n最终词嵌入：\")\n",
    "for word, idx in word_to_idx.items():\n",
    "    emb = center_embeddings.weight[idx].detach().numpy()\n",
    "    print(f\"'{word}': {np.round(emb, 4)}\")\n",
    "\n",
    "# 打印霍夫曼树路径示例\n",
    "print(\"\\n霍夫曼树路径示例：\")\n",
    "for word in [\"deep\", \"learning\", \"machine\"]:\n",
    "    if word in word_paths:\n",
    "        path = word_paths[word]\n",
    "        codes = word_codes[word]\n",
    "        print(f\"'{word}': 路径节点={path}, 方向编码={codes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
