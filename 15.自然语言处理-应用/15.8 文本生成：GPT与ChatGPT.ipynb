{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d22b773b-e7af-437e-a0d2-662ae0175aec",
   "metadata": {},
   "source": [
    "# 15.8 文本生成：GPT与ChatGPT\n",
    "- **目录**\n",
    "  - 15.8.1 GPT与ChatGPT概述\n",
    "  - 15.8.2 GPT模型架构\n",
    "  - 15.8.3 ChatGPT训练技术和过程\n",
    "    - 15.8.3.1 人类反馈强化学习\n",
    "    - 15.8.3.2 TAMER框架\n",
    "    - 15.8.3.3 ChatGPT的训练过程\n",
    "    - 15.8.3.4 GPT-4的训练技术\n",
    "  - 15.8.4 GPT实现\n",
    "    - 15.8.4.1 数据准备\n",
    "    - 15.8.3.2 构建模型\n",
    "    - 15.8.4.3 模型训练\n",
    "    - 15.8.4.4 文本生成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8b6bef-9e8e-4974-b483-842e96a2658b",
   "metadata": {},
   "source": [
    "## 15.8.1 GPT与ChatGPT概述\n",
    "\n",
    "- GPT是 OpenAI 的一系列预训练模型，GPT的全称是 **Generative Pre-Trained Transformer**。\n",
    "  - 顾名思义，GPT 的目标是通过 Transformer，使用预训练技术得到通用的语言模型。\n",
    "- 目前已经公布论文的有 GPT-1、GPT-2、GPT-3。\n",
    "- 最新的GPT-4有技术报告，但是技术细节公布的不多。\n",
    "- GPT-4 Turbo是GPT-4 的升级版本，发布于 2023 年 11 月 6 日，主要特点如下：\n",
    "  - 更大的上下文窗口：GPT-4 Turbo 具有 128K 的上下文长度，能够处理更长的文本内容，提升了对长文本的理解和处理能力。\n",
    "  - 更新的知识库：其知识库已更新至 2023 年 4 月，能够提供更及时和准确的信息。\n",
    "  - 多模态支持：支持文生图模型 DALL·E 3、具有视觉输入能力的 GPT-4 Turbo 以及新的声音合成模型（TTS）等多模态 API。\n",
    "  - 可定制微调：允许开发人员创建 ChatGPT 自定义版本，进行特定领域的预训练和强化学习后训练。\n",
    "- GPT-4o 是 OpenAI 发布的一款多模态大模型，于 2024 年 5 月 14 日发布。\n",
    "  - 其中“o”代表“omni”，该词意为“全能”，源自拉丁语“omnis”。\n",
    "  - GPT-4o 模型可以使 ChatGPT 能够处理 50 种不同的语言，并可以接受文本、音频和图像三者组合作为输入，并生成文本、音频和图像的任意组合输出。\n",
    "  - 可以在 232 毫秒内对音频输入做出反应，与人类在对话中的反应时间相近。\n",
    "  - 性能方面，在传统基准测试中，GPT-4o 在文本、推理和编码等方面实现了与 GPT-4 Turbo 级别相当的性能，同时在多语言、音频和视觉功能方面的表现分数也超过了之前的模型。\n",
    "  - 相较于 2023 年 11 月推出的 GPT-4 Turbo，GPT-4o 在处理速度上提升达到 200%，同时在价格上也下降了 50%，并分阶段集成至 OpenAI 的各个产品之中。\n",
    "- 在2025年2月OpenAI发布GPT-4.5。\n",
    "- 2025年1月国内DeepSeek AI助手上线正式提供服务，引起巨大轰动。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e39929-5be4-4892-93cf-65a1626a2bb0",
   "metadata": {},
   "source": [
    "- ChatGPT是构建在GPT之上的系列模型，早期ChatGPT基于GPT-3.5进行微调而成，ChatGPT Plus则是基于GPT-4。\n",
    "- OpenAI团队在GPT-3.5 基础上，使用**人类反馈强化学习（Reinforcement Learning from Human Feedback, RLHF）** 训练模型。\n",
    "  - 首先使用了人类标注师撰写约1.2w-1.5w条问答数据，并用其作为基础数据预训练。\n",
    "  - 随后让预训练好的**监督微调模型（Supervised Fine-Tuning, SFT）** 针对新问题列表生成若干条回答，并让人类标注师对这些回答进行排序。\n",
    "  - 这些回答的排名内容将以配对比较的方式生成一个新的**奖励模型（Reward Mode，RM）**。\n",
    "  - 最后让奖励模型在更大的数据集上重新训练SFT，并将最后两个步骤反复迭代以获得最终的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d1d894-7345-4e6d-b84c-df3bf6dfdca3",
   "metadata": {},
   "source": [
    "## 15.8.2 GPT模型架构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81c35a2-66e2-4d3f-a37b-512802efceae",
   "metadata": {},
   "source": [
    "- GPT其实并不是一种新型架构，其结构类似于transformer模型中的解码器，并在庞大的数据集上进行了训练。\n",
    "- 原始模型如下图所示：\n",
    "<center><img src='../img/15_8_1.png' width=800px></center>\n",
    "<center>图15.8.1 GPT模型图</center>\n",
    "- 注：上图来源于[此网址](https://zhuanlan.zhihu.com/p/604625917)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf2991d-9a0d-4c31-922f-232018714826",
   "metadata": {},
   "source": [
    "- 具体讲，transformer模型的Decoder部分包含MHA（多头注意力）和MMHA（掩码多头自注意力），而GPT只保留MMHA，去掉MMA。\n",
    "- 这确保了 GPT 只能关注上文的信息，从而达到单向模型的目的。\n",
    "- 如下图所示：\n",
    "<center><img src = '../img/15_8_3.png' width=400px></center>\n",
    "<center>图15.8.2 GPT模型与Encoder对比</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1854787-a552-44be-9e58-28bdb675d49b",
   "metadata": {},
   "source": [
    "- GPT-1通过**自左向右生成式**的构建预训练任务，然后得到一个通用的预训练模型，这个模型和BERT一样都可用来做下游任务的微调。\n",
    "  - GPT-1当时在9个NLP任务上取得了SOTA的效果，但GPT-1使用的模型规模和数据量都比较小，这也就促使了GPT-2的诞生。\n",
    "- 对比GPT-1，GPT-2并未在模型结构做大规模修改，只是使用了更多参数的模型和更多的训练数据。\n",
    "  - GPT-2最重要的思想是提出了“所有的有监督学习都是无监督语言模型的一个子集”的思想，这个思想也是**提示学习（Prompt Learning）** 的前身。\n",
    "  - GPT-2在诞生之初也引发了不少的轰动，它生成的新闻足以欺骗大多数人类，达到以假乱真的效果。\n",
    "- GPT-3被提出时，除了它远超GPT-2的效果外，更令人瞩目的是其1750亿参数量。\n",
    "  - GPT-3除了能完成常见的NLP任务外，研究者意外的发现GPT-3在写SQL，JavaScript等语言的代码，进行简单的数学运算也有不俗表现。\n",
    "  - GPT-3的训练使用了**情境学习（In-context Learning）**，它是一种**元学习（Meta-learning）** 。\n",
    "  - 元学习的核心思想在于通过少量的数据寻找一个合适的初始化范围，使得模型能够在有限的数据集上快速拟合，并获得不错的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fa0657-183a-4582-a1b8-95740f6f4f1a",
   "metadata": {},
   "source": [
    "-----------\n",
    "- **说明：元学习（Meta-learning）和情境学习（In-context Learning）？**\n",
    "  - 元学习（Meta-learning）和情境学习（In-context Learning）是两种复杂的学习概念，特别是在人工智能（AI）和机器学习（ML）领域中，这些理念被广泛探索和应用。\n",
    "  - 元学习（Meta-learning）：又被称为“学习的学习”，是指让机器学习模型学会如何更有效地学习的过程。这种学习方式的目的在于使模型能够通过较少的数据、较快的速度或更高的效率来学习新任务，而不是在每次面对新任务时都从头开始学习。元学习尝试找到模型学习任务的一般策略，以便当面对新的、未见过的任务时，能够快速适应。\n",
    "  - 元学习的关键在于找到有效的学习算法（学习策略），这可以通过多种方式实现，包括但不限于：\n",
    "    - **模型无关的元学习（Model-Agnostic Meta-Learning, MAML）**：这种方法旨在通过对一系列不同任务的学习，找到一个好的模型初始化，这个初始化使模型可以通过少量梯度更新步骤和少量样本就快速适应新任务。\n",
    "    - - **优化方法**：通过设计学习过程中的优化算法，来增强模型适应新任务的能力。这可能包括修改反向传播算法使其更适合新任务的快速学习。\n",
    "      - - **记忆方法**：利用外部记忆机制或增强内部表示来提升模型对以往任务的记忆能力，从而使模型能够在面对新任务时，利用过往的知识。\n",
    "  - 情境学习（In-context Learning）：是一种使模型能够根据提供在其输入中的信息（情境）来调整其行为的能力，特别是指无需显式重新训练或微调的情况下。\n",
    "    - 例如，最新一代的大型语言模型（如GPT-3等）就表现出了强大的情境学习能力：它们可以通过阅读一个问题的描述和相关的例子，然后直接在该情境中生成对应的答案或完成指定的任务。\n",
    "  - 情境学习的一个关键特点是模型的多功能性和灵活性，它能够理解并应对各种不同类型的请求，而不需要为每种请求单独训练一个专用模型。这种能力基于以下两点：\n",
    "    - **大量的训练数据**：模型在训练过程中看到了大量的语言结构和信息，因此能够理解和处理各式各样的输入。\n",
    "    - **强大的内部表示**：模型能够学会如何将输入的信息转化为内部表示，这些内部表示捕捉到了输入数据的关键特征和语义，使得模型可以在这些表示的基础上进行推理和生成答案。\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dac876b-c3e4-4376-92dd-837ff341c282",
   "metadata": {},
   "source": [
    "- 从GPT-1, GPT-2到GPT-3的结构演进如下：\n",
    "  - GPT-1:\n",
    "    - 12层transformer，每层12个注意力头。\n",
    "  - GPT-2的改进：\n",
    "    - GPT-2有48层，使用1600维向量进行词嵌入。\n",
    "    - 将层归一化移动到每个子块的输入，并在最终的自注意块后增加一层归一化。\n",
    "    - 修改初始化的残差层权重，缩放为原来的$1/\\sqrt N$，$N$是残差层的数量。\n",
    "    - 特征向量维数从768扩展到1600，词表扩大到50257。    \n",
    "  - GPT-3的改进：\n",
    "    - GPT-3有96层，每层有96个注意力头。\n",
    "    - GPT-3的单词嵌入大小从GPT-2的1600增加到12888。\n",
    "    - 上下文窗口大小从GPT-2的1024增加到GPT-3的2048。\n",
    "    - 采用**交替密度**和**局部带状稀疏注意力模式**。\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9a174e-3033-4d48-bdf0-b1cd661270f5",
   "metadata": {},
   "source": [
    "- GPT-1,GPT-2(xl),GPT-3参数对比表如下：\n",
    "\n",
    "\n",
    "| 参数                   | GPT-1              | GPT-2 (xl)         | GPT-3            |\n",
    "|------------------------|--------------------|--------------------|--------------------|\n",
    "| 参数量（Parameters）   | 117M               | 1.5B               | 175B               |\n",
    "| 层数（Layers）         | 12                 | 48                 | 96                 |\n",
    "| 注意力头数（Attention Heads） | 12             | 25                 | 96                 |\n",
    "| 嵌入维度（Embedding Dim）  | 768                | 1600               | 12288              |\n",
    "| 最大序列长度（Max Seq Length） | 512          | 1024               | 2048               |\n",
    "| 数据集大小 （Dataset Size）            | 5GB          | 40GB               | 45TB(处理前) 400B(处理后)             |\n",
    "\n",
    "- 参数解释\n",
    "  - **参数量（Parameters）**：模型中的可训练参数总数。GPT-3 的参数量是 GPT-1 的近 1500 倍。\n",
    "  - **层数（Layers）**：transformer 网络中的层数。GPT-3 的层数是 GPT-1 的 8 倍，捕捉更深层次的语言特征。\n",
    "  - **注意力头数（Attention Heads）**：每个transformer 层中的自注意力头的数量。GPT-3 的注意头数是 GPT-1 的 8 倍，显著提高了上下文理解能力。\n",
    "  - **嵌入维度（Embedding Dim）**：输入和注意力机制的向量维度。GPT-3 的嵌入维度远大于 GPT-1，允许其表示更复杂的信息。\n",
    "  - **最大序列长度（Max Seq Length）**：模型能够处理的最大输入序列长度。GPT-3 的最大序列长度是 GPT-1 的 4 倍，可以处理更长的文本输入。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd64e0bc-ddb5-44f1-b4d4-4fcc65779e9f",
   "metadata": {},
   "source": [
    "## 15.8.3 ChatGPT训练技术和过程\n",
    "从GPT-3.5开始，OpenAI加入了两种技术：**人类反馈强化学习**和**TAMER**框架。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805dbd30-92b0-40e0-9db6-e4340126840a",
   "metadata": {},
   "source": [
    "### 15.8.3.1 人类反馈强化学习\n",
    "- 人类反馈强化学习（Reinforcement Learning from Human Feedback, RLHF）通过将人类的评价和反馈融入到智能体的学习过程中，能够有效提高其在复杂和动态环境中的表现。\n",
    "- RLHF是一种结合人类反馈来强化机器学习模型的方法，它特别适用于训练在复杂、模糊或动态环境中表现优秀的智能体。\n",
    "- RLHF的核心思想是通过人类提供的指示、奖励或评分来逐步改进智能体的策略，以达到更符合人类预期或更优的行为表现。\n",
    "- 这种方法不仅能让智能体更快速地学习到高效的策略，还能借助人类智慧和经验来避开潜在的危险和误区，从而在许多实际应用中展现出广泛的前景和巨大的优势。\n",
    "- RLHF是GPT-3.5这个版本被引入的。\n",
    "  - 与GPT-3的主要区别在于，GPT-3.5新加入了被称为**人类反馈强化学习**的技术。\n",
    "  - 这一训练范式增强了人类对模型输出结果的调节，并且对结果进行了更具理解性的排序。\n",
    "- 在InstructGPT中，以下是“goodness of sentences”的评价标准。\n",
    "  - 真实性：是虚假信息还是误导性信息？\n",
    "  - 无害性：它是否对人或环境造成身体或精神上的伤害？\n",
    "  - 有用性：它是否解决了用户的任务？\n",
    "- RLHF核心概念和工作流程如下：\n",
    "  - **智能体（Agent）**: 执行操作并观察结果的主体。\n",
    "  - **环境（Environment）**: 智能体与之交互的外部世界，包含智能体的行动空间和状态空间。\n",
    "  - **行动（Action）**: 智能体在每个时间步选择的一种行为。\n",
    "  - **状态（State）**: 描述当前环境的各种信息和智能体的情境。\n",
    "  - **奖励（Reward）**: 从环境或人类反馈中得到的数值，用于指导智能体的学习过程。\n",
    "  - **策略（Policy）**: 智能体在特定状态下选择行动的规则或方法。\n",
    "- 应用RLHF的过程通常包括以下步骤：\n",
    "  - **智能体执行动作**：智能体在环境中通过一定的策略来选择和执行动作。\n",
    "  - **观察和反馈**：智能体观察执行动作后的结果。人类观察者对当前的状态和动作组合进行评价，给出反馈（如奖励或惩罚）。\n",
    "  - **更新策略**：智能体根据收到的反馈，调整其策略以在未来类似的情况下做出更优决策。\n",
    "  - **重复循环**：上述步骤重复进行，智能体通过不断试错和人类反馈不断改进。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585d2844-20d9-4619-9675-fd42c3d75c96",
   "metadata": {},
   "source": [
    "### 15.8.3.2 TAMER框架\n",
    "- **TAMER（Training an Agent Manually via Evaluative Reinforcement，评估式强化人工训练代理）** 框架，将人类标记者引入到Agents的学习循环中，可以通过人类向Agents提供奖励反馈（即指导Agents进行训练），从而快速达到训练任务目标。\n",
    "- TAMER可以将人类标记者的知识，以奖励信反馈的形式训练Agent，加快其快速收敛。\n",
    "- TAMER不需要标记者具有专业知识或编程技术，语料成本更低。通过TAMER+RL（Reinforcement Learning, 强化学习），借助人类标记者的反馈，能够增强从马尔可夫决策过程(Markov Decision Process, MDP) 奖励进行强化学习的过程。\n",
    "- TAMER架构在强化学习中的应用:\n",
    "  - 具体实现上，人类标记者扮演对话的用户和人工智能助手，提供对话样本，让模型生成一些回复，然后标记者会对回复选项打分排名，将更好的结果反馈回模型中。\n",
    "  - Agents同时从两种反馈模式中学习——人类强化和马尔可夫决策过程奖励作为一个整合的系统，通过奖励策略对模型进行微调并持续迭代。\n",
    "  - 在此基础上，ChatGPT 可以比 GPT-3 更好的理解和完成人类语言或指令，模仿人类，提供连贯的有逻辑的文本信息的能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a4a34e-7117-4eeb-bc0a-6735d01a99ce",
   "metadata": {},
   "source": [
    "### 15.8.3.3 ChatGPT的训练过程\n",
    "以GPT-3.5为例，ChatGPT的训练过程分为以下三个阶段：\n",
    "- 第一阶段：训练监督策略模型。\n",
    "  - GPT-3.5本身很难理解人类不同类型指令中蕴含的不同意图，也很难判断生成内容是否是高质量的结果。\n",
    "  - 为了让GPT-3.5初步具备理解指令的意图，首先会在数据集中随机抽取问题，由人类标注人员，给出高质量答案。\n",
    "  - 然后用这些人工标注好的数据来微调 GPT-3.5模型获得**SFT（Supervised Fine-Tuning）** 模型。\n",
    "  - 此时的SFT模型在遵循指令/对话方面已经优于 GPT-3，但不一定符合人类偏好。\n",
    "- 第二阶段：训练**奖励模型（Reward Mode，RM）**。\n",
    "  - 这个阶段的主要是通过人工标注训练数据（约33K个数据），来训练回报模型。\n",
    "  - 在数据集中随机抽取问题，使用第一阶段生成的模型，对于每个问题，生成多个不同的回答。\n",
    "  - 人类标注者对这些结果综合考虑给出排名顺序。这一过程类似于教练或老师辅导。\n",
    "  - 接下来，使用这个排序结果数据来训练奖励模型。\n",
    "  - 对多个排序结果，两两组合，形成多个训练数据对。\n",
    "  - RM模型接受一个输入，给出评价回答质量的分数。\n",
    "  - 这样，对于一对训练数据，调节参数使得高质量回答的打分比低质量的打分要高。\n",
    "- 第三阶段：采用**PPO（Proximal Policy Optimization，近端策略优化）** 强化学习来优化策略。\n",
    "  - PPO的核心思路在于将Policy Gradient中On-policy的训练过程转化为Off-policy，即将**在线学习** 转化为**离线学习**，这个转化过程被称之为Importance Sampling。\n",
    "  - 这一阶段利用第二阶段训练好的奖励模型，靠奖励打分来更新预训练模型参数。\n",
    "  - 在数据集中随机抽取问题，使用PPO模型生成回答，并用上一阶段训练好的RM模型给出质量分数。\n",
    "  - 把回报分数依次传递，由此产生策略梯度，通过强化学习的方式以更新PPO模型参数。\n",
    "  - 最后不断重复第二和第三阶段，通过迭代，会训练出更高质量的ChatGPT模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7518be81-1859-4f91-a41a-0fad40fca7f0",
   "metadata": {},
   "source": [
    "### 15.8.3.4 GPT-4的训练技术\n",
    "**GPT-4**模型及其系统产品**ChatGPT Plus**的参数规模据说已达1.8万亿。根据有关网上[技术文章](https://zhuanlan.zhihu.com/p/626463196)的研究，GPT-4技术方案可能采用了如下策略和算法：\n",
    "- **zero-shot**、**one-shot**和**few-shot**的学习能力：这个提升的理论依据很大可能是因为大模型的**涌现能力（emergent ability）**。\n",
    "- 逻辑推理能力：用到了大模型的**思维链（Chain of Thought，CoT）** 以及**自提升能力（Self-Improve Ability）** 。\n",
    "- 理解图像能力：推测借鉴了OpenAI著名的多模态模型**CLIP（对比语言-图像预处理，Contrastive Language–Image Pre-Training）** 或者是微软的多模态模型**KOSMOS-1**。\n",
    "- 更安全的文本生成能力：这一部分技术报告中介绍的比较多，主要是专家测试，幻觉检测以及**RBRM（基于规则的奖励模型，Rule-based Reward Model）**。\n",
    "- 更强的编程能力：推测这一部分借鉴了OpenAI的著名的代码生成模型：**CodeX**。\n",
    "- 处理其它语言的能力：推测可能借鉴了XLM等跨语言预训练模型的思想，或是因为涌现能力强化了GPT-4在其它语种上的表现效果。\n",
    "- 处理更长序列的能力：推测这一部分用到了处理长输入的模型**Transformer-XL**或者OpenAI提出的可以降低长数据复杂度的**Sparse Transformer**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da7647a-972c-471c-8ee9-fe5e5bb50038",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7ac2b5-2b61-4eab-9c55-551ef09ed21d",
   "metadata": {},
   "source": [
    "- **说明：**\n",
    "- **（1）何为对比语言-图像预训练CLIP？**\n",
    "  - **对比语言-图像预训练（Contrastive Language–Image Pre-Training，CLIP）** 是一种深度学习方法，用于同时理解图像内容和相关的文本信息。它通过对比学习的框架来优化模型，使得模型能够更好地将图像和对应的文本描述联系起来。\n",
    "  - CLIP能够大幅提高模型处理和理解图文信息任务的能力。\n",
    "  - 通过这种联合预训练方法，模型不仅能学习到丰富的视觉特征，还能学习到复杂的语义信息，从而在多种跨模态任务上实现更优的性能。\n",
    "  - 在对比语言-图像预训练中，主要目标是训练一个能够理解图像及其相关文本的表示的模型。\n",
    "  - 这种方法通常涉及到两个主要的组件：\n",
    "    - 一是视觉编码器，用于提取图像特征；\n",
    "    - 二是语言编码器，用于提取文本特征。\n",
    "    - 这两个编码器被同时训练，以确保它们能够生成相似的表示形式。\n",
    "    - 当输入的文本描述与图像内容相关时，模型会试图将二者的表示拉近；相反，如果输入的文本描述与图像不相关，模型则会推开二者的表示。\n",
    "  - CLIP技术细节如下：\n",
    "    - **对比损失函数**：这项技术的核心在于使用对比损失（Contrastive Loss），也称为三元组损失（Triplet Loss），来训练模型。这种损失函数鼓励模型使得相匹配的图像和文本对的表示更接近，同时使得不匹配的图像和文本对的表示相互远离。\n",
    "    - **多模态学习**：Contrastive Language–Image Pre-Training是一项多模态学习方法，因为它涉及到处理并理解两种或两种以上的模态（图像和文本）。这种多模态学习方法能够显著提高模型在多种任务上的表现，比如图像标注、视觉问题回答（Visual Question Answering, VQA）以及跨模态信息检索等。\n",
    "    - **预训练和微调**：这种技术通常包含两个阶段。首先，在预训练阶段，模型在大规模的图文配对数据集上进行训练，目的是学习通用的视觉-语言表示。然后，在微调阶段，模型在特定任务的较小数据集上进行进一步训练，以优化其在特定任务上的表现。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb66ed60-b3e0-4604-a376-92c9fdb8ff26",
   "metadata": {},
   "source": [
    "- **（2）何为涌现能力（emergent ability）？**\n",
    "  - 大语言模型的**涌现能力（emergent ability）** 是指当语言模型达到一定的规模和复杂性时，它们能够表现出原本在训练过程中未明确训练或设计的新能力和行为。\n",
    "  - 这些能力可能包括对新问题的理解、综合信息的能力，甚至一些创造性的任务执行，这些都是在训练数据中没有直接指导的。\n",
    "  - 以下是一些涌现能力的例子和详细说明：\n",
    "    - **更强的语境理解**：随着模型的规模增大，它们开始更准确地理解复杂的语境关系和更微妙的语言使用。这意味着大型模型能够根据上下文提供更精确的回应。\n",
    "    - **知识内化和推理**：大型语言模型在其庞大的数据库中积累了大量知识，并且随着规模扩展，模型能够更好地内化这些信息，并在必要时进行逻辑推理。\n",
    "    - **自我修正能力**：在某些情况下，大型模型会表现出能够从自身的错误中学习并进行自我修正的能力，即使在训练中没有特定地教给它们这样做\n",
    "    - **多步骤任务处理**：当语言模型的规模让它们能够处理更复杂的多步骤任务时，比如先进行研究再回答问题，这种能力没有被明确地教给模型，而是随着训练数据和参数规模的增长而自然出现的。\n",
    "    - **创造性生成**：对于如OpenAI的DALL·E这样的模型，随着规模的增长，它们开始表现出能够创造新图像的能力，这些图像不仅仅是对训练数据的复制，而是原创的、有创意的产物。\n",
    "    - **自然语言理解的深度**：大型模型能够理解和使用双关语、隐喻、幽默或其他复杂的语言表达方式。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f223b003-fc09-4404-bbf7-77b3f6f30c6c",
   "metadata": {},
   "source": [
    "- **（3）何为\"Zero-shot\"、\"One-shot\"和\"Few-shot\"？**\n",
    "  - \"Zero-shot\"、\"One-shot\"和\"Few-shot\"学习是指深度学习模型在不同数量的示例下进行学习的能力。\n",
    "  - **Zero-shot learning**：在此场景中，模型能够在没有任何具体例子即示例的情况下理解和执行新任务。模型利用已有的知识和理解来推断任务要求并试图给出正确的输出。这通常依赖于模型的泛化能力。\n",
    "    - 大语言模型如GPT可以进行zero-shot学习。如果你要求它回答一个问题，比如“谁是第一位踏上月球的人？”即使模型没有被明确地训练来回答这个特定的问题，它可能已经在背景材料或相关文本中学会了答案，因此可以给出正确答案，即尼尔·阿姆斯特朗。\n",
    "  - **One-shot learning**：在one-shot学习中，模型会看到一个示例，然后就需要执行与该示例相关的任务。模型根据单个实例理解任务要求，并应用于新的情况。\n",
    "     - 在使用GPT模型时，你可以给它一个示例，比如展示一个格式化的日期：“March 14, 1879 - Albert Einstein's birthdate”。然后询问，“April 15, 1452”，期望它识别这是描述日期和著名人物生日的方式，模型将从给出的一个样本中学习并尝试返回：“April 15, 1452 - Leonardo da Vinci's birthdate”。\n",
    "  - **Few-shot learning**：在few-shot学习场景下，模型会看到少量的示例来理解新任务。模型利用这些有限的情境来调整自己对任务的理解，并在新的情况中使用这个概念。\n",
    "    - 如果你问GPT一个分类问题，并给它几个示例分类，如：“苹果 - 水果，胡萝卜 - 蔬菜”，然后提出一个新项，“番茄”，模型将根据先前的示例推断番茄是个蔬菜（尽管生物学上是水果，但在烹饪中通常被视为蔬菜）。\n",
    "  - 在这些场景中，模型的预训练部分学习了大范围的语言模式和知识，这使得在没有专门针对新任务进行额外数据训练的情况下，模型还是能够处理这些任务。\n",
    "  - OpenAI的GPT模型特别擅长这些学习方法，因为它们在大规模数据集上进行了训练，从而理解了大量的概念和任务。这样的模型可以应用于各种不同的情境，只需很少或没有额外的示例来展示如何完成新任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5f9e1c-97c3-471c-8cea-31babdd3f67d",
   "metadata": {},
   "source": [
    "- **（4）何为RBRM（基于规则的奖励模型，Rule-based Reward Model）？**\n",
    "  - GPT使用的RBRM（基于规则的奖励模型，Rule-based Reward Model）技术是一种用于改善语言模型输出质量和安全性的方法。RBRM通过一组预定义的规则来评估模型的输出，并据此提供正向或负向的反馈。这些规则通常是由人类专家制定的，旨在引导模型生成更符合期望的响应。\n",
    "  - 以下是RBRM的一些关键特点和应用场景：\n",
    "    - **规则定义**：RBRM的核心是一组规则，这些规则定义了模型输出的期望属性，比如内容的准确性、适当性或安全性。这些规则可以是具体的指令，也可以是评价标准。\n",
    "    - **零样本分类器**：RBRM使用零样本（Zero-shot）分类器，这意味着它们不需要针对特定任务进行训练。这些分类器能够根据预定义的规则对行为或事件进行分类。\n",
    "    - **奖励信号**：在强化学习（Reinforcement Learning, RL）框架中，RBRM提供了额外的奖励信号，指导模型学习并优化其行为，使其更加符合既定的安全和质量标准。\n",
    "    - **多输入处理**：RBRM可以接受多种输入，包括提示（可选）、策略模型的输出，以及人类编写的评估准则。这些输入帮助模型理解任务要求，并据此生成响应。\n",
    "    - **分类输出**：RBRM将根据提供的规则集对模型的输出进行分类。例如，它可以指示模型将响应分类为期望的拒绝、不期望的拒绝、包含不允许的内容，或是安全且非拒绝的响应。\n",
    "    - **微调过程**：RBRM通常用于微调（Fine-tuning）阶段，在此阶段，模型通过与RBRM的交互学习如何更好地遵循规则并生成合适的输出。\n",
    "    - **安全性和质量控制**：RBRM有助于确保模型的输出不包含不当内容，如仇恨言论、歧视性语言或不准确的信息，从而提高模型的安全性和输出质量。\n",
    "    - **迭代改进**：通过RBRM的反馈，模型开发者可以识别和解决模型的潜在问题，不断迭代和改进模型的性能。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ba4036-df2a-4d3e-875a-b116125ccaf1",
   "metadata": {},
   "source": [
    "- **（4）何为Transformer-XL？**\n",
    "  - **Transformer-XL**是一种基于Transformer架构的预训练语言模型，它旨在解决传统Transformer模型在处理长序列数据时的长度限制问题。\n",
    "  - 在标准的Transformer模型中，由于其自注意力机制的限制，通常只能处理固定长度的序列，这被称为“长度限制”问题。\n",
    "  - Transformer-XL通过引入一种新颖的“可重复的缓存机制”来克服这一限制。\n",
    "  - Transformer-XL的关键特点包括：\n",
    "    - **可重复的缓存机制**：Transformer-XL引入了段级别的循环机制，允许模型在处理新的输入段时，重复使用之前段的隐藏状态。这种机制使得模型能够维持长距离依赖关系，并且能够处理比标准Transformer更长的序列。\n",
    "    - **前向缓存和后向缓存**：Transformer-XL使用前向缓存（将当前段的输出传递到下一个段）和后向缓存（将之前段的信息传递回当前段），这有助于在不同段之间传递信息，增强模型对长距离依赖的捕捉能力。\n",
    "    - **缓解长度限制**：通过这种方式，Transformer-XL能够处理比自身实际长度限制更长的序列，因为它可以利用之前处理的序列信息。\n",
    "    - **改善长距离依赖问题**：Transformer-XL特别适合于需要理解长距离依赖的语言任务，如文档摘要、文本生成等\n",
    "    - **预训练任务**：Transformer-XL通常在大量文本数据上进行预训练，以学习语言的通用表示，然后可以在特定任务上进行微调。\n",
    "    - **性能提升**：在多项自然语言处理任务上，Transformer-XL已经展现出优于标准Transformer和其他序列模型的性能。\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a924a1c-1ea7-4136-9b7f-408421a64ba8",
   "metadata": {},
   "source": [
    "- **（5）何为近端策略优化（Proximal Policy Optimization, PPO）**\n",
    "  - **近端策略优化**是一种用于**强化学习**的**策略优化算法**，由OpenAI提出。\n",
    "    - 该技术在现实应用中表现出色，被广泛用于训练复杂任务中的智能体。\n",
    "    - **PPO**在训练稳定性和样本效率方面进行了优化，是深度强化学习领域中最受欢迎和有效的算法之一。\n",
    "  - **核心思想**:\n",
    "    - PPO的主要目标是优化策略，使智能体在不断试错中学到最优的行为策略。\n",
    "    - 其核心思想是通过对策略进行小范围的更新，从而避免大幅度策略变化可能导致的不稳定性。\n",
    "  - PPO的**主要特点**：\n",
    "    - **基于策略的优化**：\n",
    "      - PPO是一种策略梯度方法，直接优化智能体的策略。\n",
    "      - 与值函数方法不同，策略方法直接优化策略而不需要通过值函数间接优化。\n",
    "    - **限制策略变化幅度**：\n",
    "      - PPO通过限制每次更新的策略变化，防止策略更新过度，保持训练过程的稳定性。\n",
    "    - **分阶段更新**：\n",
    "      - PPO在每个训练阶段中多次更新策略，而不是在每一步更新。\n",
    "      - 在一个学习周期结束后，利用所有的经验进行优化。\n",
    "  - **PPO的工作流程**：PPO的核心步骤包括采样、计算优势函数、构建目标函数和策略更新。\n",
    "    - **采样**：智能体在环境中执行一系列动作，收集经验样本，这些样本包括**状态、动作、奖励和下一状态**等信息。\n",
    "    - **计算优势函数（Advantage Function）**：优势函数用于衡量某个动作在某状态下相比其他动作的优越性。计算方式通常为：\n",
    "      $$A_t = R_t + \\gamma V(s_{t+1}) - V(s_t)$$\n",
    "      - $ R_t $ 是在时间步 $ t $ 获得的奖励。\n",
    "      - $ V(s_{t+1}) $ 和 $ V(s_t) $ 分别是状态 $ s_{t+1} $ 和 $ s_t $ 的值函数估计。\n",
    "      - $\\gamma$ 是折扣因子。\n",
    "    - **构建目标函数**: PPO的目标函数通过剪切损失函数的方式限制策略变化幅度，具体形式为：\n",
    "    $$L^{\\text{CLIP}}(\\theta) = \\mathbb{E}_t \\bigg[ \\min\\left( r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_t \\right) \\bigg] $$\n",
    "      - $ r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)} $ 是新的策略和旧的策略比率。\n",
    "      - $\\hat{A}_t$ 是优势函数估计值。\n",
    "      - $\\epsilon$ 是一个超参数，用于控制策略变化范围。\n",
    "      - $\\text{clip}(r, 1 - \\epsilon, 1 + \\epsilon)$ 用于限制目标函数内部比率的变化幅度。\n",
    "    - **策略更新**: 通过梯度下降法优化目标函数，以更新策略参数 $\\theta$。\n",
    "  - 优势和应用:\n",
    "    - **稳定性高**：通过限制每次更新的策略变化范围，PPO在训练过程中更加稳定，避免了策略剧烈波动带来的不稳定性。\n",
    "    - **样本效率高**：PPO在每个学习阶段多次使用采样数据，提高了样本利用效率。\n",
    "    - **实现简便**：PPO相较于一些复杂的策略优化算法，更易于实现且计算效率高。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f0f3b6-48e4-4fdf-b6f8-75fe4fce70bc",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3def3f18-bf7b-4bce-b24e-14d09b5faec9",
   "metadata": {},
   "source": [
    "## 15.8.4 GPT实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b01c7b-0823-438e-a885-b9ff6a2a09e2",
   "metadata": {},
   "source": [
    "- 本节使用GPT-2的实现为例进行说明，代码借鉴Andrej karpathy的[nanoGPT](https://github.com/karpathy/nanoGPT)。\n",
    "\n",
    "### 15.8.4.1 数据准备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3e330c-4900-42ee-95d1-fd53f86c6106",
   "metadata": {},
   "source": [
    "- 为了最快了解GPT的原理以及实战技巧，karpathy提供了一个训练GPT莎士比亚作品的数据集。\n",
    "- 首先，我们将其下载为单个(1MB)文件，并将其从原始文本转换为一个大的整数流。\n",
    "- 训练参数：\n",
    "  - 以字符为单位的数据集长度:1115394\n",
    "  - 所有唯一性字符，即词表中的词元:\n",
    "    - $ !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz$\n",
    "  - vocab size: 65\n",
    "  - train有1003854个词元\n",
    "  - val有111540个词元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92e432d6-9dfd-4cd2-bea1-1dbecd1a0a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 1,115,394\n",
      "all the unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab size: 65\n",
      "train has 1,003,854 tokens\n",
      "val has 111,540 tokens\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "准备字符级的文本数据\n",
    "\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "# 下载莎士比亚微型数据集，并文本字符串读入到data变量\n",
    "input_file_path = '../data/gpt2/tinyshakespeare_input.txt'\n",
    "with open(input_file_path, 'r') as f:\n",
    "    data = f.read()\n",
    "print(f\"length of dataset in characters: {len(data):,}\")\n",
    "\n",
    "\n",
    "# 获取文本中的所有字符，包括大小写字母和标点符号\n",
    "# 作为词表的词元，是字符级词元\n",
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "print(\"all the unique characters:\", ''.join(chars))\n",
    "print(f\"vocab size: {vocab_size:,}\")\n",
    "\n",
    "# 创建字符和整数之间的映射集\n",
    "stoi = { ch:i for i,ch in enumerate(chars) } # 字符到整数\n",
    "itos = { i:ch for i,ch in enumerate(chars) } #整数到字符\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s] # 编码器: 输入一个字符串, 输出一个整数列表\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l]) # 解码器: 输入一个整数列表, 输出一个字符串\n",
    "\n",
    "# 将数据分割成训练集和测试集\n",
    "n = len(data)\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]\n",
    "\n",
    "# 将训练和测试验证集编码成整数\n",
    "train_ids = encode(train_data)\n",
    "val_ids = encode(val_data)\n",
    "print(f\"train has {len(train_ids):,} tokens\")\n",
    "print(f\"val has {len(val_ids):,} tokens\")\n",
    "\n",
    "# 将训练和测试集的ID输出到二进制文件，\n",
    "# 通俗讲就是将文本数据集编码成词表对应的整数后，输出到二进制文件\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "train_ids.tofile(os.path.join('../data/gpt2', 'train.bin'))\n",
    "val_ids.tofile(os.path.join('../data/gpt2', 'val.bin'))\n",
    "\n",
    "# 保存元信息，以帮助稍后进行的编码/解码操作\n",
    "meta = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'itos': itos,\n",
    "    'stoi': stoi,\n",
    "}\n",
    "with open(os.path.join('../data/gpt2', 'meta.pkl'), 'wb') as f:\n",
    "    pickle.dump(meta, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79e36150-1133-42e4-adda-89a6369e161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 字符与整数之间的映射\n",
    "#stoi,itos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c0e4cb-72f9-410b-8f3f-e3cc7f6caea1",
   "metadata": {},
   "source": [
    "### 15.8.3.2 构建模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5108a0-759d-4cda-8f4c-d483720bffcb",
   "metadata": {},
   "source": [
    "- 构建GPT2模型\n",
    "- 模型的组件包括\n",
    "  - 层归一化\n",
    "  - 因果自注意力\n",
    "  - 多层感知机"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f643d6e-a008-45cc-b963-4e146ced4494",
   "metadata": {},
   "source": [
    "- **层归一化**\n",
    "  - 该代码块定义了一个名为`LayerNorm`的类，它继承自`torch.nn.Module`，是一个PyTorch模块，用来实现层归一化（Layer Normalization）功能。\n",
    "  - **层归一化操作**：   层归一化是对神经网络层的输入进行归一化处理，使其具有0的均值和单位方差。\n",
    "    - 这种归一化是在特征维度上进行的，而不像批归一化是在批量数据维度上。归一化可以帮助减少内部协变量的移动，从而使模型训练更加稳定。\n",
    "  - **归一化参数**：\n",
    "    - `self.weight`：一个可学习的参数，提供了对归一化后数据进行缩放的能力。初始化为全1，表示在训练开始时不对归一化后的值进行缩放。\n",
    "    - `self.bias`：一个可学习的参数，提供了对归一化后数据进行位移的能力。如果`bias`参数为真，则初始化为全0，表示在训练开始时不对归一化后的值进行偏移。\n",
    "  - **前向传播（`forward`方法）**：   在前向传播当中，对输入数据`input`应用层归一化。\n",
    "    - 方法`F.layer_norm`是一个调用PyTorch函数库中的层归一化函数。\n",
    "    - 参数包括输入数据、归一化时要考虑的形状（维度）、缩放权重、偏移偏置以及归一化时考虑的数值稳定性而添加的微小常数（epsilon）1e-5。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48be2c75-698c-4b6e-8ebc-9eba56d745f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from contextlib import nullcontext\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    '''\n",
    "    层归一化。\n",
    "    与OpenAI使用TensorFlow开发的官方模型不同，\n",
    "    Pytorch模型不带bias=False选项\n",
    "    '''\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea0e40e-64ee-489a-b9b3-edf8eb61513c",
   "metadata": {},
   "source": [
    "- **因果自注意力模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47b44c20-e385-4d2b-8785-9338d3379c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    '''\n",
    "    自注意力模型的实现\n",
    "    '''\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0 # 确保嵌入维度能被头的数量整除\n",
    "        # 自注意力所有头的key,query和value映射，以批量为单位\n",
    "        # 使用一个线性层将输入嵌入映射到3倍的嵌入维度，本例：768->3*768\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # 输出映射，再次使用一个线性层来映射自注意力操作的输出到原始的嵌入维度\n",
    "        # 768->768\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # 模型正则化，设置Dropout层,dropout=0.2,即保留20%的单元\n",
    "        self.attn_dropout = nn.Dropout(config.dropout) # 应用于注意力权重上\n",
    "        self.resid_dropout = nn.Dropout(config.dropout) # 应用于自注意力层输出\n",
    "        # 注意力头数，嵌入的维度，Dropout的比例参数\n",
    "        self.n_head = config.n_head # 12\n",
    "        self.n_embd = config.n_embd # 768\n",
    "        self.dropout = config.dropout # 0.2\n",
    "        # flash attention：优化内存使用和计算速度\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # 因果掩码，以确保注意力只应用于输入序列中的左侧\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # 批量大小, 序列长度, 嵌入维度 (n_embd)\n",
    "\n",
    "        # 计算批处理中所有自注意力头的查询、键、值，并将自注意力头的维度和序列长度的维度互换\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # 自注意力: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # 使用 Flash Attention CUDA核，实现更有效的自注意力机制\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # 手工实现自注意力\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) # 缩放自注意力\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf')) # 掩码自注意力\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "            \n",
    "        # 注意contiguous的用法，tensor多次transpose会使其丧失连续型，通过contiguous可以将其恢复连续型     \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # 重组所有的自注意力输出，恢复原来形状和维度\n",
    "           \n",
    "        # 输出映射，同样也需要使用dropout\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50ea441-8044-40a0-b2ea-5cebecd4ebe8",
   "metadata": {},
   "source": [
    "- MLP类作为一个前馈网络层的实现。\n",
    "  - 在Transformer架构中，这样的前馈网络常见于每一个注意力模块之后的位置，并对序列中的每个位置都执行相同的操作。\n",
    "  - 这是一个完全连接的网络层，通常用于**特征的非线性变换**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c96ab24e-eead-41ea-8e68-06b7797ea66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    多层感知机作为前馈网络层存在。\n",
    "    '''\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU() # Gaussian Error Linear Unit激活函数\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da401120-01ad-482e-9c05-a01b58b8f1bc",
   "metadata": {},
   "source": [
    "- GPTConfig是一个辅助类，用于在命令行端运行时，接受输入的参数值，并覆盖各参数的默认值。\n",
    "  - 本节对代码进行修改，使之可在notebook中直接运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9b79bf9-cfda-4b16-90ad-ef2728c9028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 65 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba45901-b87b-4658-bcb4-6b7c68bfc490",
   "metadata": {},
   "source": [
    "- 构建Decoder块，正如前文所述，和transformer原始解码器的结构有所区别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df4bb8b7-535d-4d19-9641-b33ee4c0a4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT解码器块，和transformer的原始解码器有所区别\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e8cdcd-46af-4ad0-a194-d6a7fcba0917",
   "metadata": {},
   "source": [
    "- GPT-2\n",
    "  - 包括模型的构建、前向传播、参数初始化、优化器配置以及如何用模型进行文本生成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86badf94-eed3-4134-98a9-13f38bd17f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2模型\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None # 决定模型捕捉词元依赖关系的距离\n",
    "        self.config = config # 使用GPTConfig对象配置模型参数，一般来自命令行输入\n",
    "        \n",
    "        # 构造解码器层\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd), # 词嵌入\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd), # 位置嵌入\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),# 6层解码器\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        ))\n",
    "        # 最后一个线性层 ，用于将解码器层的输出投影回词表空间，以便计算最终的词元概率分布。\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)           \n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # 初始化所有参数\n",
    "        self.apply(self._init_weights)\n",
    "        # 对残差投影应用特殊缩放初始化\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # 报告参数个数\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        '''\n",
    "        返回模型中参数的个数。\n",
    "        （1）默认情况下（即在不计算嵌入层参数的情况下），位置嵌入（position embeddings）的参数数量会被从总数中减去。\n",
    "             位置嵌入用于为序列模型中每个元素提供其位置信息。\n",
    "             在计算总参数数量时，可能需要把位置嵌入的参数数量排除在外。\n",
    "        （1）通常情况下，如果进行非嵌入层的参数计数，也会从总数中减去词嵌入（token embeddings）的参数，\n",
    "             就像位置嵌入一样。\n",
    "             然而，由于参数共享（parameter sharing）的原因，这些词嵌入参数实际上在模型的最后一层被用作权重。\n",
    "             所以，尽管词嵌入是嵌入的一部分，通常还是将其参数计入参数总数。\n",
    "        '''\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    # 前向传播\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t},  block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "        # 前向传播模型\n",
    "        tok_emb = self.transformer.wte(idx) # 词嵌入的形状 (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # 位置嵌入的形状 (t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb) # 两个嵌入相加\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        # 如果提供了 targets，则计算交叉熵损失；否则，如果是生成任务，则不需要计算损失。\n",
    "        # 分别运行在训练阶段和生成阶段\n",
    "        if targets is not None:\n",
    "            # 根据给定的期望目标，计算损失\n",
    "            logits = self.lm_head(x)\n",
    "            # 交叉熵损失\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # 推断时间的微型优化(mini-optimization):只转发最后一个位置的lm_head\n",
    "            logits = self.lm_head(x[:, [-1], :]) # 注意: 使用list [-1] 保存序列维度的大小，即序列的长度\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss   \n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        # 从所有候选参数开始\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # 过滤掉不需要的梯度\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        '''\n",
    "        （1）decay_params: 如果参数的维度大于等于2（通常指的是矩阵，如权重矩阵），\n",
    "             则认为这些参数需要应用权重衰减。权重衰减主要用于正则化，可以减少模型的过拟合。\n",
    "        （2）nodecay_params: 如果参数的维度小于2（通常是向量，如偏置项和层规范化的参数），\n",
    "             这些参数不应用权重衰减。\n",
    "        （3）len(decay_params)：表示“被权重衰减的参数组中参数矩阵的数量”。\n",
    "             换句话说，它统计的是有多少个权重矩阵被包含在应用权重衰减的参数列表中。\n",
    "             例如，如果你有两个卷积层和一个全连接层，这可能会是三——每层一个权重矩阵。\n",
    "        （4）num_decay_params: 这表示“在所有应用权重衰减的参数矩阵中的总参数数量”。\n",
    "             这是统计具体有多少个浮点数（权重）将要被优化。计算方法是将每个参数矩阵的元素数量(numel)加起来。\n",
    "             如果有一个有1000个元素的权重矩阵和一个有500个元素的权重矩阵，则 num_decay_params将会是1500。\n",
    "        '''\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "\n",
    "        '''\n",
    "        创建AdamW优化器，决定是否使用fused操作。\n",
    "        如果AdamW的fused参数可用，并且device_type是'cuda'，则use_fused将被设置为True。\n",
    "        fused版本的AdamW将执行梯度下降算法中的几个步骤合并成一个GPU操作：\n",
    "        参数的更新（包括momentum和variance的计算、权重衰减和参数更新）是作为单个内核运行，而非若干个离散步骤。\n",
    "        这能够减少GPU内核之间的上下文切换以及减少对全局内存的读写次数。\n",
    "        '''\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "        return optimizer\n",
    "\n",
    "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
    "        \"\"\" 以A100 bfloat16峰值flops为单位估计模型的flops利用率(MFU) \"\"\"\n",
    "        # 首先估计每次迭代的flops次数        \n",
    "        N = self.get_num_params()\n",
    "        cfg = self.config\n",
    "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
    "        flops_per_token = 6*N + 12*L*H*Q*T\n",
    "        flops_per_fwdbwd = flops_per_token * T\n",
    "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
    "        # 将我们的flops吞吐量表示为A100 bfloat16峰值flops的比率\n",
    "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
    "        flops_promised = 312e12 # A100 GPU bfloat16峰值flops为312 TFLOPS\n",
    "        mfu = flops_achieved / flops_promised\n",
    "        return mfu\n",
    "        \n",
    "    @torch.no_grad() # 停止梯度传播\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        取索引idx(形状为(b,t)的LongTensor)的条件序列，并完成序列max_new_tokens，每次将预测反馈回模型。\n",
    "        最好确保在model.eval()模式下执行此操作。\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # 如果序列上下文长得太长，此处必须裁剪为block_size长度\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # 前向传播模型以获得序列中索引的logit，模型预测下一个token的每个可能值的原始分数\n",
    "            logits, _ = self(idx_cond)\n",
    "            '''\n",
    "            logits是神经网络最后一层输出的原始分数，是一个三维张量，其形状通常为 (batch_size, \n",
    "            sequence_length, vocab_size)：\n",
    "              - batch_size表示一批处理的数据大小。\n",
    "              - sequence_length表示序列的长度，或者说是已经生成的token的数量。\n",
    "              - vocab_size是模型词汇表的大小，即模型可以选择的下一个token的所有可能值。\n",
    "            当进行文本生成时，每次都基于当前的序列（context）来预测下一个token。\n",
    "            因此对于每个序列，都只关心其最后一个位置的logits，也就是最新生成的token的分数，\n",
    "            因为这个位置的分数将用来预测下一个token，这就是logits[:, -1, :]的取法原因。\n",
    "            '''\n",
    "            logits = logits[:, -1, :] / temperature # \n",
    "            # 将logits裁剪为仅前k个选项，此步是可选的\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # 应用softmax将对数转换为(规范化)概率\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            '''\n",
    "             根据概率分布进行随机采样的一个函数。它的作用是从给定的概率分布中随机抽取样本。\n",
    "             这个函数在文本生成任务中特别有用，因为它可以根据模型输出的概率分布来选择下一个词。\n",
    "             基于提供的概率分布（第一个参数张量的每一行），随机地从这些分布中抽取num_samples个样本。\n",
    "             这对于基于预测概率执行随机决策的场景非常有用，如文本生成中的下一个词的选择，\n",
    "             因为它允许模型引入随机性，避免每次都选择概率最大的词，而是给予较小概率的词一定的机会，\n",
    "             这有助于生成更多样化和有趣的文本。\n",
    "            '''\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # 将抽样索引附加到运行序列并继续\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1aad14f0-d863-4444-89b7-2a8254d78039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'params': <Parameter \"params: Union[Iterable[torch.Tensor], Iterable[Dict[str, Any]]]\">,\n",
       "              'lr': <Parameter \"lr: Union[float, torch.Tensor] = 0.001\">,\n",
       "              'betas': <Parameter \"betas: Tuple[float, float] = (0.9, 0.999)\">,\n",
       "              'eps': <Parameter \"eps: float = 1e-08\">,\n",
       "              'weight_decay': <Parameter \"weight_decay: float = 0.01\">,\n",
       "              'amsgrad': <Parameter \"amsgrad: bool = False\">,\n",
       "              'maximize': <Parameter \"maximize: bool = False\">,\n",
       "              'foreach': <Parameter \"foreach: Optional[bool] = None\">,\n",
       "              'capturable': <Parameter \"capturable: bool = False\">,\n",
       "              'differentiable': <Parameter \"differentiable: bool = False\">,\n",
       "              'fused': <Parameter \"fused: Optional[bool] = None\">})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect.signature函数的用法\n",
    "inspect.signature(torch.optim.AdamW).parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963b16ef-b5fd-4cf1-87f1-ad8842f14568",
   "metadata": {},
   "source": [
    "### 15.8.4.3 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dc7f0a-e990-48ce-b5da-7a5b38809800",
   "metadata": {},
   "source": [
    "- 本示例主要是演示GPT-2模型结构，因此使用karpathy提供的莎士比亚数据进行训练。\n",
    "- 此微型模型是一个字符级，主要为了便于在PC机上进行训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7054c488-adbc-4367-8278-9459fd5f105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "# 最低硬件条件训练的一个微型GPT-2模型\n",
    "out_dir = '../data/gpt2'\n",
    "eval_interval = 50 # 评估间隔\n",
    "eval_iters = 200\n",
    "log_interval = 10 # 日志间隔\n",
    "\n",
    "# 在小数据集上过拟合，然后便于在验证集上训练有改善时才保存\n",
    "always_save_checkpoint = False\n",
    "\n",
    "bias = False\n",
    "dataset = '../data'\n",
    "gradient_accumulation_steps = 1\n",
    "batch_size = 64\n",
    "block_size = 256 # 最多256个字符的上下文\n",
    "\n",
    "# 微型GPT-2模型的参数\n",
    "n_layer = 6 # 解码器的层数\n",
    "n_head = 6 # 自注意力的头数\n",
    "n_embd = 384 # 嵌入的维数\n",
    "dropout = 0.2 # 暂退的比率\n",
    "\n",
    "learning_rate = 1e-3 # 对于微型模型，学习率设置比较高\n",
    "# max_iters = 5000 # 该参数原值，但是在PC机上运行较为缓慢\n",
    "max_iters = 400\n",
    "# lr_decay_iters = 5000 \n",
    "lr_decay_iters = 400 # 通常和max_iters参数值相同\n",
    "min_lr = 1e-4 # 通常等于learning_rate / 10\n",
    "init_from = 'scratch'\n",
    "warmup_iters = 100 # 并非很必要\n",
    "\n",
    "# device = 'cpu'  # 在CPU上运行\n",
    "compile = False # do not torch compile the model\n",
    "\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "device='cuda'\n",
    "dtype = 'float16'\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "weight_decay = 1e-1\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "data_dir = os.path.join(dataset,'gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb514346-de56-43df-a995-78cd42c8d0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_str = {'out_dir':'../data/gpt2',\n",
    "'eval_interval' : 250,\n",
    "'eval_iters' : 200,\n",
    "'log_interval' : 10,\n",
    "'always_save_checkpoint' : False,\n",
    "'bias' :False,\n",
    "'dataset' : '../data',\n",
    "'gradient_accumulation_steps' : 1,\n",
    "'batch_size' : 64,\n",
    "'block_size' : 256,\n",
    "'n_layer' : 6,\n",
    "'n_head' : 6,\n",
    "'n_embd' : 384,\n",
    "'dropout' : 0.2,\n",
    "'learning_rate' : 1e-3,\n",
    "'max_iters' : 400, \n",
    "'lr_decay_iters' : 400,\n",
    "'min_lr' : 1e-4 ,\n",
    "'init_from' : 'scratch',\n",
    "'warmup_iters' : 100,\n",
    "'compile':False,\n",
    "'iter_num':0,\n",
    "'best_val_loss':1e9,\n",
    "'device':'cuda',\n",
    "'dtype' : 'float16',\n",
    "'beta1' : 0.9,\n",
    "'beta2' : 0.95,\n",
    "'weight_decay' : 1e-1,\n",
    "'device_type' : 'cuda' ,\n",
    "'data_dir' :'../data/gpt2'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "848a2b8e-26c5-4ea3-b4bc-2998d5aca5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取批量\n",
    "def get_batch(split):\n",
    "    if split == 'train':\n",
    "        '''\n",
    "        函数使用 numpy的 memmap功能载入对应的二进制数据文件。\n",
    "        memmap创建了一个内存映射数组，它允许大文件被部分读入内存，\n",
    "        对于处理不能完全装入RAM的大型数据集特别有用。\n",
    "        '''\n",
    "        data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "    else:\n",
    "        data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "        \n",
    "    # 生成0到len(data) - block_size之间的随机数，size是batch_size，即批量的大小\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # 生成输入序列和标签的批次，本质上是对应token在词表中的索引\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    # 标签是输入序列中每个token右边紧邻的一个token，这也是自回归的基础\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # 使用 pin_memory()方法准备输入序列的Tensor，以便将之异步地传输到GPU\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e41660c-40f3-4f34-945e-e9758d613709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found vocab_size = 65 (inside ../data/gpt2/meta.pkl)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "感觉这些参数在训练模型时没用到。\n",
    "'''\n",
    "meta_path = '../data/gpt2/meta.pkl'\n",
    "meta_vocab_size = None\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "        #print('meta:----------',meta)\n",
    "    meta_vocab_size = meta['vocab_size']\n",
    "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
    "config_str['meta_path'] = meta_path\n",
    "config_str['meta_vocab_size'] = meta_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cc7d052-3aef-498d-9dc4-c3c21832a14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing a new model from scratch\n",
      "number of parameters: 10.65M\n"
     ]
    }
   ],
   "source": [
    "# 模型初始化\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=None, dropout=dropout) # 设置模型参数\n",
    "print(\"Initializing a new model from scratch\")\n",
    "model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
    "gptconf = GPTConfig(**model_args)\n",
    "#print(gptconf) \n",
    "model = GPT(gptconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ec5861f-29b9-4cff-863b-3e6eee59ff62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 26, with 10,740,096 parameters\n",
      "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
      "using fused AdamW: True\n"
     ]
    }
   ],
   "source": [
    "model.to(device) # 将模型转移到到GPU\n",
    "# 初始化GradScaler对象，如果enabled设为False，则梯度伸缩器则不被操作\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "# 如果在GPU上运行，则在float32和float16两个精度之间自动转换\n",
    "# 在启用AMP时通常使用float16来尝试加速计算并减少内存占用\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "# 配置优化器\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "checkpoint = None # 释放内存\n",
    "\n",
    "# 估计损失\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx: #在GPU上自动使用float16计算\n",
    "                _, loss = model(X, Y) # GPT类的forward方法第二个返回值即loss\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# 学习率衰减调度器(带预热的余弦算法)\n",
    "def get_lr(it):\n",
    "    # 1) 学习率从0开始线性增加到初始学习率，防止模型参数更新过快，确保模型的稳定性\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) 如果it > lr_decay_iters, 返回最小学习率\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) 当前迭代次数在预热和最大衰减迭代之间，函数将使用余弦衰减公式来计算学习率。\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    # 实现周期性变化，即开始减小然后增大，在下一个周期再次减小\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3f12ac7-0d1f-411d-b7df-df9bd926af66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练循环\n",
    "X, Y = get_batch('train') # 获取第一个批量\n",
    "t0 = time.time()\n",
    "local_iter_num = 0 # 本进程生命周期中的迭代次数\n",
    "decay_lr = True # 是需要设置学习衰减\n",
    "running_mfu = -1.0\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "eval_only = False # 如果为True，则训练脚本在第一次eval之后退出\n",
    "grad_clip = 1.0 # 使用参数裁剪梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb68f4da-86a6-4502-9ba4-c824877e5bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.3926, val loss 4.3856\n",
      "iter 0: loss 4.3597, time 317620.96ms, mfu -100.00%\n",
      "iter 10: loss 3.2321, time 75.37ms, mfu 4.94%\n",
      "iter 20: loss 2.8081, time 34.00ms, mfu 5.55%\n",
      "iter 30: loss 2.6389, time 41.00ms, mfu 5.90%\n",
      "iter 40: loss 2.5645, time 36.00ms, mfu 6.34%\n",
      "step 50: train loss 2.5136, val loss 2.5185\n",
      "saving checkpoint to ../data/gpt2\n",
      "iter 50: loss 2.5301, time 352987.16ms, mfu 5.71%\n",
      "iter 60: loss 2.5249, time 36.99ms, mfu 6.15%\n",
      "iter 70: loss 2.4842, time 42.00ms, mfu 6.42%\n",
      "iter 80: loss 2.4915, time 61.70ms, mfu 6.38%\n",
      "iter 90: loss 2.4608, time 29.97ms, mfu 6.99%\n",
      "step 100: train loss 2.4474, val loss 2.4782\n",
      "saving checkpoint to ../data/gpt2\n",
      "iter 100: loss 2.4637, time 353625.14ms, mfu 6.29%\n",
      "iter 110: loss 2.4395, time 36.03ms, mfu 6.69%\n",
      "iter 120: loss 2.4470, time 71.00ms, mfu 6.55%\n",
      "iter 130: loss 2.4428, time 42.00ms, mfu 6.78%\n",
      "iter 140: loss 2.4260, time 33.98ms, mfu 7.20%\n",
      "step 150: train loss 2.3625, val loss 2.3981\n",
      "saving checkpoint to ../data/gpt2\n",
      "iter 150: loss 2.3909, time 362501.33ms, mfu 6.48%\n",
      "iter 160: loss 2.3567, time 66.00ms, mfu 6.40%\n",
      "iter 170: loss 2.3570, time 120.37ms, mfu 6.07%\n",
      "iter 180: loss 2.3004, time 64.04ms, mfu 6.04%\n",
      "iter 190: loss 2.2852, time 59.01ms, mfu 6.07%\n",
      "step 200: train loss 2.1909, val loss 2.2445\n",
      "saving checkpoint to ../data/gpt2\n",
      "iter 200: loss 2.2727, time 359927.68ms, mfu 5.46%\n",
      "iter 210: loss 2.2194, time 61.71ms, mfu 5.52%\n",
      "iter 220: loss 2.1980, time 50.20ms, mfu 5.71%\n",
      "iter 230: loss 2.1505, time 34.94ms, mfu 6.21%\n",
      "iter 240: loss 2.1210, time 59.39ms, mfu 6.21%\n",
      "step 250: train loss 2.0175, val loss 2.1007\n",
      "saving checkpoint to ../data/gpt2\n",
      "iter 250: loss 2.0760, time 368742.32ms, mfu 5.59%\n",
      "iter 260: loss 2.0819, time 45.49ms, mfu 5.85%\n",
      "iter 270: loss 2.0457, time 34.00ms, mfu 6.36%\n",
      "iter 280: loss 2.0147, time 38.00ms, mfu 6.71%\n",
      "iter 290: loss 2.0076, time 41.01ms, mfu 6.94%\n",
      "step 300: train loss 1.9068, val loss 2.0314\n",
      "saving checkpoint to ../data/gpt2\n",
      "iter 300: loss 1.9847, time 359508.34ms, mfu 6.25%\n",
      "iter 310: loss 1.9876, time 80.00ms, mfu 6.09%\n",
      "iter 320: loss 1.9382, time 43.69ms, mfu 6.33%\n",
      "iter 330: loss 1.9421, time 37.49ms, mfu 6.70%\n",
      "iter 340: loss 1.9178, time 35.48ms, mfu 7.08%\n",
      "step 350: train loss 1.8325, val loss 1.9660\n",
      "saving checkpoint to ../data/gpt2\n",
      "iter 350: loss 1.9133, time 362340.41ms, mfu 6.37%\n",
      "iter 360: loss 1.9265, time 33.75ms, mfu 6.84%\n",
      "iter 370: loss 1.8786, time 35.23ms, mfu 7.21%\n",
      "iter 380: loss 1.8848, time 21.84ms, mfu 8.20%\n",
      "iter 390: loss 1.8675, time 31.00ms, mfu 8.58%\n",
      "step 400: train loss 1.7869, val loss 1.9382\n",
      "saving checkpoint to ../data/gpt2\n",
      "iter 400: loss 1.8652, time 362856.62ms, mfu 7.72%\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # 确定和设置本次迭代的学习率\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # 评估训练或验证的损失，然后写进checkpoints里\n",
    "    if iter_num % eval_interval == 0 :\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config_str,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "        \n",
    "    #向前向后更新，如果数据类型为float16，可选择梯度累积来模拟更大的批处理大小并使用GradScaler\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "            loss = loss / gradient_accumulation_steps # 按比例计算损失，以说明梯度积累\n",
    "        # 当模型在GPU上进行前向传递时，立即异步预取下一批\n",
    "        X, Y = get_batch('train')\n",
    "        # 在fp16中进行梯度缩放训练\n",
    "        scaler.scale(loss).backward()\n",
    "    # 裁剪梯度\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    # 在fp16中步进优化器和缩放器的训练\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # 刷新梯度，然后释放内存\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # 计时与日志\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0:        \n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        if local_iter_num >= 5: # 让训练稳定下来\n",
    "            mfu = model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "    # 终止条件\n",
    "    if iter_num > max_iters:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590cbad2-39b8-4c13-9b98-2d49c8f12655",
   "metadata": {},
   "source": [
    "### 15.8.4.4 文本生成\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dc7774f-b62f-458c-b6e5-d635add1f48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "从以训练好模型中采样和生成文本\n",
    "\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import tiktoken\n",
    "#from model import GPTConfig, GPT\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "# -----------------------------------------------------------------------------\n",
    "# init_from = 'resume' # 'resume' 或者gpt2变体 (比如'gpt2-xl')\n",
    "out_dir = '../data/gpt2' # 如果init_from参数不是 'resume'，则忽略该参数\n",
    "start = \"\\n\" # 或者为 \"<|endoftext|>\" ，也可以指定为一个文件比如: \"FILE:prompt.txt\"\n",
    "num_samples = 10 # 生成的样本数\n",
    "max_new_tokens = 500 # 每个样本生成的词元数\n",
    "temperature = 0.8 # 在预测时，1.0 = 没变化, < 1.0 = 更小随机, > 1.0 = 更随机\n",
    "top_k = 200 # 只保留top_k个可能性最大的词元，将其他词元的概率设为0\n",
    "seed = 1337\n",
    "device = 'cuda' # 比如: 'cpu', 'cuda', 'cuda:0', 'cuda:1'\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' 或者 'bfloat16' 或者 'float16'\n",
    "compile = False # 使用Pytorch2.0以上版本支持compile功能，但是即便版本达到标准，在windows下也不是很可靠\n",
    "#exec(open('configurator.py').read()) # 使用命令行或配置文件中的参数覆盖默认参数\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a83178e-a8e9-4e63-b6a5-696a04786271",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)# 设置种子可以确保每次运行代码时都能产生相同的随机数序列\n",
    "torch.cuda.manual_seed(seed) #保证在CUDA上的操作有相同的随机性，尤其在使用GPU进行计算时\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # 它允许使用TensorFloat-32进行矩阵乘法matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' \n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32488509-48ab-402e-954f-04f3afd345fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 10.65M\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "# 从前面训练好的模型中加载模型参数文件\n",
    "ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "model = GPT(gptconf)\n",
    "state_dict = checkpoint['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7fc59b1-1e42-41be-898b-bf7dedf32c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "And thy bridce.\n",
      "\n",
      "STANUS:\n",
      "The madere my be to take rudter the hands:\n",
      "Whith foul heart. Whe dilth a endway, what fast to morous\n",
      "You some for the coul the will at miree,\n",
      "In con latist in overs, and the now they juse, less dience.\n",
      "\n",
      "KING RICHARD:\n",
      "Sup Maiss hiwhy hell and not of this death:\n",
      "Is wor moth keard in on her evicks the moses\n",
      "That in him he pood of his but thand thrup this ar is his shat thy ale of their Prience of my thy suk!\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "That he may in courrear tey I what I\n",
      "And Edve y\n",
      "---------------\n",
      "\n",
      "Men pand king my thou cont beent of a cemose.\n",
      "\n",
      "DUKE OF YORK:\n",
      "Artand the caucil son the grove, with courfe:\n",
      "I shout might to Enks fortul work.\n",
      "\n",
      "LUCIO:\n",
      "And thou to didings, to dess know thy see:\n",
      "He dous the gRiet, I you, may sheask not my sided.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Hatake begue here cowar som thy live on made so lack.\n",
      "\n",
      "Prive, I my have, his I stard poiven's ond Camme,\n",
      "Thou hat is is ofter not the summe of my and is be thand the some.\n",
      "\n",
      "BRTHUS:\n",
      "Thou so have reave is wat do it of the reft!\n",
      "Goode comente\n",
      "---------------\n",
      "\n",
      "Mere and it wee. \n",
      "Firstrut:\n",
      "The whe wher the couth ser this to dand:\n",
      "Why hoose and men, me we mines so tentlesing.\n",
      "\n",
      "GREO:\n",
      "I the have ast hide scal frien\n",
      "He be caunIn thy a caniet to may, the and in thim\n",
      "To me arteling foleire brom not my havere word,\n",
      "And the stably of He wheremperss brook!\n",
      "And to the ead just the to proke have,\n",
      "What the sten nies his; do bell treentungle'd you him\n",
      "The sould with vone you be to to the band I nowm;\n",
      "And the numbonm the eate you sle, grove soye you,\n",
      "But on hole the \n",
      "---------------\n",
      "\n",
      "The she fork will knith mart say, as and the work\n",
      "And scan Clan! Wenter this then word rist are gen wach preirs\n",
      "Where he knings the rettlest descemanrish dain?\n",
      "\n",
      "JULET:\n",
      "As is it his my mahtreathoass of the hathese\n",
      "But an her the the groves it brow thy be corcousecilt,\n",
      "And the to of matter with the say befaine,\n",
      "And of at but the peertievelife he pusing of how the cauld;\n",
      "Show now's nuch, thy prome, mand and seat,\n",
      "But hat well of to he sto come one:\n",
      "In is shall' dogkent me sue well whe be on of\n",
      "Marr\n",
      "---------------\n",
      "\n",
      "Beter:\n",
      "O bell thy him lorter crother in tir.\n",
      "\n",
      "KING RICHARD II:\n",
      "Why soe, but the ever dien to her do.\n",
      "\n",
      "GLOUCESTER:\n",
      "I'll to carce me I wen then, and wish mood\n",
      "The sid is it the evimne of our for miorth\n",
      "And your your mand his me the and them them\n",
      "And beent thee chame on the mandunest of thee, some and and your awce,\n",
      "And that thu sien all dently, is more can they da my son,\n",
      "What the be hearrow dos parks this a eard,\n",
      "And the move cionet the afforthand the your jear deest\n",
      "To not teeppost but warks for\n",
      "---------------\n",
      "\n",
      "\n",
      "MEONES:\n",
      "And the she son fore starl, theirs he true,\n",
      "And way days the cricess? What be deed in the warto come thy at\n",
      "The the wreath as be or more.\n",
      "\n",
      "MENENIUS:\n",
      "And look for you padty me weree sgeen,\n",
      "To like neat the me of it must,\n",
      "The you roth king they, and fore this bithis shond, here\n",
      "but have to muttant word, But then, I this not ithey gaideixh,\n",
      "That thou kine prome in thy treathey conthand,\n",
      "Ou to pritseers; dence the that be criuss you hath\n",
      "do so red Richmanst is dearth'd sto now farre.\n",
      "\n",
      "COMPE\n",
      "---------------\n",
      "\n",
      "Shat cone manurs, for forters,\n",
      "In these wich quing what sooding the king and,\n",
      "The with that shall the are fration their stands\n",
      "That know thou dricess ther a tiold the my of By\n",
      "Feres mor lorme not that lork of the sleaves:\n",
      "Thered who how shall give the hearts a hus ance of the ried\n",
      "ware the this the heard of then of he deave.\n",
      "\n",
      "BRUTHAST:\n",
      "How you fath and or hish lever a will to meerther,\n",
      "That gaing of well a dood, all tentlem'd and\n",
      "Oat of apple worce a carce a mark to is all we of that,\n",
      "Sthat of t\n",
      "---------------\n",
      "\n",
      "IS nother drealoing thou hat spay be sain,\n",
      "Ang and devess a comlan the of the rear stame the worth.\n",
      "\n",
      "First:\n",
      "Shich is the to grocts, this best the dience.\n",
      "\n",
      "MENENIUS:\n",
      "I duke do to mortarth so in she wor hady;\n",
      "Thou hir thou that sue deed noth coliss hose sidee.\n",
      "\n",
      "DUKE OF Eetter:\n",
      "Hiss the sbind rect in the death, the in on freague\n",
      "To the theyee to surter of you mort;\n",
      "And Why caust me met thanks a now here the fordien the words\n",
      "Thy sent her in in thus a dike my rectoness,\n",
      "In the a dome the sue cally w\n",
      "---------------\n",
      "\n",
      "Sell parcers on will shall my comman's beat and:\n",
      "All your like of thing shall seelf, ast your soosts\n",
      "hall have the starkl poce to lises on.\n",
      "\n",
      "QUEEN EN ELIZABELABETH:\n",
      "The mon bore hose gaing they dike his?\n",
      "\n",
      "KING RICHARD IIII:\n",
      "I will I rrost thee live is of crackaw thy we becterviond\n",
      "The shall veak he that in not fartaul, will of unstings,\n",
      "Why his he hereard the shalk it is in grive,\n",
      "And all the gione the to be of You\n",
      "Bot thou bronst to is it have at supart und:\n",
      "I the reave dry shee.\n",
      "\n",
      "KING RICHARD:\n",
      "---------------\n",
      "\n",
      "QUEEN OF HENRY IV:\n",
      "Say, behter my and spost, I well he wa leath a beatce,\n",
      "And but you brave a gainter stanter the the is heater\n",
      "This to thinge thou maing thy not the seats of the rento vicess\n",
      "There grish thou were his and your to make stainly\n",
      "And to let seectide a bount\n",
      "And lave-diwn iny the uppraint Mard omed:\n",
      "And the with of the creave to mantion,\n",
      "And the of the with theres theeen to she cittile such: hap a sone\n",
      "That it our the not of chis tire as to the his that hat's the king,\n",
      "And in the fre\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "start_ids = encode(start)\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "# run generation\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        for k in range(num_samples):\n",
    "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "            print(decode(y[0].tolist()))\n",
    "            print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf9e3b7-c272-4899-9d76-06fe457d86da",
   "metadata": {},
   "source": [
    "- **参考资料**\n",
    "  - [Improving Language Understandingby Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)\n",
    "  - [OpenAI ChatGPT（二）：十分钟读懂 GPT-1](https://zhuanlan.zhihu.com/p/604625917)\n",
    "  - [ChatGPT/InstructGPT详解](https://zhuanlan.zhihu.com/p/590311003)\n",
    "  - [GPT-4核心技术探秘](https://zhuanlan.zhihu.com/p/626463196)\n",
    "  - [图解GPT-2 | The Illustrated GPT-2](https://lolitasian.blog.csdn.net/article/details/125529598)(强烈推荐)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
