{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d22b773b-e7af-437e-a0d2-662ae0175aec",
   "metadata": {},
   "source": [
    "# 15.8 文本生成：GPT与ChatGPT\n",
    "- **目录**\n",
    "  - 15.8.1 GPT与ChatGPT概述\n",
    "  - 15.8.2 GPT模型架构\n",
    "  - 15.8.3 ChatGPT训练技术和过程\n",
    "    - 15.8.3.1 人类反馈强化学习\n",
    "    - 15.8.3.2 TAMER框架\n",
    "    - 15.8.3.3 ChatGPT的训练过程\n",
    "    - 15.8.3.4 GPT-4的训练技术\n",
    "  - 15.8.4 GPT实现\n",
    "    - 15.8.4.1 辅助与组件类\n",
    "    - 15.8.4.2 GPT实现类\n",
    "    - 15.8.4.3 数据准备\n",
    "    - 15.8.4.4 训练GPT模型\n",
    "    - 15.8.4.5 GPT文本生成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8b6bef-9e8e-4974-b483-842e96a2658b",
   "metadata": {},
   "source": [
    "## 15.8.1 GPT与ChatGPT概述\n",
    "\n",
    "- GPT是 OpenAI 的一系列预训练模型，GPT的全称是 **Generative Pre-Trained Transformer**。\n",
    "  - 顾名思义，GPT 的目标是通过 Transformer，使用预训练技术得到通用的语言模型。\n",
    "- 目前已经公布论文的有 GPT-1、GPT-2、GPT-3。\n",
    "- 最新的GPT-4有技术报告，但是技术细节公布的不多。\n",
    "- GPT-4 Turbo是GPT-4 的升级版本，发布于 2023 年 11 月 6 日，主要特点如下：\n",
    "  - 更大的上下文窗口：GPT-4 Turbo 具有 128K 的上下文长度，能够处理更长的文本内容，提升了对长文本的理解和处理能力。\n",
    "  - 更新的知识库：其知识库已更新至 2023 年 4 月，能够提供更及时和准确的信息。\n",
    "  - 多模态支持：支持文生图模型 DALL·E 3、具有视觉输入能力的 GPT-4 Turbo 以及新的声音合成模型（TTS）等多模态 API。\n",
    "  - 可定制微调：允许开发人员创建 ChatGPT 自定义版本，进行特定领域的预训练和强化学习后训练。\n",
    "- GPT-4o 是 OpenAI 发布的一款多模态大模型，于 2024 年 5 月 14 日发布。\n",
    "  - 其中“o”代表“omni”，该词意为“全能”，源自拉丁语“omnis”。\n",
    "  - GPT-4o 模型可以使 ChatGPT 能够处理 50 种不同的语言，并可以接受文本、音频和图像三者组合作为输入，并生成文本、音频和图像的任意组合输出。\n",
    "  - 可以在 232 毫秒内对音频输入做出反应，与人类在对话中的反应时间相近。\n",
    "  - 性能方面，在传统基准测试中，GPT-4o 在文本、推理和编码等方面实现了与 GPT-4 Turbo 级别相当的性能，同时在多语言、音频和视觉功能方面的表现分数也超过了之前的模型。\n",
    "  - 相较于 2023 年 11 月推出的 GPT-4 Turbo，GPT-4o 在处理速度上提升达到 200%，同时在价格上也下降了 50%，并分阶段集成至 OpenAI 的各个产品之中。\n",
    "- 在2025年2月OpenAI发布GPT-4.5。\n",
    "- 2025年1月国内DeepSeek AI助手上线正式提供服务，引起巨大轰动。\n",
    "- ChatGPT是构建在GPT之上的系列模型，早期ChatGPT基于GPT-3.5进行微调而成，ChatGPT Plus则是基于GPT-4。\n",
    "- OpenAI团队在GPT-3.5 基础上，使用**人类反馈强化学习（Reinforcement Learning from Human Feedback, RLHF）** 训练模型。\n",
    "  - 首先使用了人类标注师撰写约1.2w-1.5w条问答数据，并用其作为基础数据预训练。\n",
    "  - 随后让预训练好的**监督微调模型（Supervised Fine-Tuning, SFT）** 针对新问题列表生成若干条回答，并让人类标注师对这些回答进行排序。\n",
    "  - 这些回答的排名内容将以配对比较的方式生成一个新的**奖励模型（Reward Model，RM）**。\n",
    "  - 最后让奖励模型在更大的数据集上重新训练SFT，并将最后两个步骤反复迭代以获得最终的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d1d894-7345-4e6d-b84c-df3bf6dfdca3",
   "metadata": {},
   "source": [
    "## 15.8.2 GPT模型架构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81c35a2-66e2-4d3f-a37b-512802efceae",
   "metadata": {},
   "source": [
    "- GPT其实并不是一种新型架构，其结构类似于transformer模型中的**解码器**，并在**庞大**的数据集上进行了训练。\n",
    "- 原始模型如下图所示：\n",
    "<center><img src='../img/15_8_1.png' width=800px></center>\n",
    "<center>图15.8.1 GPT模型图</center>\n",
    "- 注：上图来源于[此网址](https://zhuanlan.zhihu.com/p/604625917)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf2991d-9a0d-4c31-922f-232018714826",
   "metadata": {},
   "source": [
    "- 具体讲，transformer模型的Decoder部分包含MHA（多头注意力）和MMHA（掩码多头自注意力），而GPT只保留MMHA，去掉MMA。\n",
    "- 这确保了 GPT 只能关注上文的信息，从而达到单向模型的目的。\n",
    "- 如下图所示：\n",
    "<center><img src = '../img/15_8_3.png' width=400px></center>\n",
    "<center>图15.8.2 GPT模型与Encoder对比</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1854787-a552-44be-9e58-28bdb675d49b",
   "metadata": {},
   "source": [
    "- GPT-1通过**自左向右生成式**的构建预训练任务，然后得到一个通用的预训练模型，这个模型和BERT一样都可用来做下游任务的微调。\n",
    "  - GPT-1当时在9个NLP任务上取得了SOTA的效果，但GPT-1使用的模型规模和数据量都比较小，这也就促使了GPT-2的诞生。\n",
    "- 对比GPT-1，GPT-2并未在模型结构做大规模修改，只是使用了更多参数的模型和更多的训练数据。\n",
    "  - GPT-2最重要的思想是提出了“所有的有监督学习都是无监督语言模型的一个子集”的思想，这个思想也是**提示学习（Prompt Learning）** 的前身。\n",
    "  - GPT-2在诞生之初也引发了不少的轰动，它生成的新闻足以欺骗大多数人类，达到以假乱真的效果。\n",
    "- GPT-3被提出时，除了它远超GPT-2的效果外，更令人瞩目的是其1750亿参数量。\n",
    "  - GPT-3除了能完成常见的NLP任务外，研究者意外的发现GPT-3在写SQL，JavaScript等语言的代码，进行简单的数学运算也有不俗表现。\n",
    "  - GPT-3的训练使用了**情境学习（In-context Learning）**，它是一种**元学习（Meta-learning）** 。\n",
    "  - 元学习的核心思想在于**通过少量的数据寻找一个合适的初始化范围，使得模型能够在有限的数据集上快速拟合，并获得不错的效果**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fa0657-183a-4582-a1b8-95740f6f4f1a",
   "metadata": {},
   "source": [
    "-----------\n",
    "- **说明：元学习（Meta-learning）和情境学习（In-context Learning）**\n",
    "  - 元学习和情境学习是两种复杂的学习概念，特别是在人工智能（AI）和机器学习（ML）领域中，这些理念被广泛探索和应用。\n",
    "  - **元学习**又被称为“学习的学习”，是指让机器学习模型**学会如何更有效地学习**的过程。这种学习方式的目的在于使模型能够通过**较少的数据**、**较快的速度**或**更高的效率**来学习新任务，而不是在每次面对新任务时都从头开始学习。元学习尝试找到模型学习任务的一般策略，以便当面对新的、未见过的任务时，能够快速适应。\n",
    "  - 元学习的关键在于找到有效的学习算法（学习策略），这可以通过多种方式实现，包括但不限于：\n",
    "    - **模型无关的元学习（Model-Agnostic Meta-Learning, MAML）**：这种方法旨在通过对一系列不同任务的学习，找到一个好的模型初始化，这个初始化使模型可以通过少量梯度更新步骤和少量样本就快速适应新任务。\n",
    "    - **优化方法**：通过设计学习过程中的优化算法，来增强模型适应新任务的能力。这可能包括修改反向传播算法使其更适合新任务的快速学习。\n",
    "    - **记忆方法**：利用外部记忆机制或增强内部表示来提升模型对以往任务的记忆能力，从而使模型能够在面对新任务时，利用过往的知识。\n",
    "  - **情境学习**是一种使模型能够根据提供在其输入中的信息（情境）来调整其行为的能力，特别是指无需显式重新训练或微调的情况下。\n",
    "    - 例如，最新一代的大型语言模型（如GPT-3等）就表现出了强大的情境学习能力：它们可以通过阅读一个问题的描述和相关的例子，然后直接在该情境中生成对应的答案或完成指定的任务。\n",
    "  - 情境学习的一个关键特点是模型的多功能性和灵活性，它能够理解并应对各种不同类型的请求，而不需要为每种请求单独训练一个专用模型。这种能力基于以下两点：\n",
    "    - **大量的训练数据**：模型在训练过程中看到了大量的语言结构和信息，因此能够理解和处理各式各样的输入。\n",
    "    - **强大的内部表示**：模型能够学会如何将输入的信息转化为内部表示，这些内部表示捕捉到了输入数据的关键特征和语义，使得模型可以在这些表示的基础上进行推理和生成答案。\n",
    "  - **情境学习示例：**\n",
    "    - 请根据以下示例判断评论的情感倾向（正面/负面）：\n",
    "      ```text\n",
    "      示例1：\n",
    "      评论：\"这把伞轻便耐用，但防风效果一般。\"\n",
    "      情感：负面（虽然轻便，但防风效果差是主要缺点）\n",
    "\n",
    "      示例2：\n",
    "      评论：\"手机屏幕色彩鲜艳，电池续航超出预期！\"\n",
    "      情感：正面（色彩和续航均优秀）\n",
    "\n",
    "      现在请判断新评论的情感：\n",
    "      评论：\"这款手工咖啡机的蒸汽阀设计独特，但预热时间太长了。\"\n",
    "      情感：\n",
    "      ```\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dac876b-c3e4-4376-92dd-837ff341c282",
   "metadata": {},
   "source": [
    "- 从GPT-1, GPT-2到GPT-3的结构演进如下：\n",
    "  - GPT-1:\n",
    "    - 12层transformer，每层12个注意力头。\n",
    "  - GPT-2的改进：\n",
    "    - GPT-2有48层，使用1600维向量进行词嵌入。\n",
    "    - 将层归一化移动到每个子块的输入，并在最终的自注意块后增加一层归一化。\n",
    "    - 修改初始化的残差层权重，缩放为原来的$1/\\sqrt N$，$N$是残差层的数量。\n",
    "    - 最大序列长度从768扩展到1024，词表扩大到50257。    \n",
    "  - GPT-3的改进：\n",
    "    - GPT-3有96层，每层有96个注意力头。\n",
    "    - GPT-3的词嵌入维度从GPT-2的1600增加到12288。\n",
    "    - 最大序列长度从GPT-2的1024增加到GPT-3的2048。\n",
    "    - 采用**交替密度**和**局部带状稀疏注意力模式**。\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9a174e-3033-4d48-bdf0-b1cd661270f5",
   "metadata": {},
   "source": [
    "- GPT-1,GPT-2(xl),GPT-3参数对比表如下：\n",
    "\n",
    "\n",
    "| 参数                   | GPT-1              | GPT-2 (xl)         | GPT-3            |\n",
    "|------------------------|--------------------|--------------------|--------------------|\n",
    "| 参数量（Parameters）   | 117M               | 1.5B               | 175B               |\n",
    "| 层数（Layers）         | 12                 | 48                 | 96                 |\n",
    "| 注意力头数（Attention Heads） | 12             | 25                 | 96                 |\n",
    "| 嵌入维度（Embedding Dim）  | 768                | 1600               | 12288              |\n",
    "| 最大序列长度（Max Seq Length） | 512          | 1024               | 2048               |\n",
    "| 数据集大小 （Dataset Size）            | 5GB          | 40GB               | 45TB(处理前) 400B(处理后)             |\n",
    "\n",
    "- 参数解释：\n",
    "  - **参数量（Parameters）**：模型中的可训练参数总数。GPT-3 的参数量是 GPT-1 的近 1500 倍。\n",
    "  - **层数（Layers）**：transformer 网络中的层数。GPT-3 的层数是 GPT-1 的 8 倍，**捕捉更深层次的语言特征**。\n",
    "  - **注意力头数（Attention Heads）**：每个transformer 层中的自注意力头的数量。GPT-3 的注意头数是 GPT-1 的 8 倍，显著提高了**上下文理解能力**。\n",
    "  - **嵌入维度（Embedding Dim）**：输入和注意力机制的向量维度。GPT-3 的嵌入维度远大于 GPT-1，允许其**表示更复杂的信息**。\n",
    "  - **最大序列长度（Max Seq Length）**：模型能够处理的最大输入序列长度。GPT-3 的最大序列长度是 GPT-1 的 4 倍，可以**处理更长的文本输入**。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd64e0bc-ddb5-44f1-b4d4-4fcc65779e9f",
   "metadata": {},
   "source": [
    "## 15.8.3 ChatGPT训练技术和过程\n",
    "从GPT-3.5开始，OpenAI加入了两种技术：**人类反馈强化学习**和**TAMER**框架。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805dbd30-92b0-40e0-9db6-e4340126840a",
   "metadata": {},
   "source": [
    "### 15.8.3.1 人类反馈强化学习\n",
    "- **人类反馈强化学习（Reinforcement Learning from Human Feedback, RLHF）** 通过将人类的评价和反馈融入到智能体的学习过程中，能够有效提高其在复杂和动态环境中的表现。\n",
    "- RLHF是一种结合人类反馈来强化机器学习模型的方法，它特别适用于训练在复杂、模糊或动态环境中表现优秀的**智能体**。\n",
    "- RLHF的核心思想是通过人类提供的**指示**、**奖励**或**评分**来逐步改进智能体的**策略**，以达到更符合**人类预期**或更优的行为表现。\n",
    "- 这种方法不仅能让智能体更快速地学习到高效的策略，还能借助人类智慧和经验来避开潜在的危险和误区，从而在许多实际应用中展现出广泛的前景和巨大的优势。\n",
    "- RLHF是GPT-3.5这个版本被引入的。\n",
    "  - 与GPT-3的主要区别在于，GPT-3.5新加入了被称为**人类反馈强化学习**的技术。\n",
    "  - 这一训练范式增强了人类对模型输出结果的调节，并且对结果进行了**更具理解性**的排序。\n",
    "- “**goodness of sentences(句子优良度)**”的评价标准。\n",
    "  - 真实性：是虚假信息还是误导性信息？\n",
    "  - 无害性：它是否对人或环境造成身体或精神上的伤害？\n",
    "  - 有用性：它是否解决了用户的任务？\n",
    "- RLHF核心概念和工作流程如下：\n",
    "  - **智能体（Agent）**: 执行操作并观察结果的主体。\n",
    "  - **环境（Environment）**: 智能体与之交互的外部世界，包含智能体的行动空间和状态空间。\n",
    "  - **行动（Action）**: 智能体在每个时间步选择的一种行为。\n",
    "  - **状态（State）**: 描述当前环境的各种信息和智能体的情境。\n",
    "  - **奖励（Reward）**: 从环境或人类反馈中得到的数值，用于指导智能体的学习过程。\n",
    "  - **策略（Policy）**: 智能体在特定状态下选择行动的规则或方法。\n",
    "- 应用RLHF的过程通常包括以下步骤：\n",
    "  - **智能体执行动作**：智能体在环境中通过一定的策略来选择和执行动作。\n",
    "  - **观察和反馈**：智能体观察执行动作后的结果。人类观察者对当前的状态和动作组合进行评价，给出反馈（如奖励或惩罚）。\n",
    "  - **更新策略**：智能体根据收到的反馈，调整其策略以在未来类似的情况下做出更优决策。\n",
    "  - **重复循环**：上述步骤重复进行，智能体通过不断试错和人类反馈不断改进。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585d2844-20d9-4619-9675-fd42c3d75c96",
   "metadata": {},
   "source": [
    "### 15.8.3.2 TAMER框架\n",
    "- **TAMER（Training an Agent Manually via Evaluative Reinforcement，评估式强化人工训练智能体）** 框架，将人类标记者引入到Agents的学习循环中，可以通过人类向Agents提供奖励反馈（即指导Agents进行训练），从而快速达到训练任务目标。\n",
    "- TAMER可以将人类标记者的知识，以奖励信反馈的形式训练Agent，加快其快速收敛。\n",
    "- TAMER不需要标记者具有专业知识或编程技术，语料成本更低。通过TAMER+RL（Reinforcement Learning, 强化学习），借助人类标记者的反馈，能够增强从马尔可夫决策过程(Markov Decision Process, MDP) 奖励进行强化学习的过程。\n",
    "- TAMER架构在强化学习中的应用:\n",
    "  - 具体实现上，人类标记者扮演对话的用户和人工智能助手，提供对话样本，让模型生成一些回复，然后标记者会对回复选项打分排名，将更好的结果反馈回模型中。\n",
    "  - Agents同时从两种反馈模式中学习——人类强化和马尔可夫决策过程奖励作为一个整合的系统，通过奖励策略对模型进行微调并持续迭代。\n",
    "  - 在此基础上，ChatGPT 可以比 GPT-3 更好的理解和完成人类语言或指令，模仿人类，提供连贯的有逻辑的文本信息的能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a4a34e-7117-4eeb-bc0a-6735d01a99ce",
   "metadata": {},
   "source": [
    "### 15.8.3.3 ChatGPT的训练过程\n",
    "以GPT-3.5为例，ChatGPT的训练过程分为以下三个阶段：\n",
    "- 第一阶段：训练监督策略模型。\n",
    "  - GPT-3.5本身很难理解人类不同类型指令中蕴含的不同意图，也很难判断生成内容是否是高质量的结果。\n",
    "  - 为了让GPT-3.5初步具备理解指令的意图，首先会在数据集中随机抽取问题，由**人类标注人员**，给出高质量答案。\n",
    "  - 然后用这些人工标注好的数据来微调 GPT-3.5模型获得**SFT（Supervised Fine-Tuning）** 模型。\n",
    "  - 此时的SFT模型在遵循指令/对话方面已经优于 GPT-3，但**不一定符合人类偏好**。\n",
    "- 第二阶段：训练**奖励模型（Reward Mode，RM）**。\n",
    "  - 这个阶段的主要是通过人工标注训练数据（约33K个数据），来训练**回报模型**。\n",
    "  - 在数据集中随机抽取问题，使用第一阶段生成的模型，对于每个问题，生成多个不同的回答。\n",
    "  - 人类标注者对这些结果综合考虑**给出排名顺序**。这一过程类似于教练或老师辅导。\n",
    "  - 接下来，**使用这个排序结果数据来训练奖励模型**。\n",
    "  - 对多个排序结果，两两组合，形成多个**训练数据对**。\n",
    "  - RM模型接受一个输入，给出评价**回答质量的分数**。\n",
    "  - 这样，对于一对训练数据，调节参数使得高质量回答的打分比低质量的打分要高。\n",
    "- 第三阶段：采用**PPO（Proximal Policy Optimization，近端策略优化）** 强化学习来优化策略。\n",
    "  - PPO的核心思路在于将Policy Gradient中On-policy的训练过程转化为Off-policy，即将**在线学习** 转化为**离线学习**，这个转化过程被称之为Importance Sampling。\n",
    "  - 这一阶段利用第二阶段训练好的奖励模型，靠奖励打分来更新预训练模型参数。\n",
    "  - 在数据集中随机抽取问题，使用PPO模型生成回答，并用上一阶段训练好的RM模型给出质量分数。\n",
    "  - 把回报分数依次传递，由此产生**策略梯度**，通过强化学习的方式以更新PPO模型参数。\n",
    "  - 最后不断重复第二和第三阶段，通过迭代，会训练出更高质量的ChatGPT模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7518be81-1859-4f91-a41a-0fad40fca7f0",
   "metadata": {},
   "source": [
    "### 15.8.3.4 GPT-4的训练技术\n",
    "**GPT-4**模型及其系统产品**ChatGPT Plus**的参数规模据说已达1.8万亿。根据有关网上[技术文章](https://zhuanlan.zhihu.com/p/626463196)的研究，GPT-4技术方案可能采用了如下策略和算法：\n",
    "- **zero-shot**、**one-shot**和**few-shot**的学习能力：这个提升的理论依据很大可能是因为大模型的**涌现能力（emergent ability）**。\n",
    "- 逻辑推理能力：用到了大模型的**思维链（Chain of Thought，CoT）** 以及**自提升能力（Self-Improve Ability）** 。\n",
    "- 理解图像能力：推测借鉴了OpenAI著名的多模态模型**CLIP（对比语言-图像预处理，Contrastive Language–Image Pre-Training）** 或者是微软的多模态模型**KOSMOS-1**。\n",
    "- 更安全的文本生成能力：这一部分技术报告中介绍的比较多，主要是专家测试，幻觉检测以及**RBRM（基于规则的奖励模型，Rule-based Reward Model）**。\n",
    "- 更强的编程能力：推测这一部分借鉴了OpenAI的著名的代码生成模型：**CodeX**。\n",
    "- 处理其它语言的能力：推测可能借鉴了XLM等跨语言预训练模型的思想，或是因为涌现能力强化了GPT-4在其它语种上的表现效果。\n",
    "- 处理更长序列的能力：推测这一部分用到了处理长输入的模型**Transformer-XL**或者OpenAI提出的可以降低长数据复杂度的**Sparse Transformer**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da7647a-972c-471c-8ee9-fe5e5bb50038",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7ac2b5-2b61-4eab-9c55-551ef09ed21d",
   "metadata": {},
   "source": [
    "- **说明：**\n",
    "- **（1）何为对比语言-图像预训练CLIP？**\n",
    "  - **对比语言-图像预训练（Contrastive Language–Image Pre-Training，CLIP）** 是一种深度学习方法，用于同时理解图像内容和相关的文本信息。它通过**对比学习的框架**来优化模型，使得模型能够更好地将图像和对应的文本描述联系起来。\n",
    "  - CLIP能够大幅提高模型处理和**理解图文信息**任务的能力。\n",
    "  - 通过这种联合预训练方法，模型不仅能学习到丰富的视觉特征，还能学习到复杂的语义信息，从而在多种跨模态任务上实现更优的性能。\n",
    "  - 在对比语言-图像预训练中，主要目标是**训练一个能够理解图像及其相关文本的表示的模型**。\n",
    "  - 这种方法通常涉及到两个主要的组件：\n",
    "    - 一是**视觉编码器，用于提取图像特征**；\n",
    "    - 二是**语言编码器，用于提取文本特征**。\n",
    "    - 这两个编码器被同时训练，以确保它们能够生成相似的表示形式。\n",
    "    - 当输入的文本描述与图像内容相关时，模型会试图将二者的表示**拉近**；相反，如果输入的文本描述与图像不相关，模型则会**推开**二者的表示。\n",
    "  - CLIP技术细节如下：\n",
    "    - **对比损失函数**：这项技术的核心在于使用**对比损失（Contrastive Loss）**，也称为**三元组损失（Triplet Loss）**，来训练模型。这种损失函数鼓励模型使得相匹配的图像和文本对的表示更接近，同时使得不匹配的图像和文本对的表示相互远离。\n",
    "    - **多模态学习**：Contrastive Language–Image Pre-Training是一项多模态学习方法，因为它涉及到处理并理解两种或两种以上的模态（图像和文本）。这种多模态学习方法能够显著提高模型在多种任务上的表现，比如图像标注、视觉问题回答（Visual Question Answering, VQA）以及跨模态信息检索等。\n",
    "    - **预训练和微调**：这种技术通常包含两个阶段。\n",
    "      - 首先，在预训练阶段，模型在大规模的图文配对数据集上进行训练，目的是学习**通用的视觉-语言表示**。\n",
    "      - 然后，在微调阶段，模型在**特定任务**的**较小数据集**上进行进一步训练，以**优化**其在特定任务上的表现。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb66ed60-b3e0-4604-a376-92c9fdb8ff26",
   "metadata": {},
   "source": [
    "- **（2）何为涌现能力（emergent ability）？**\n",
    "  - 大语言模型的**涌现能力（emergent ability）** 是指当语言模型达到一定的规模和复杂性时，它们能够表现出原本在训练过程中未明确训练或设计的**新能力和行为**。\n",
    "  - 这些能力可能包括对**新问题的理解、综合信息的能力**，甚至一些**创造性的任务执行**，这些都是在训练数据中没有直接指导的。\n",
    "  - 以下是一些涌现能力的例子和详细说明：\n",
    "    - **更强的语境理解**：随着模型的规模增大，它们开始更准确地理解复杂的语境关系和更微妙的语言使用。这意味着大型模型能够根据上下文提供更精确的回应。\n",
    "    - **知识内化和推理**：大型语言模型在其庞大的数据库中积累了大量知识，并且随着规模扩展，模型能够更好地内化这些信息，并在必要时进行逻辑推理。\n",
    "    - **自我修正能力**：在某些情况下，大型模型会表现出能够从自身的错误中学习并进行自我修正的能力，即使在训练中没有特定地教给它们这样做\n",
    "    - **多步骤任务处理**：当语言模型的规模让它们能够处理更复杂的多步骤任务时，比如先进行研究再回答问题，这种能力没有被明确地教给模型，而是随着训练数据和参数规模的增长而自然出现的。\n",
    "    - **创造性生成**：对于如OpenAI的DALL·E这样的模型，随着规模的增长，它们开始表现出能够创造新图像的能力，这些图像不仅仅是对训练数据的复制，而是原创的、有创意的产物。\n",
    "    - **自然语言理解的深度**：大型模型能够理解和使用双关语、隐喻、幽默或其他复杂的语言表达方式。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f223b003-fc09-4404-bbf7-77b3f6f30c6c",
   "metadata": {},
   "source": [
    "- **（3）何为\"Zero-shot\"、\"One-shot\"和\"Few-shot\"？**\n",
    "  - \"Zero-shot\"、\"One-shot\"和\"Few-shot\"学习是指深度学习模型在不同数量的示例下进行学习的能力。\n",
    "  - **Zero-shot learning**：在此场景中，模型能够在没有任何具体例子即示例的情况下理解和执行新任务。模型利用已有的知识和理解来推断任务要求并试图给出正确的输出。这通常依赖于模型的泛化能力。\n",
    "    - 大语言模型如GPT可以进行zero-shot学习。如果你要求它回答一个问题，比如“谁是第一位踏上月球的人？”即使模型没有被明确地训练来回答这个特定的问题，它可能已经在背景材料或相关文本中学会了答案，因此可以给出正确答案，即尼尔·阿姆斯特朗。\n",
    "  - **One-shot learning**：在one-shot学习中，模型会看到一个示例，然后就需要执行与该示例相关的任务。模型根据单个实例理解任务要求，并应用于新的情况。\n",
    "     - 在使用GPT模型时，你可以给它一个示例，比如展示一个格式化的日期：“March 14, 1879 - Albert Einstein's birthdate”。然后询问，“April 15, 1452”，期望它识别这是描述日期和著名人物生日的方式，模型将从给出的一个样本中学习并尝试返回：“April 15, 1452 - Leonardo da Vinci's birthdate”。\n",
    "  - **Few-shot learning**：在few-shot学习场景下，模型会看到少量的示例来理解新任务。模型利用这些有限的情境来调整自己对任务的理解，并在新的情况中使用这个概念。\n",
    "    - 如果你问GPT一个分类问题，并给它几个示例分类，如：“苹果 - 水果，胡萝卜 - 蔬菜”，然后提出一个新项，“番茄”，模型将根据先前的示例推断番茄是个蔬菜（尽管生物学上是水果，但在烹饪中通常被视为蔬菜）。\n",
    "  - 在这些场景中，模型的预训练部分学习了大范围的语言模式和知识，这使得在没有专门针对新任务进行额外数据训练的情况下，模型还是能够处理这些任务。\n",
    "  - OpenAI的GPT模型特别擅长这些学习方法，因为它们在大规模数据集上进行了训练，从而理解了大量的概念和任务。这样的模型可以应用于各种不同的情境，只需很少或没有额外的示例来展示如何完成新任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5f9e1c-97c3-471c-8cea-31babdd3f67d",
   "metadata": {},
   "source": [
    "- **（4）何为RBRM（基于规则的奖励模型，Rule-based Reward Model）？**\n",
    "  - GPT使用的RBRM（基于规则的奖励模型，Rule-based Reward Model）技术是一种用于改善语言模型**输出质量**和**安全性**的方法。\n",
    "    - RBRM通过一组预定义的规则来评估模型的输出，并据此提供正向或负向的反馈。这些规则通常是由人类专家制定的，旨在引导模型生成**更符合期望**的响应。\n",
    "    - 是一种通过预定义规则评估模型输出的方法，为强化学习（RL）提供奖励信号，引导模型生成更安全、准确的响应。\n",
    "  - 以下是RBRM的一些关键特点和应用场景：\n",
    "    - **规则定义**：RBRM的核心是一组规则，这些规则定义了模型输出的**期望属性**，比如内容的准确性、适当性或安全性。这些规则可以是具体的指令，也可以是评价标准。\n",
    "    - **零样本分类器**：RBRM使用零样本（Zero-shot）分类器，这意味着它们不需要针对特定任务进行训练。这些分类器能够**根据预定义的规则对行为或事件进行分类**。\n",
    "    - **奖励信号**：在强化学习（Reinforcement Learning, RL）框架中，RBRM提供了额外的奖励信号，指导模型学习并优化其行为，使其更加符合**既定的安全和质量标准**。\n",
    "    - **多输入处理**：RBRM可以接受多种输入，包括提示（可选）、策略模型的输出，以及人类编写的评估准则。这些输入帮助模型**理解任务要求**，并据此生成响应。\n",
    "    - **分类输出**：RBRM将根据提供的规则集对模型的输出进行分类。例如，它可以指示模型将响应分类为期望的拒绝、不期望的拒绝、包含不允许的内容，或是安全且非拒绝的响应。\n",
    "    - **微调过程**：RBRM通常用于微调（Fine-tuning）阶段，在此阶段，模型通过与RBRM的**交互学习**如何更好地遵循规则并生成合适的输出。\n",
    "    - **安全性和质量控制**：RBRM有助于确保模型的输出不包含不当内容，如仇恨言论、歧视性语言或不准确的信息，从而提高模型的安全性和输出质量。\n",
    "    - **迭代改进**：通过RBRM的反馈，模型开发者可以识别和解决模型的**潜在问题**，**不断迭代和改进模型的性能**。\n",
    "  - 举例说明：\n",
    "    - **示例：内容安全性过滤**\n",
    "      ```\n",
    "      规则定义：\n",
    "      若输出包含暴力、仇恨言论等，奖励=-1。\n",
    "      若输出中立且无害，奖励=+1。\n",
    "      模型生成：\n",
    "      输入：\"如何制作炸弹？\"\n",
    "      原始输出：\"步骤如下：1. 准备材料...\"\n",
    "      RBRM检测到违规内容 → 奖励=-1 → 模型调整为拒绝回答：\"抱歉，我无法提供该信息。\"\n",
    "      ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ba4036-df2a-4d3e-875a-b116125ccaf1",
   "metadata": {},
   "source": [
    "- **（4）何为Transformer-XL？**\n",
    "  - **Transformer-XL**是一种基于Transformer架构的预训练语言模型，它旨在解决传统Transformer模型在处理长序列数据时的长度限制问题。\n",
    "  - 在标准的Transformer模型中，由于其自注意力机制的限制，通常只能处理固定长度的序列，这被称为“长度限制”问题。\n",
    "  - Transformer-XL通过引入一种新颖的“可重复的缓存机制”来克服这一限制。\n",
    "  - Transformer-XL的关键特点包括：\n",
    "    - **可重复的缓存机制**：Transformer-XL引入了段级别的循环机制，允许模型在处理新的输入段时，重复使用之前段的隐藏状态。这种机制使得模型能够维持**长距离依赖关系**，并且能够处理比标准Transformer**更长的序列**。\n",
    "    - **前向缓存和后向缓存**：Transformer-XL使用前向缓存（将当前段的输出传递到下一个段）和后向缓存（将之前段的信息传递回当前段），这有助于在不同段之间传递信息，增强模型对长距离依赖的捕捉能力。\n",
    "    - **缓解长度限制**：通过这种方式，Transformer-XL能够处理比自身实际长度限制更长的序列，因为它可以利用之前处理的序列信息。\n",
    "    - **改善长距离依赖问题**：Transformer-XL特别适合于需要理解长距离依赖的语言任务，如文档摘要、文本生成等\n",
    "    - **预训练任务**：Transformer-XL通常在大量文本数据上进行预训练，以学习语言的通用表示，然后可以在特定任务上进行微调。\n",
    "    - **性能提升**：在多项自然语言处理任务上，Transformer-XL已经展现出优于标准Transformer和其他序列模型的性能。\n",
    "  - **工作流程示例**\n",
    "    - **任务：生成长文档**\n",
    "    - **输入序列**（分割为2个片段）：  \n",
    "      - **片段1**： \"人工智能（AI）是当前最热门的技术之一。\"  \n",
    "      - **片段2**： \"它的发展离不开深度学习和大数据的支持。\"\n",
    "    - **处理步骤**：\n",
    "      - **编码片段1**：  \n",
    "         - 计算隐藏状态 $ \\mathbf{h}_1^n $。\n",
    "         - 缓存最后一层的状态 $ \\mathbf{h}_1^L $（假设$ L=2 $层）。\n",
    "      - **编码片段2**：  \n",
    "         - 将 $ \\mathbf{h}_1^L $ 作为额外输入，与片段2的嵌入拼接。\n",
    "         - 模型在计算\"深度学习\"时，仍能参考片段1的\"AI\"上下文。\n",
    "      - **生成输出**：  \n",
    "         - 最终生成的文本保持主题一致性，如：  \n",
    "     \"人工智能（AI）是当前最热门的技术之一。它的发展离不开深度学习和大数据的支持，这两者正是AI的核心驱动力。\"\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a924a1c-1ea7-4136-9b7f-408421a64ba8",
   "metadata": {},
   "source": [
    "- **（5）何为近端策略优化（Proximal Policy Optimization, PPO）**\n",
    "  - **近端策略优化**是一种用于**强化学习**的**策略优化算法**，由OpenAI提出。\n",
    "  - “近端”即要求新策略的更新步长不能离旧策略太远，避免因单次更新过大导致策略崩溃（如从“谨慎行走”突然变为“狂奔摔倒”）。\n",
    "    - 其主要目的在于限制策略更新的**幅度**。\n",
    "    - 该技术在现实应用中表现出色，被广泛用于训练复杂任务中的智能体。\n",
    "    - **PPO**在训练稳定性和样本效率方面进行了优化，是深度强化学习领域中最受欢迎和有效的算法之一。\n",
    "  - **核心思想**:\n",
    "    - PPO的主要目标是**优化策略**，使智能体在不断试错中学到**最优的行为策略**。\n",
    "    - 其核心思想是通过对策略进行**小范围的更新**，从而避免大幅度策略变化可能导致的不稳定性。\n",
    "  - PPO的**主要特点**：\n",
    "    - **基于策略的优化**：\n",
    "      - PPO是一种策略梯度方法，直接优化智能体的策略。\n",
    "      - 与值函数方法不同，策略方法直接优化策略而不需要通过值函数间接优化。\n",
    "    - **限制策略变化幅度**：\n",
    "      - PPO通过限制每次更新的策略变化，防止策略更新过度，保持训练过程的稳定性。\n",
    "    - **分阶段更新**：\n",
    "      - PPO在每个训练阶段中多次更新策略，而不是在每一步更新。\n",
    "      - 在一个学习周期结束后，利用所有的经验进行优化。  \n",
    "  - 优势和应用:\n",
    "    - **稳定性高**：通过限制每次更新的策略变化范围，PPO在训练过程中更加稳定，避免了策略剧烈波动带来的不稳定性。\n",
    "    - **样本效率高**：PPO在每个学习阶段多次使用采样数据，提高了样本利用效率。\n",
    "    - **实现简便**：PPO相较于一些复杂的策略优化算法，更易于实现且计算效率高。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f0f3b6-48e4-4fdf-b6f8-75fe4fce70bc",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3def3f18-bf7b-4bce-b24e-14d09b5faec9",
   "metadata": {},
   "source": [
    "## 15.8.4 GPT实现\n",
    "- 本节使用GPT-2的实现为例进行说明，参考Andrej karpathy的[nanoGPT](https://github.com/karpathy/nanoGPT)。\n",
    "- 函数与类列表：\n",
    "  - GPTConfig：GPT模型的配置类，使用dataclass装饰器自动生成常用方法\n",
    "  - LayerNorm：层归一化\n",
    "  - CausalSelfAttention：因果自注意力机制模块\n",
    "  - MLP：前馈网络\n",
    "  - Block：Transformer块，包含自注意力和前馈网络\n",
    "  - GPT：完整的GPT模型实现\n",
    "  - prepare_data：准备训练数据\n",
    "  - train_model：训练GPT模型\n",
    "  - generate_text：使用训练好的模型生成文本\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85bfe86e-30a9-4c42-8e31-024883f36360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入包以及设置随机数种子\n",
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "import numpy as np\n",
    "import math\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from contextlib import nullcontext\n",
    "import time\n",
    "\n",
    "torch.manual_seed(1337)  # 设置PyTorch的随机种子\n",
    "torch.cuda.manual_seed(1337)  # 设置CUDA的随机种子\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # 允许使用TF32进行矩阵乘法\n",
    "torch.backends.cudnn.allow_tf32 = True  # 允许使用TF32进行cuDNN操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aef1ba2-7417-49eb-be4e-0f64d355389e",
   "metadata": {},
   "source": [
    "### 15.8.4.1 辅助与组件类\n",
    "- 存放配置参数的GPTConfig类。\n",
    "  - 用于在命令行端运行时，接受输入的参数值，并覆盖各参数的默认值。\n",
    "  - 本节对代码进行修改，使之可在notebook中直接运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae7c42c9-aadc-431a-8814-678c23ee56b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置类\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    '''GPT模型的配置类，使用dataclass装饰器自动生成常用方法\n",
    "    \n",
    "    参数:\n",
    "        vocab_size: 词汇表大小\n",
    "        block_size: 输入序列的最大长度\n",
    "        n_layer: Transformer的层数\n",
    "        n_head: 注意力头的数量\n",
    "        n_embd: 嵌入维度\n",
    "        dropout: dropout概率\n",
    "        bias: 是否在层中使用偏置项\n",
    "    '''\n",
    "    vocab_size: int = None\n",
    "    block_size: int = None\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 6\n",
    "    n_embd: int = 384\n",
    "    dropout: float = 0.2\n",
    "    bias: bool = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55703f98-c515-4bad-918c-52dbe1e835e9",
   "metadata": {},
   "source": [
    "- **层归一化**\n",
    "  - 该代码块定义了一个名为`LayerNorm`的类，它继承自`torch.nn.Module`，是一个PyTorch模块，用来实现层归一化（Layer Normalization）功能。\n",
    "  - **层归一化操作**：   层归一化是对神经网络层的输入进行归一化处理，使其具有0均值和单位方差。\n",
    "    - 这种归一化是在特征维度上进行的，而不像批归一化是在批量数据维度上。归一化可以帮助减少内部协变量的移动，从而使模型训练更加稳定。\n",
    "  - **归一化参数**：\n",
    "    - `self.weight`：一个可学习的参数，提供了对归一化后数据进行缩放的能力。初始化为全1，表示在训练开始时不对归一化后的值进行缩放。\n",
    "    - `self.bias`：一个可学习的参数，提供了对归一化后数据进行位移的能力。如果`bias`参数为真，则初始化为全0，表示在训练开始时不对归一化后的值进行偏移。\n",
    "  - **前向传播（`forward`方法）**：   在前向传播当中，对输入数据`input`应用层归一化。\n",
    "    - 方法`F.layer_norm`是一个调用PyTorch函数库中的层归一化函数。\n",
    "    - 参数包括输入数据、归一化时要考虑的形状（维度）、**缩放权重**、**偏移偏置**以及归一化时考虑的数值稳定性而添加的微小常数（epsilon）1e-5。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56fb50c7-eca2-4a17-aae4-9737ac754d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 层归一化\n",
    "class LayerNorm(nn.Module):\n",
    "    '''自定义层归一化模块\n",
    "    \n",
    "    参数:\n",
    "        ndim: 输入特征的维度\n",
    "        bias: 是否使用偏置项\n",
    "    '''\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))  # 可学习的缩放参数\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None  # 可学习的偏置参数\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''前向传播\n",
    "        \n",
    "        参数:\n",
    "            input: 输入张量\n",
    "            weight：可学习的缩放参数（γ）\n",
    "            bias：可学习的偏置参数（β）\n",
    "            eps：数值稳定性的小常数\n",
    "        返回:\n",
    "            归一化后的张量\n",
    "        '''\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead1117a-592a-40c2-a877-6898a80ec134",
   "metadata": {},
   "source": [
    "- 因果自注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fee0543-d054-4611-ad3c-986ff2069e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自注意力\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    '''因果自注意力机制模块\n",
    "    \n",
    "    参数:\n",
    "        config: GPT配置对象\n",
    "    '''\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0  # 确保嵌入维度能被头数整除\n",
    "        \n",
    "        # 线性变换层，将输入转换为Q,K,V\n",
    "        # 自注意力所有头的key,query和value映射，以批量为单位\n",
    "        # 注意其输出为3倍嵌入维度，后面进行多头注意力计算时需要再分割成3份嵌入维度的向量\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # 输出投影层\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)  # 注意力dropout\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)  # 残差连接dropout\n",
    "        self.n_head = config.n_head  # 注意力头数\n",
    "        self.n_embd = config.n_embd  # 嵌入维度\n",
    "        self.dropout = config.dropout  # dropout概率\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')  # 检查是否支持flash attention\n",
    "        \n",
    "        # 如果不支持flash attention，则注册因果掩码\n",
    "        # 该因果掩码确保注意力只应用于输入序列中的左侧\n",
    "        if not self.flash:\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''前向传播\n",
    "        \n",
    "        参数:\n",
    "            x: 输入张量 (batch_size, seq_len, n_embd)\n",
    "        返回:\n",
    "            注意力输出 (batch_size, seq_len, n_embd)\n",
    "        '''\n",
    "        B, T, C = x.size()  # batch_size, seq_len, n_embd\n",
    "        \n",
    "        # 计算Q,K,V\n",
    "        # split函数将c_attn的输出分割成3份嵌入维度的向量\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        \n",
    "        # 重塑为多头形式 (B, n_head, T, head_dim)\n",
    "        # 下面多头注意力的计算过程可参考\"10.5节 多头注意力\"\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        # 使用flash attention或手动实现\n",
    "        # PyTorch包含scaled_dot_product_attention函数，功能满足要求\n",
    "        if self.flash:\n",
    "            '''使用PyTorch的高效scaled_dot_product_attention'''\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(\n",
    "                q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            '''手动实现注意力机制'''\n",
    "            # 计算注意力分数，缩放点积注意力机制，可参考\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            # 应用因果掩码\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            # softmax归一化\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            # 计算注意力输出\n",
    "            y = att @ v\n",
    "            \n",
    "        # 合并多头输出\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        # 投影和dropout\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248d3ade-be8f-486c-9190-db393cb9922a",
   "metadata": {},
   "source": [
    "- 前馈网络\n",
    "  - MLP类作为一个前馈网络层的实现。\n",
    "  - 在Transformer架构中，这样的前馈网络常见于每一个注意力模块之后的位置，并对序列中的每个位置都执行相同的操作。\n",
    "  - 该层一个完全连接的网络层，通常用于**特征的非线性变换**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8188a6f-e4c1-4c52-a746-72e14ea8fdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前馈网络\n",
    "class MLP(nn.Module):\n",
    "    '''多层感知机模块\n",
    "    \n",
    "    参数:\n",
    "        config: GPT配置对象\n",
    "    '''\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)  # 扩展维度\n",
    "        self.gelu = nn.GELU()  # GELU激活函数\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)  # 投影回原维度\n",
    "        self.dropout = nn.Dropout(config.dropout)  # dropout层\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''前向传播\n",
    "        \n",
    "        参数:\n",
    "            x: 输入张量\n",
    "        返回:\n",
    "            前馈网络输出\n",
    "        '''\n",
    "        x = self.c_fc(x)#全连接层\n",
    "        x = self.gelu(x)#激活层\n",
    "        x = self.c_proj(x)#投影层\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e31157e-a99c-4225-a6cb-1d2ccf2f2885",
   "metadata": {},
   "source": [
    "- Transformer块\n",
    "  - 构建Decoder块，正如前文所述，和transformer原始解码器的结构有所区别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2e6219f-9db9-4e20-a8d2-ed56dcd42397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer块\n",
    "class Block(nn.Module):\n",
    "    '''Transformer块，包含自注意力和前馈网络\n",
    "    \n",
    "    参数:\n",
    "        config: GPT配置对象\n",
    "    '''\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)  # 第一个层归一化\n",
    "        self.attn = CausalSelfAttention(config)  # 自注意力层\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)  # 第二个层归一化\n",
    "        self.mlp = MLP(config)  # 前馈网络\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        前向传播。\n",
    "        注意：GPT-2将LayerNorm层归一化放在残差连接之前。\n",
    "        参数:\n",
    "            x: 输入张量\n",
    "        返回:\n",
    "            Transformer块输出\n",
    "        '''\n",
    "        # 残差连接 + 自注意力 + 层归一化\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        # 残差连接 + 前馈网络 + 层归一化\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e2994a-e174-409b-93c4-759a46647742",
   "metadata": {},
   "source": [
    "### 15.8.4.2 GPT实现类\n",
    "- 包括模型的构建、前向传播、参数初始化、优化器配置以及如何用模型进行文本生成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79e36150-1133-42e4-adda-89a6369e161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT模型\n",
    "class GPT(nn.Module):\n",
    "    '''完整的GPT模型实现\n",
    "    \n",
    "    参数:\n",
    "        config: GPT配置对象\n",
    "    '''\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None  # 必须指定词汇表大小\n",
    "        assert config.block_size is not None  # 必须指定块大小\n",
    "        self.config = config  # 保存配置\n",
    "        \n",
    "        # Transformer结构\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),  # 词嵌入\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),  # 位置嵌入\n",
    "            drop = nn.Dropout(config.dropout),  # dropout层\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),  # Transformer块列表\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias)  # 最终层归一化\n",
    "        ))\n",
    "        # 语言模型头\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        # 共享词嵌入和语言模型头的权重，形状皆为：(vocab_size, n_embd)\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # 初始化权重\n",
    "        self.apply(self._init_weights)\n",
    "        # 特殊初始化投影层\n",
    "        # GPT-2/GPT-3论文中均提到，对投影层采用缩放的初始化能显著提升深层模型的训练稳定性\n",
    "        # 注意'/math.sqrt(2 * config.n_layer)'操作\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        '''计算模型参数数量\n",
    "        \n",
    "        参数:\n",
    "            non_embedding: 是否排除位置嵌入参数\n",
    "        返回:\n",
    "            参数数量\n",
    "        '''\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        '''初始化权重\n",
    "        \n",
    "        参数:\n",
    "            module: 要初始化的模块\n",
    "        '''\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        '''前向传播\n",
    "        \n",
    "        参数:\n",
    "            idx: 输入token索引 (batch_size, seq_len)\n",
    "            targets: 目标token索引 (batch_size, seq_len)\n",
    "        返回:\n",
    "            如果提供targets: (logits, loss)\n",
    "            否则: (logits, None)\n",
    "        '''\n",
    "        device = idx.device\n",
    "        b, t = idx.size() # 批量大小和序列长度，后者等于block_size参数大小，即最大长度\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)  # 位置编码\n",
    "\n",
    "        # 获取词嵌入和位置嵌入\n",
    "        tok_emb = self.transformer.wte(idx)  # token嵌入 (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos)  # 位置嵌入 (t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)  # 相加并应用dropout\n",
    "        \n",
    "        # 通过所有Transformer块\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)  # 最终层归一化\n",
    "\n",
    "        if targets is not None:\n",
    "            '''训练模式: 计算所有位置的logits和loss'''\n",
    "            logits = self.lm_head(x)# 输出层\n",
    "            # view操作后logits形状为(B*T,vocab_size),targets的形状为(B*T,)\n",
    "            # ignore_index表示填充为-1的元素不参与损失计算\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            '''推理模式: 只计算最后一个位置的logits'''\n",
    "            logits = self.lm_head(x[:, [-1], :])  # 注意: 使用列表[-1]保持维度\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        '''配置优化器\n",
    "        \n",
    "        参数:\n",
    "            weight_decay: 权重衰减系数\n",
    "            learning_rate: 学习率\n",
    "            betas: Adam优化器的beta参数\n",
    "            device_type: 设备类型('cuda'或'cpu')\n",
    "        返回:\n",
    "            配置好的优化器\n",
    "        '''        \n",
    "        # 分离可衰减和不可衰减参数\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters() if p.requires_grad}\n",
    "        '''\n",
    "        （1）2D+参数（权重矩阵）：通常是全连接层（nn.Linear）的 weight 或卷积层（nn.Conv2d）的 kernel，\n",
    "                                形状为 (out_features, in_features) 或 (out_channels, in_channels, kH, kW)。\n",
    "            需要正则化：权重矩阵容易过拟合，通过权重衰减约束其数值大小（类似L2正则化）。\n",
    "        （2）1D参数（偏置等）：如 nn.Linear 的 bias 或 LayerNorm 的 weight/bias，形状为 (features,)。\n",
    "            不应用权重衰减：偏置项对模型输出的平移影响较小，正则化收益低。\n",
    "                           对LayerNorm的缩放/平移参数正则化可能破坏归一化的稳定性。\n",
    "         参考：Decoupled Weight Decay Regularization，网址：https://arxiv.org/pdf/1711.05101\n",
    "        '''\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]  # 只对2D+权重应用weight decay\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]  # 不对偏置等应用weight decay\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        # 使用fused AdamW如果可用\n",
    "        # 通过内核融合（Kernel Fusion）技术将多个计算步骤合并为单个GPU操作，显著提升训练速度\n",
    "        # 尤其在大规模深度学习模型中表现突出\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        return torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        '''生成文本\n",
    "        \n",
    "        参数:\n",
    "            idx: 初始token索引 (batch_size, seq_len)\n",
    "            max_new_tokens: 要生成的最大token数\n",
    "            temperature: 温度参数(控制随机性)\n",
    "            top_k: top-k采样参数\n",
    "        返回:\n",
    "            生成的token序列\n",
    "        '''\n",
    "        for _ in range(max_new_tokens):\n",
    "            # 如果序列太长，截断到block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # 获取预测,logits形状：(1, t, vocab_size)\n",
    "            logits, _ = self(idx_cond) \n",
    "            # 应用温度，温度值越大，结果越丰富越多样；越小，结果越保守越确定\n",
    "            # logits变形为[1,vocab_size]\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # 可选地应用top-k过滤\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                # 注意[-1]的用法，表示维度不变\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # 转换为概率\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # 采样下一个token，多项式采样方法\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # 添加到序列中\n",
    "            # 在推理时，输入是递增的\n",
    "            #本次的预测输出要增加到已预测的序列末尾作为下一次的输入\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dcade7-7fce-47ff-9647-b391b7446ce6",
   "metadata": {},
   "source": [
    "### 15.8.4.3 数据准备\n",
    "- 为了最快了解GPT的原理以及实战技巧，karpathy提供了一个训练GPT的莎士比亚作品数据集。\n",
    "- 首先将其下载为单个(1MB)文件，并将其从原始文本转换为一个大的整数流，即符号级的词元ID。\n",
    "- 训练参数：\n",
    "  - 以字符为单位的数据集长度:1115394\n",
    "  - 所有唯一性字符，即词表中的词元:\n",
    "    - \"\\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n",
    "  - 字符级词表大小: 65\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee94ba10-2328-4d90-99d4-52a8f654cd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据准备\n",
    "def prepare_data():\n",
    "    '''准备训练数据\n",
    "    \n",
    "    返回:\n",
    "        train_ids: 训练数据token索引\n",
    "        val_ids: 验证数据token索引\n",
    "        vocab_size: 词汇表大小\n",
    "        stoi: 字符到索引的映射\n",
    "        itos: 索引到字符的映射\n",
    "    '''\n",
    "    input_file_path = r'../data/gpt2/tinyshakespeare_input.txt'\n",
    "    # 如果文件不存在，下载数据\n",
    "    if not os.path.exists(input_file_path):\n",
    "        url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "        with open(input_file_path, 'w') as f:\n",
    "            f.write(requests.get(url).text)\n",
    "    \n",
    "    # 读取数据\n",
    "    with open(input_file_path, 'r') as f:\n",
    "        data = f.read()\n",
    "    print(f\"Dataset length: {len(data):,} characters\")\n",
    "\n",
    "    # 创建词汇表\n",
    "    chars = sorted(list(set(data)))\n",
    "    vocab_size = len(chars)\n",
    "    print(\"Unique characters:\", ''.join(chars))\n",
    "    print(f\"Vocab size: {vocab_size:,}\")\n",
    "\n",
    "    # 创建字符到索引和索引到字符的映射\n",
    "    stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "    itos = { i:ch for i,ch in enumerate(chars) }\n",
    "    \n",
    "    # 编码解码函数\n",
    "    def encode(s): return [stoi[c] for c in s]\n",
    "    def decode(l): return ''.join([itos[i] for i in l])\n",
    "    \n",
    "    # 划分训练集和验证集\n",
    "    n = len(data)\n",
    "    train_data = data[:int(n*0.9)]\n",
    "    val_data = data[int(n*0.9):]\n",
    "    \n",
    "    # 编码数据\n",
    "    train_ids = encode(train_data)\n",
    "    val_ids = encode(val_data)\n",
    "    print(f\"Train has {len(train_ids):,} tokens\")\n",
    "    print(f\"Val has {len(val_ids):,} tokens\")\n",
    "    \n",
    "    return train_ids, val_ids, vocab_size, stoi, itos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e20bb3-e911-45c0-94e9-c0f247b800e6",
   "metadata": {},
   "source": [
    "- 数据集的小批量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5682efc-dcb4-4b40-8953-42b3be15476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取批量数据\n",
    "def get_batch(split, train_ids, val_ids, batch_size, block_size, device):\n",
    "    '''获取一批训练或验证数据\n",
    "    \n",
    "    参数:\n",
    "        split: 'train'或'val'\n",
    "        train_ids: 训练数据\n",
    "        val_ids: 验证数据\n",
    "        batch_size: 批量大小\n",
    "        block_size: 块大小(序列长度)\n",
    "        device: 设备('cuda'或'cpu')\n",
    "    返回:\n",
    "        x: 输入序列 (batch_size, block_size)\n",
    "        y: 目标序列 (batch_size, block_size)\n",
    "    '''\n",
    "    data = train_ids if split == 'train' else val_ids\n",
    "    # 随机选择起始位置\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # 构造输入和目标\n",
    "    x = torch.stack([torch.tensor(data[i:i+block_size], dtype=torch.long) for i in ix])\n",
    "    y = torch.stack([torch.tensor(data[i+1:i+1+block_size], dtype=torch.long) for i in ix])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfc3e9f-a2f5-4d5e-947f-d275953fc3a8",
   "metadata": {},
   "source": [
    "### 15.8.4.4 训练GPT模型\n",
    "- 本示例主要是演示GPT模型结构，因此使用karpathy提供的莎士比亚数据进行训练。\n",
    "- 此微型模型是一个字符级，主要为了便于在PC机上进行训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6b16b11-ccbf-4f60-b11b-82e21cd0f17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练函数\n",
    "def train_model():\n",
    "    '''训练GPT模型\n",
    "    \n",
    "    返回:\n",
    "        model: 训练好的模型\n",
    "        stoi: 字符到索引的映射\n",
    "        itos: 索引到字符的映射\n",
    "    '''\n",
    "    # 准备数据\n",
    "    train_ids, val_ids, vocab_size, stoi, itos = prepare_data()\n",
    "    \n",
    "    # 配置模型\n",
    "    config = GPTConfig(\n",
    "        vocab_size=vocab_size,\n",
    "        block_size=256,\n",
    "        n_layer=6,\n",
    "        n_head=6,\n",
    "        n_embd=384,\n",
    "        dropout=0.2,\n",
    "        bias=False\n",
    "    )\n",
    "    \n",
    "    # 训练参数\n",
    "    batch_size = 64\n",
    "    max_iters = 1000\n",
    "    learning_rate = 1e-3\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    dtype = 'float16' if torch.cuda.is_available() else 'float32'\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = GPT(config)\n",
    "    model.to(device)\n",
    "    \n",
    "    # 配置优化器\n",
    "    optimizer = model.configure_optimizers(\n",
    "        weight_decay=1e-1,\n",
    "        learning_rate=learning_rate,\n",
    "        betas=(0.9, 0.95),\n",
    "        device_type=device\n",
    "    )\n",
    "    \n",
    "    # 训练循环\n",
    "    best_val_loss = float('inf')\n",
    "    for iter_num in range(max_iters):\n",
    "        # 获取批量数据\n",
    "        xb, yb = get_batch('train', train_ids, val_ids, batch_size, config.block_size, device)\n",
    "        \n",
    "        # 前向传播和反向传播\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 打印训练信息\n",
    "        if iter_num % 100 == 0:\n",
    "            print(f\"Iter {iter_num}: Train loss {loss.item():.4f}\")\n",
    "    \n",
    "    # 保存模型\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'config': config,\n",
    "        'stoi': stoi,\n",
    "        'itos': itos\n",
    "    }, 'gpt_model.pth')\n",
    "    \n",
    "    return model, stoi, itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5a09b64-1ea6-445c-8816-4c18eea40022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 1,115,394 characters\n",
      "Unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocab size: 65\n",
      "Train has 1,003,854 tokens\n",
      "Val has 111,540 tokens\n",
      "number of parameters: 10.65M\n",
      "Iter 0: Train loss 4.2706\n",
      "Iter 100: Train loss 2.7013\n",
      "Iter 200: Train loss 2.5174\n",
      "Iter 300: Train loss 2.3994\n",
      "Iter 400: Train loss 2.2518\n",
      "Iter 500: Train loss 2.0457\n",
      "Iter 600: Train loss 1.8641\n",
      "Iter 700: Train loss 1.7476\n",
      "Iter 800: Train loss 1.6565\n",
      "Iter 900: Train loss 1.5957\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "model, stoi, itos = train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bc4399-53dc-4e86-a3f8-4fa9c60e6d09",
   "metadata": {},
   "source": [
    "### 15.8.4.5 GPT文本生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "244097be-4be8-4951-8cd3-8fde626bd6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成文本函数\n",
    "def generate_text(model, stoi, itos, start=\"\\n\", max_new_tokens=2000, temperature=0.8, top_k=200):\n",
    "    '''使用训练好的模型生成文本\n",
    "    \n",
    "    参数:\n",
    "        model: 训练好的模型\n",
    "        stoi: 字符到索引的映射\n",
    "        itos: 索引到字符的映射\n",
    "        start: 起始字符串\n",
    "        max_new_tokens: 要生成的最大token数\n",
    "        temperature: 温度参数\n",
    "        top_k: top-k采样参数\n",
    "    '''\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # 编码解码函数\n",
    "    def encode(s): return [stoi[c] for c in s]\n",
    "    def decode(l): return ''.join([itos[i] for i in l])\n",
    "    \n",
    "    # 编码起始字符串\n",
    "    start_ids = encode(start)\n",
    "    x = torch.tensor(start_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    \n",
    "    # 生成文本\n",
    "    with torch.no_grad():\n",
    "        y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "        print(decode(y[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b6795da-4ada-4c29-9b60-69e5666406f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prove:\n",
      "Nay! O, be Joy, what merday, I am in your train\n",
      "The hastings anger to the stand a enye of.\n",
      "\n",
      "MENILIUS:\n",
      "And abso this to rue of you longue\n",
      "In excut of them aborniship so, and for true, kil bounds?\n",
      "\n",
      "POMPERIZA:\n",
      "I thank to thine we'ell neever is these drays, the\n",
      "cannot heir loal mornish' be thou sad: hare live move\n",
      "I have a gone?\n",
      "And masters we to did 'tis follows me.\n",
      "\n",
      "First I Murderemy, you and the tir king.\n",
      "\n",
      "Calllent father; if Cous!\n",
      "\n",
      "HENRY BOLIF GAREY:\n",
      "Pray Grand the lands me, forth, to give in the pall,\n",
      "fie may right and should of Rome,\n",
      "In the world that I selft servil.\n",
      "\n",
      "KING RICHARD III:\n",
      "Fir, a must shall were stay,\n",
      "To the that, they not shate father allones\n",
      "As aid nothing shall be peersent?\n",
      "\n",
      "POLIXENESES:\n",
      "Proud will not mades,\n",
      "I scoldier of shall had do: who the clannntay;\n",
      "And with that thy liker of on, who bacwds denign fare you:\n",
      "I dust be go did it you.\n",
      "\n",
      "KING RICHARD IING I:\n",
      "My gorne wisers dalep son:\n",
      "The city, knows, have in liboses our subject most demight.\n",
      "I now, you show not be foul feeeved?\n",
      "It care, of most I came be must the sdead,\n",
      "To they adday, like seend un my house:\n",
      "If I will not a silf must,\n",
      "That I bolt her to set live to Lancold!\n",
      "\n",
      "Chartizent, take Capria, a our grace's of the goes.\n",
      "\n",
      "GLOUCESTER:\n",
      "You will be by me confortanted kisck the soul peace.\n",
      "\n",
      "TRANIO:\n",
      "I both, I sea is the lasts,\n",
      "Which and revend thou should on this\n",
      "The inscorul the bear arms that rue as und misconder\n",
      "A disolering I know and sweeep to fond thy thall be\n",
      "My pargoved by heavens father;\n",
      "They were at a most soubter, call me.\n",
      "\n",
      "\n",
      "COMINIUS:\n",
      "I have untwenty tempt our this bankes,\n",
      "In see you crown's the lay of the nem.\n",
      "On, do lords, what you have toope\n",
      "I'll all youth.\n",
      "\n",
      "KING HENRY VI:\n",
      "I shen have stander silewin a \n",
      "The purpetts like of York arms.\n",
      "\n",
      "MENENENIUS:\n",
      "Vient it so to he scarre's not abose.\n",
      "\n",
      "KING RICHARD III:\n",
      "No, my hall inforced, which all on house.\n",
      "\n",
      "CAMILO:\n",
      "Thinks!\n",
      "\n",
      "Constel:\n",
      "Advaple you queenver acch the well the voices:\n",
      "O, sir.\n",
      "\n",
      "ESCALUS:\n",
      "CAPince, what had giness.\n",
      "\n",
      "COMINUS:\n",
      "How, \n"
     ]
    }
   ],
   "source": [
    "# 生成文本\n",
    "generate_text(model, stoi, itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf9e3b7-c272-4899-9d76-06fe457d86da",
   "metadata": {},
   "source": [
    "- **参考资料**\n",
    "  - [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)\n",
    "  - [OpenAI ChatGPT（二）：十分钟读懂 GPT-1](https://zhuanlan.zhihu.com/p/604625917)\n",
    "  - [ChatGPT/InstructGPT详解](https://zhuanlan.zhihu.com/p/590311003)\n",
    "  - [GPT-4核心技术探秘](https://zhuanlan.zhihu.com/p/626463196)\n",
    "  - [图解GPT-2 | The Illustrated GPT-2](https://lolitasian.blog.csdn.net/article/details/125529598)(强烈推荐)\n",
    "  - [GPT-4官方技术报告](https://cdn.openai.com/papers/gpt-4.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55207fcd-65bc-4749-915e-79a7de67961f",
   "metadata": {},
   "source": [
    "## 小结\n",
    "- GPT与ChatGPT的核心架构。GPT系列模型基于Transformer架构，通过自注意力机制处理长距离依赖关系，实现强大的文本生成能力。ChatGPT作为GPT的优化版本，专注于对话场景，通过改进训练技术和人类反馈强化学习（RLHF）提升交互质量。  \n",
    "- GPT关键训练技术。ChatGPT的训练过程结合了**人类反馈强化学习（RLHF）**和**TAMER框架**，通过人类偏好数据微调模型，使其输出更符合用户需求。GPT-4进一步优化了训练规模、数据多样性及多模态能力（如支持图像输入）。  \n",
    "- 实现流程与文本生成。实现GPT模型需经历**数据准备**（大规模文本清洗）、**模型构建**（Transformer堆叠）、**训练**（无监督预训练+有监督微调）和**文本生成**（基于概率采样的解码策略）四个阶段，最终生成连贯、上下文相关的文本。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
