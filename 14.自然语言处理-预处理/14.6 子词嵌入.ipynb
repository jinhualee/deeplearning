{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 14.6 子词嵌入(Subword Embedding)\n",
    "- **目录**\n",
    "  - 14.6.1 fastText模型\n",
    "  - 14.6.2 字节对编码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "- 在英语中，“helps”、“helped”和“helping”等单词都是同一个词“help”的变形形式。\n",
    "- “dog”和“dogs”之间的关系与“cat”和“cats”之间的关系相同，“boy”和“boyfriend”之间的关系与“girl”和“girlfriend”之间的关系相同。\n",
    "- 在法语和西班牙语等其他语言中，许多动词有40多种变形形式，而在芬兰语中，名词最多可能有15种变形。\n",
    "- 在语言学中，**形态学研究单词形成和词汇关系**。\n",
    "- 但是，word2vec和GloVe都没有对词的内部结构进行探讨。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "## 14.6.1 fastText模型\n",
    "\n",
    "回想一下词在word2vec中是如何表示的。在跳元模型和连续词袋模型中，**同一词的不同变形形式直接由不同的向量表示**，不需要共享参数。为了使用形态信息，**fastText模型**提出了一种**子词嵌入**方法，其中子词是一个字符$n$-gram 。fastText可以被认为是**子词级跳元模型**，而非学习词级向量表示，其中每个**中心词**由其**子词级向量之和**表示。\n",
    "\n",
    "让我们来说明如何以单词“where”为例获得fastText中每个中心词的子词。首先，在词的开头和末尾添加特殊字符“&lt;”和“&gt;”，以将前缀和后缀与其他子词区分开来。\n",
    "然后，从词中提取字符$n$-gram。\n",
    "例如，值$n=3$时，我们将获得长度为3的所有子词：\n",
    "“&lt;wh”、“whe”、“her”、“ere”、“re&gt;”和特殊子词“&lt;where&gt;”。\n",
    "\n",
    "在fastText中，对于任意词$w$，用$\\mathcal{G}_w$表示其长度在3和6之间的所有子词与其特殊子词的并集。词表是所有词的子词的集合。假设$\\mathbf{z}_g$是词典中的子词$g$的向量，则跳元模型中作为中心词的词$w$的向量$\\mathbf{v}_w$是其子词向量的和：\n",
    "\n",
    "$$\\mathbf{v}_w = \\sum_{g\\in\\mathcal{G}_w} \\mathbf{z}_g. \\tag{14.6.1}$$\n",
    "\n",
    "fastText的其余部分与跳元模型相同。与跳元模型相比，fastText的词量更大，模型参数也更多。此外，**为了计算一个词的表示，它的所有子词向量都必须求和**，这导致了更高的计算复杂度。然而，**由于具有相似结构的词之间共享来自子词的参数，罕见词甚至词表外的词在fastText中可能获得更好的向量表示**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText子词: ['<wh', 'whe', 'her', 'ere', 're>', '<whe', 'wher', 'here', 'ere>', '<wher', 'where', 'here>', '<where', 'where>', '<where>']\n"
     ]
    }
   ],
   "source": [
    "# fastText的n-gram生成\n",
    "word = \"where\"\n",
    "word = '<' + word + '>'\n",
    "ngrams = []\n",
    "for n in range(3, 7):\n",
    "    for i in range(len(word) - n + 1):\n",
    "        ngrams.append(word[i:i+n])\n",
    "ngrams.append(word)\n",
    "print(\"FastText子词:\", ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "- **要点：**\n",
    "  - **词的表示**：在word2vec中，同一词的不同形态变形是通过不同的向量直接表示的，不共享参数。\n",
    "  - **子词嵌入**：为了利用**形态信息**，fastText模型引入了子词嵌入方法。这里的子词指的是字符$n$-gram。\n",
    "  - **中心词表示**：fastText可以看作是子词级的跳元模型。与其学习词级的向量表示，每个中心词在fastText中是由其子词的向量之和表示的。\n",
    "  - **子词的提取**：\n",
    "    - 为词加上特殊字符“<”和“>”标记词的开头和结尾。\n",
    "    - 提取长度为$n$的字符子词。例如，对于词“where”且$n=3$，我们有子词：“<wh”、“whe”、“her”、“ere”、“re>”和特殊子词“<where>”。\n",
    "  - **子词集合**：对于任何词$w$，$\\mathcal{G}_w$表示其所有长度在3和6之间的子词加上它的特殊子词的集合。\n",
    "  - **词表**：是所有词的子词的集合。词$w$的向量表示$\\mathbf{v}_w$是其所有子词向量的和，如公式\\(14.6.1\\)所示。\n",
    "  - **与跳元模型的对比**：\n",
    "    - fastText的词量更大，有更多的模型参数。\n",
    "    - 计算一个词的表示需要求所有子词向量的和，导致更高的计算复杂度。\n",
    "    - 但由于形态相似的词可以共享子词参数，所以罕见词和词表外的词在fastText中可能获得更好的向量表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "## 14.6.2 字节对编码（Byte Pair Encoding）\n",
    "- 在fastText中，所有提取的子词都必须是指定的长度，例如$3$到$6$，因此词表大小不能预定义。\n",
    "- 为了在固定大小的词表中**允许可变长度的子词**，可以应用一种称为**字节对编码（Byte Pair Encoding，BPE）** 的压缩算法来提取子词。\n",
    "  - 字节对编码执行训练数据集的统计分析，以发现单词内的**公共符号**，诸如任意长度的连续字符。\n",
    "  - 从长度为1的符号开始，字节对编码**迭代地合并最频繁的连续符号对**以产生新的更长的符号。\n",
    "    - 请注意，为提高效率，不考虑跨越单词边界的对。\n",
    "  - 最后可以使用像**子词**这样的符号来切分单词。\n",
    "  - 字节对编码及其变体已经用于诸如GPT-2和RoBERTa等自然语言处理预训练模型中的输入表示。\n",
    "- GPT-3也使用BPE算法构建词表：\n",
    "  - 词表包含50,257个token，包含常见单词、子词（subword）、符号和部分多语言字符。\n",
    "  - 其中256个token 保留给特殊控制字符（如换行符、制表符等）。\n",
    "  - 中文处理：\n",
    "    - 单个汉字通常作为独立 token（如 \"人工智能\" → \"人\" + \"工\" + \"智\" + \"能\"），效率较低。\n",
    "    - 部分常见词组可能合并（如 “北京” → 单个 token）。\n",
    "  - 代码处理：编程语言关键词（如 if, def）和符号（如 +=）有独立 token。\n",
    "  - GPT-3通过BPE编码的语料库大概有400B(4000亿)个左右的token，处理之前的原始语料是45T。\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "- 字节对编码算法包含如下步骤及其变量与函数：\n",
    "  - 初始化符号词表symbols。\n",
    "  - 统计训练数据中每个词及其频率raw_token_freqs。\n",
    "  - 将每个词拆分为单字符并用空格分隔，形成符号序列token_freqs。\n",
    "  - 迭代合并高频符号对：\n",
    "    - 使用get_max_freq_pair函数统计相邻符号对频率。\n",
    "    - 使用merge_symbols函数合并最高频符号对\n",
    "  - 生成最终子词词表。\n",
    "  - 使用segment_BPE函数进行语料数据的子词分割（推理阶段）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "- 首先，此处将符号词表初始化为所有**英文小写字符**、**特殊的词尾函数符号`'_'`** 和 **特殊的未知符号`'[UNK]'`**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "origin_pos": 1,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "symbols = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "           'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
    "           '_', '[UNK]']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 2
   },
   "source": [
    "- 因为不考虑跨越词边界的符号对（即仅统计和合并同一个单词内部的连续字符对），所以我们只需要一个字典`raw_token_freqs`将词映射到数据集中的频率（出现次数）。\n",
    "- 注意，特殊符号`'_'`被附加到每个词的尾部，以便可以容易地从输出符号序列（例如，“a_ tall er_ man”）恢复单词序列（例如，“a taller man”）。\n",
    "- 由于我们仅从单个字符和特殊符号的词开始合并处理，所以在每个词（词典`token_freqs`的键）内的每对连续字符之间插入空格。\n",
    "- 换句话说，**空格是词中符号之间的分隔符**。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "origin_pos": 3,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f a s t _': 4, 'f a s t e r _': 3, 't a l l _': 5, 't a l l e r _': 4}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_token_freqs = {'fast_': 4, 'faster_': 3, 'tall_': 5, 'taller_': 4}\n",
    "token_freqs = {}\n",
    "\n",
    "for token, freq in raw_token_freqs.items():\n",
    "    # 注意list和' '.join的用法\n",
    "    token_freqs[' '.join(list(token))] = raw_token_freqs[token]\n",
    "token_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "- 定义以下`get_max_freq_pair`函数：\n",
    "  - 该函数返回词内**最频繁的连续符号对**。\n",
    "  - 其中词来自输入词典`token_freqs`的键。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "origin_pos": 5,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "## 原代码\n",
    "def get_max_freq_pair(token_freqs):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for token, freq in token_freqs.items():\n",
    "        symbols = token.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            # “pairs”的键是两个连续符号的元组\n",
    "            pairs[symbols[i], symbols[i + 1]] += freq\n",
    "    return max(pairs, key=pairs.get)  # 具有最大值的“pairs”键"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get_max_freq_pair完整注释"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 获取最频繁的符号对\n",
    "## 接受一个词频词典token_freqs作为参数\n",
    "def get_max_freq_pair(token_freqs):\n",
    "    '''\n",
    "    使用collections.defaultdict初始化一个默认字典pairs。\n",
    "    该字典用于存储符号对及其出现的频率。\n",
    "    defaultdict(int)意味着任何尚未存在于字典中的键都会默认具有值int()，也就是0。\n",
    "    '''\n",
    "    pairs = collections.defaultdict(int)\n",
    "    ## 开始遍历token_freqs字典的每个条目，其中token是词，freq是词的出现频率\n",
    "    for token, freq in token_freqs.items():\n",
    "        ## 将token（一个由空格分隔的符号字符串）拆分成一个符号列表，并赋值给symbols\n",
    "        symbols = token.split()\n",
    "        ## 遍历symbols列表以便查找两个连续符号，但停在倒数第二个符号\n",
    "        for i in range(len(symbols) - 1):\n",
    "            # “pairs”的键是两个连续符号的元组\n",
    "            '''\n",
    "            更新pairs字典中当前连续符号对的频率。\n",
    "            键是符号对的元组(symbols[i], symbols[i + 1])，增加该键对应的频率值freq，\n",
    "            比如'f a'在'f a s t _'的词频是4, 在'f a s t e r _'是3，二者相加等于7。\n",
    "            '''\n",
    "            pairs[symbols[i], symbols[i + 1]] += freq\n",
    "    \n",
    "    '''\n",
    "    在所有连续符号对中返回具有最大频率的那对。\n",
    "    key=pairs.get意味着使用pairs词典的值（即频率）作为决定“最大”的标准。\n",
    "    '''\n",
    "    return max(pairs, key=pairs.get)  # 具有最大值的“pairs”键"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('t', 'a')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 获取最频繁符号对方法调用示例\n",
    "get_max_freq_pair(token_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('c', 'd')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max与字典类型结合的用法\n",
    "d = {('a','b'):10, ('b','c'):20, ('c','d'):30}\n",
    "max(d, key=d.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "- 作为基于连续符号频率的贪心方法，字节对编码将使用以下`merge_symbols`函数来合并最频繁的连续符号对以产生新符号。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "## 原代码\n",
    "def merge_symbols(max_freq_pair, token_freqs, symbols):\n",
    "    symbols.append(''.join(max_freq_pair))\n",
    "    new_token_freqs = dict()\n",
    "    for token, freq in token_freqs.items():\n",
    "        new_token = token.replace(' '.join(max_freq_pair),\n",
    "                                  ''.join(max_freq_pair))\n",
    "        new_token_freqs[new_token] = token_freqs[token]\n",
    "    return new_token_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- merge_symbols完整注释"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "## 注释代码\n",
    "'''\n",
    "两种解释：\n",
    "（1）合并最高频率的符号对，并更新词频字典来反映这种合并。\n",
    "     这是字节对编码（BPE）算法中的一个关键步骤，\n",
    "     持续合并最常见的符号对，直到达到所需的词表大小或其他停止条件。\n",
    "（2）将token_freqs中出现频率最高的符号对max_freq_pair合并，\n",
    "     并更新token_freqs字典来反映这种合并。\n",
    "     同时，新的合并符号被添加到symbols列表中。\n",
    "'''\n",
    "## 三个参数：最高频率的符号对max_freq_pair、词频字典token_freqs和符号列表symbols。\n",
    "def merge_symbols(max_freq_pair, token_freqs, symbols):\n",
    "    '''\n",
    "    将max_freq_pair中的两个符号合并成一个字符串，\n",
    "    然后将其添加到symbols列表的末尾。\n",
    "    例如，如果max_freq_pair是('t', 'a')，\n",
    "    则它会被合并为'ta'并添加到symbols列表中。\n",
    "    '''\n",
    "    symbols.append(''.join(max_freq_pair))\n",
    "    ## 新字典对象用于存储合并后的词及其频率\n",
    "    new_token_freqs = dict()\n",
    "    \n",
    "    for token, freq in token_freqs.items():\n",
    "        '''\n",
    "        替换词中的最高频率的符号对。\n",
    "        它首先将max_freq_pair中的符号转换为由空格分隔的字符串（例如，'t a'），\n",
    "        然后在token中找到这个字符串并将其替换为无空格的版本（例如，'ta'）。\n",
    "        结果存储在new_token中。\n",
    "        '''\n",
    "        new_token = token.replace(' '.join(max_freq_pair),\n",
    "                                  ''.join(max_freq_pair))\n",
    "        ## 将原始频率从token_freqs复制到新词典new_token_freqs，以new_token为键。\n",
    "        new_token_freqs[new_token] = token_freqs[token]\n",
    "    return new_token_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f a s t _': 4, 'f a s t e r _': 3, 'ta l l _': 5, 'ta l l e r _': 4}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## merge_symbols调用示例\n",
    "w = get_max_freq_pair(token_freqs)\n",
    "merge_symbols(w, token_freqs, symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "- 现在对词典`token_freqs`的键**迭代地**执行字节对编码算法。\n",
    "  - 在第一次迭代中，最频繁的连续符号对是`'t'`和`'a'`，因此字节对编码将它们合并以产生新符号`'ta'`。\n",
    "  - 在第二次迭代中，字节对编码继续合并`'ta'`和`'l'`以产生另一个新符号`'tal'`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "origin_pos": 9,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合并# 1: ('t', 'a')\n",
      "合并# 2: ('ta', 'l')\n",
      "合并# 3: ('tal', 'l')\n",
      "合并# 4: ('f', 'a')\n",
      "合并# 5: ('fa', 's')\n",
      "合并# 6: ('fas', 't')\n",
      "合并# 7: ('e', 'r')\n",
      "合并# 8: ('er', '_')\n",
      "合并# 9: ('tall', '_')\n",
      "合并# 10: ('fast', '_')\n"
     ]
    }
   ],
   "source": [
    "num_merges = 10\n",
    "for i in range(num_merges):\n",
    "    max_freq_pair = get_max_freq_pair(token_freqs)\n",
    "    token_freqs = merge_symbols(max_freq_pair, token_freqs, symbols)\n",
    "    print(f'合并# {i+1}:',max_freq_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "- 在字节对编码的10次迭代之后，可以看到列表`symbols`现在又包含10个从其他符号迭代合并而来的符号。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "origin_pos": 11,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '_', '[UNK]', 'ta', 'ta', 'tal', 'tall', 'fa', 'fas', 'fast', 'er', 'er_', 'tall_', 'fast_']\n"
     ]
    }
   ],
   "source": [
    "print(symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "- 对于在词典`raw_token_freqs`的键中指定的同一数据集，作为字节对编码算法的结果，数据集中的每个词现在被子词“fast_”、“fast”、“er_”、“tall_”和“tall”分割。\n",
    "  - 例如，单词“faster_”和“taller_”分别被分割为“fast er_”和“tall er_”。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "origin_pos": 13,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fast_', 'fast er_', 'tall_', 'tall er_']\n"
     ]
    }
   ],
   "source": [
    "print(list(token_freqs.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fast_': 4, 'fast er_': 3, 'tall_': 5, 'tall er_': 4}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 14
   },
   "source": [
    "- 请注意，字节对编码的结果取决于正在使用的数据集。\n",
    "- 还可以使用**从一个数据集学习的子词来切分另一个数据集的单词**。\n",
    "- 作为一种**贪心方法**，下面的`segment_BPE`函数尝试将单词从输入参数`symbols`分成**可能最长**的子词。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "origin_pos": 15,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def segment_BPE(tokens, symbols):\n",
    "    outputs = []\n",
    "    for token in tokens:\n",
    "        start, end = 0, len(token)\n",
    "        cur_output = []\n",
    "        # 具有符号中可能最长子字的词元段\n",
    "        while start < len(token) and start < end:\n",
    "            if token[start: end] in symbols:\n",
    "                cur_output.append(token[start: end])\n",
    "                start = end\n",
    "                end = len(token)\n",
    "            else:\n",
    "                end -= 1\n",
    "        if start < len(token):\n",
    "            cur_output.append('[UNK]')\n",
    "        outputs.append(' '.join(cur_output))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- segment_BPE详细注释"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokens是待分割的新词元，一般是完整的词元加上\"_\"符号\n",
    "## symbols是经过BPE子词划分后的符号，是子词分割的依据\n",
    "def segment_BPE(tokens, symbols):\n",
    "    outputs = [] #保存子词分割后的列表\n",
    "    for token in tokens: #取出一个词元\n",
    "        start, end = 0, len(token) #子词分割的起始点和结束点\n",
    "        cur_output = [] #当前词元的子词划分\n",
    "        # 具有符号中可能最长子字的词元段\n",
    "        '''\n",
    "        对于词元的子词分割使用贪婪算法，即从最大长度开始匹配。\n",
    "        （1）如果最大长度的子词在符号列表symbols中存在，\n",
    "             即将其作为子词存放在当前词元的子词列表cur_output中。\n",
    "             然后将start赋值为end。\n",
    "        （2）如果token[start: end]在symbols不存在，则将end减1，\n",
    "             此时token[start: end]的字符减少一个即最后一个字符。\n",
    "             然后再将token[start: end]匹配symbols，即回到步骤（1）。\n",
    "             \n",
    "        以token='tallest_'为例：\n",
    "        （1）按照贪心算法多次迭代即end-1后，\n",
    "             symbols中的'tall'匹配。\n",
    "        （2）然后start=end，此时start指向'tallest_'\n",
    "             中的'e'，end=len(token)再次指向'tallest_'的末尾后面一个字符。\n",
    "        （3）对剩下部分\"est_\"进行子词分割,回到第一步。\n",
    "\n",
    "        注意end是词元长度，按照列表的索引方式，它是指向token最后一个字符的后面一个单元。\n",
    "        比如刚开始迭代时，token[start: end]包含整个token的所有字符。\n",
    "        '''\n",
    "        while start < len(token) and start < end: #示例：tallest_\n",
    "            if token[start: end] in symbols:\n",
    "                cur_output.append(token[start: end])\n",
    "                start = end\n",
    "                end = len(token)\n",
    "            else:\n",
    "                end -= 1\n",
    "        # 如果直到匹配结束在symbols仍找不到子词，则将'[UNK]'添加到当前词元的子词列表中\n",
    "        # 表示未知符号或子词\n",
    "        if start < len(token):\n",
    "            cur_output.append('[UNK]')\n",
    "        # 将当前词元的子词使用空格分开，然后作为一个完整字符串添加到最终输出\n",
    "        outputs.append(' '.join(cur_output))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 16
   },
   "source": [
    "- 使用列表`symbols`中的子词（从前面提到的数据集学习）来表示另一个数据集的`tokens`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "origin_pos": 17,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tall e s t _', 'fa t t er_']\n"
     ]
    }
   ],
   "source": [
    "tokens = ['tallest_', 'fatter_']\n",
    "print(segment_BPE(tokens, symbols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 18
   },
   "source": [
    "## 小结\n",
    "\n",
    "* fastText模型提出了一种子词嵌入方法：基于word2vec中的跳元模型，它将中心词表示为其子词向量之和。\n",
    "* 字节对编码执行训练数据集的统计分析，以发现词内的公共符号。作为一种贪心方法，字节对编码迭代地合并最频繁的连续符号对。\n",
    "* 子词嵌入可以提高稀有词和词典外词的表示质量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "- **说明：DeepSeek的词表构建与分词技术**\n",
    "  - DeepSeek 作为一款强大的中文大模型，在词表（Tokenization）设计上采用了多项针对中文优化的技术，以解决传统BPE方法在中文处理上的效率低下问题。\n",
    "  - **1. 混合分词策略（Hybrid Tokenization）**\n",
    "    - **问题**：传统 BPE 对中文按单字拆分（如 \"人工智能\" → `\"人\"+\"工\"+\"智\"+\"能\"`），导致 token 数量爆炸，效率低下。  \n",
    "    - **DeepSeek 的解决方案：结合 BPE 与词典分词**：  \n",
    "      - 预加载高频中文词汇（如“人工智能”“北京”），直接保留为完整 token，减少拆分。  \n",
    "      - 低频词仍按 BPE 子词规则处理。  \n",
    "    - **示例**：  \n",
    "      - 传统 BPE：\"人工智能\" → 4 tokens  \n",
    "      - DeepSeek：\"人工智能\" → 1 token（若在预置词典中）\n",
    "  - **2. 汉字优先编码（Chinese-Character-Centric BPE）**\n",
    "    - **问题**：通用 BPE 对多语言混合优化，中文压缩率低。  \n",
    "    - **DeepSeek 的优化：单独训练中文子词表**：  \n",
    "      - 在中文语料上独立训练 BPE，优先合并常见汉字组合（如“的”“是”“中国”）。  \n",
    "      - 英文和符号沿用标准 BPE，但权重降低。  \n",
    "    - **效果**：  \n",
    "      - 中文平均 token 数减少 30%-50%（如句子“深度学习很重要”从 7 tokens 降至 3-4 tokens）。\n",
    "  - **3. 动态分词（Dynamic Tokenization）**\n",
    "    - **问题**：专业领域术语（如“Transformer”“贝叶斯”）可能被错误拆分。  \n",
    "    - **DeepSeek 的改进：领域自适应词表**  \n",
    "      - 针对医学、法律、编程等领域，动态加载领域词典，强制保留术语为完整 token。  \n",
    "        - 例如：“冠状动脉”在医疗文本中作为 1 个 token，而非 `\"冠状\"+\"动脉\"`。  \n",
    "      - **用户自定义词表**：允许用户添加新词（如品牌名“深度求索”），避免拆分。\n",
    "  - **4. Unicode 归一化与繁体字处理**\n",
    "    - **问题**：中文存在简繁变体（如“语” vs “語”），增加词表冗余。  \n",
    "    - **DeepSeek 的优化**：\n",
    "      - **简繁映射**：  \n",
    "        - 在 tokenizer 预处理阶段，将繁体字自动转换为简体（可配置关闭）。  \n",
    "        - 减少词表中重复 token（如“说”和“說”合并）。  \n",
    "      - **全角/半角统一**： 将全角符号$，．$转为半角$, .$，降低符号多样性。\n",
    "  - **5. 高频子词强制合并**\n",
    "    - **问题**：常见中文后缀（如“们”“的”“性”）被重复编码。  \n",
    "    - **DeepSeek 的策略：统计驱动合并**\n",
    "      - 强制合并高频后缀/前缀（如“的”、“了”、“主义”），即使 BPE 统计频率未达阈值。  \n",
    "      - 例如：“科学家们” → `\"科学家\" + \"们\"`（而非 `\"科学\" + \"家\" + \"们\"`）。\n",
    "\n",
    "  - **6. 与其他语言的兼容性**\n",
    "    - **多语言混合处理**：  \n",
    "      - **分层词表**：  \n",
    "        - 中文和英文分别使用独立的子词表，在模型输入层拼接。  \n",
    "        - 避免中英混合文本的冲突（如“ChatGPT”和“聊天机器人”各自优化）。  \n",
    "      - **代码处理**：保留编程语言关键词（如 `if`, `def`）为完整 token，与中文自然分词隔离。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **附录：fastText模型训练代码示例**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [00:01<00:00, 17.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 41.6985013759154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 26/26 [00:00<00:00, 214.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 24.534544253865114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 26/26 [00:00<00:00, 127.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 11.722990141462752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 26/26 [00:00<00:00, 159.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 2.8384052148978447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 26/26 [00:00<00:00, 129.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 6.967051905382994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_ngrams(word, min_n=3, max_n=6):\n",
    "    word = \"<\" + word + \">\"\n",
    "    ngrams = []\n",
    "    for n in range(min_n, max_n + 1):\n",
    "        for i in range(len(word) - n + 1):\n",
    "            ngrams.append(word[i:i+n])\n",
    "    return ngrams + [\"<\" + word[1:-1] + \">\"]  # 添加整个单词作为特殊子词\n",
    "\n",
    "def build_vocab(corpus, min_count=1):\n",
    "    word_counts = Counter(corpus)\n",
    "    vocab = {word: idx for idx, (word, count) in enumerate(word_counts.items())}\n",
    "    \n",
    "    # 为所有单词生成子词并加入词汇表\n",
    "    subword_vocab = set()\n",
    "    for word in vocab:\n",
    "        subword_vocab.update(get_ngrams(word))\n",
    "    \n",
    "    # 为子词分配索引，从len(vocab)开始\n",
    "    subword_vocab = {g: idx + len(vocab) for idx, g in enumerate(subword_vocab)}\n",
    "    \n",
    "    # 合并单词和子词词汇表\n",
    "    vocab.update(subword_vocab)\n",
    "    return vocab\n",
    "\n",
    "class FastText(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(FastText, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.subword_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def forward(self, word_ids, subword_ids_list):\n",
    "        word_vecs = self.word_embeddings(word_ids)\n",
    "        \n",
    "        subword_vecs = []\n",
    "        for subword_ids in subword_ids_list:\n",
    "            # 确保所有子词索引有效\n",
    "            valid_subword_ids = [idx for idx in subword_ids if idx < self.vocab_size]\n",
    "            if not valid_subword_ids:\n",
    "                # 如果没有有效子词，使用零向量\n",
    "                subword_vec = torch.zeros(self.embedding_dim)\n",
    "            else:\n",
    "                subword_vec = self.subword_embeddings(torch.LongTensor(valid_subword_ids))\n",
    "                subword_vec = torch.sum(subword_vec, dim=0)\n",
    "            subword_vecs.append(subword_vec)\n",
    "        \n",
    "        subword_vecs = torch.stack(subword_vecs)\n",
    "        combined_vecs = word_vecs + subword_vecs\n",
    "        return combined_vecs\n",
    "\n",
    "def negative_sampling(target_word_idx, vocab, num_neg_samples=5):\n",
    "    valid_indices = [idx for idx in range(len(vocab)) if idx != target_word_idx]\n",
    "    return random.sample(valid_indices, min(num_neg_samples, len(valid_indices)))\n",
    "\n",
    "def train_fasttext(corpus, embedding_dim=100, epochs=10, lr=0.01):\n",
    "    vocab = build_vocab(corpus)\n",
    "    train_data = []\n",
    "    \n",
    "    # 构建训练数据 (中心词, 上下文词) 对\n",
    "    window_size = 2\n",
    "    for i, center_word in enumerate(corpus):\n",
    "        for j in range(max(0, i-window_size), min(len(corpus), i+window_size+1)):\n",
    "            if i != j:\n",
    "                train_data.append((center_word, corpus[j]))\n",
    "    \n",
    "    model = FastText(len(vocab), embedding_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for center_word, context_word in tqdm(train_data):\n",
    "            # 确保单词在词汇表中\n",
    "            if center_word not in vocab or context_word not in vocab:\n",
    "                continue\n",
    "                \n",
    "            center_id = vocab[center_word]\n",
    "            context_id = vocab[context_word]\n",
    "            \n",
    "            # 获取中心词的子词\n",
    "            subword_ids = [vocab[g] for g in get_ngrams(center_word) if g in vocab]\n",
    "            \n",
    "            # 前向传播\n",
    "            combined_vec = model(torch.LongTensor([center_id]), [subword_ids])\n",
    "            \n",
    "            # 负采样\n",
    "            neg_samples = negative_sampling(context_id, vocab)\n",
    "            neg_samples_vec = model.word_embeddings(torch.LongTensor(neg_samples))\n",
    "            \n",
    "            # 计算损失\n",
    "            pos_score = torch.matmul(combined_vec, model.word_embeddings.weight[context_id].unsqueeze(0).T)\n",
    "            neg_scores = torch.matmul(combined_vec, neg_samples_vec.T)\n",
    "            logits = torch.cat([pos_score, neg_scores], dim=1)\n",
    "            labels = torch.LongTensor([0])  # 正样本在logits的第0位置\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            # 反向传播\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_data)}\")\n",
    "\n",
    "    return model, vocab\n",
    "\n",
    "# 训练\n",
    "corpus = [\"I\", \"love\", \"natural\", \"language\", \"processing\", \"fasttext\", \"is\", \"great\"]\n",
    "model, vocab = train_fasttext(corpus, embedding_dim=50, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9717,  0.6840,  0.0682,  ...,  0.4309, -1.5068,  1.2907],\n",
       "        [-0.4200, -0.6563, -0.5629,  ...,  0.0742,  0.3027, -1.5285],\n",
       "        [-0.2640,  0.6243, -0.0507,  ...,  1.1935,  0.2019,  0.2567],\n",
       "        ...,\n",
       "        [-0.8082, -0.2924, -0.6600,  ..., -0.3907,  1.1086,  0.1791],\n",
       "        [ 1.2585,  2.0222,  2.1341,  ...,  1.0742, -1.9727,  1.9272],\n",
       "        [-1.2519, -0.2590,  0.4494,  ..., -2.1394,  1.3334, -0.6800]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.word_embeddings.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6084,  1.6022,  0.5933,  ...,  1.3943, -0.7877, -1.1504],\n",
       "        [-0.3314,  0.1283,  0.4402,  ...,  0.2284, -0.0147,  0.3598],\n",
       "        [-0.0056, -1.7762, -1.0277,  ...,  0.9704,  1.8002,  0.8024],\n",
       "        ...,\n",
       "        [-0.7988,  0.0888,  1.0798,  ..., -1.7981,  1.0652,  0.6942],\n",
       "        [-1.4244,  1.4719,  1.3277,  ..., -1.0287,  0.3502,  0.2620],\n",
       "        [-1.0979,  0.1336,  0.8745,  ..., -1.4876, -0.9884, -0.7860]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.subword_embeddings.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147,\n",
       " ['I',\n",
       "  'love',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'processing',\n",
       "  'fasttext',\n",
       "  'is',\n",
       "  'great',\n",
       "  'ttext',\n",
       "  'text'],\n",
       " ['essin',\n",
       "  '<lan',\n",
       "  '<natur',\n",
       "  'ces',\n",
       "  '<fasttext>',\n",
       "  '<love>',\n",
       "  'asttex',\n",
       "  'cess',\n",
       "  'atur',\n",
       "  'oces'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(vocab)),list(vocab)[:10],list(vocab)[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
