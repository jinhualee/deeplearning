{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22d7d6fd-fa7a-433b-a2ba-5390ea28bbcd",
   "metadata": {},
   "source": [
    "## 16.2 开源大语言模型部署应用\n",
    "- **目录**\n",
    "  - 16.2.1 Gradio部署开源大语言模型\n",
    "    - 16.2.1.1 Gradio简介\n",
    "    - 16.2.1.2 使用Gradio部署基于LLM的聊天机器人\n",
    "  - 16.2.2 Ollama与Open WebUI部署开源大语言模型\n",
    "    - 16.2.2.1 Ollama与Open WebUI简介\n",
    "    - 16.2.2.2 Ollama部署大语言模型的过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47156610-972d-45b8-b256-4ccf99331e1f",
   "metadata": {},
   "source": [
    "- 用于开源大语言模型部署和应用的程序和工具有很多，比如LangChain(包括Web LangChain和LangServe等), FastChat, vLLM, LLMStack, llama.cpp, Gradio, Ollama, Open WebUI等。\n",
    "- 下面使用Gradio和Ollama两个开源软件，部署和应用开源LLMs。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333153aa-7e90-4d64-a0c9-fed908687824",
   "metadata": {},
   "source": [
    "### 16.2.1 Gradio部署开源大语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52848cee-d4dc-4a1d-82c4-10d84342bc63",
   "metadata": {},
   "source": [
    "#### 16.2.1.1 Gradio简介\n",
    "\n",
    "- Gradio是一个开源的 Python 库，它允许你快速地为机器学习模型创建可交互的网页界面。使用Gradio，研究人员和开发人员可以方便地展示他们的工作，使得非技术用户也能够轻松地尝试机器学习模型的功能，无需进行复杂的安装或设置。\n",
    "- **核心特性：**\n",
    "  - **易用性**：\n",
    "    - Gradio 可以用几行代码就将机器学习模型变为交互式应用。它提供了各种输入和输出组件，如文本框、图像上传、滑块等。\n",
    "  - **灵活性**：\n",
    "    - 支持多种数据类型的输入和输出，包括文本、图像、音频和视频。\n",
    "  - **兼容性**：\n",
    "    - 可以与 TensorFlow、PyTorch、scikit-learn 等主流机器学习框架无缝集成。\n",
    "  - **即时反馈**：\n",
    "     - 用户可以实时看到他们输入数据的模型预测结果，这是一个强大的特性，尤其是在演示模型时。\n",
    "  - **共享和部署**：\n",
    "    - Gradio 提供了一个链接，通过这个链接其他用户可以访问你的模型，无需设置服务器或使用云服务。\n",
    "  - **隐私保护**：\n",
    "    - 所有数据都在用户的本地计算机上处理，不通过外部服务器。\n",
    "  - **集成到科学生态系统**：\n",
    "    - 可以集成到 Jupyter 笔记本或 Python 脚本中。\n",
    "  - **定制化**：\n",
    "    - 提供了样式和布局的自定义选项，以及用于更复杂交互的接口供了样式和布局的自定义选项，以及用于更复杂交互的接口。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be008018-b6d5-42f9-8bfc-101dea2c58d6",
   "metadata": {},
   "source": [
    "- Gradio为用户自己设计的机器学习模型、API或任何Python函数构建一个简单的演示系统或Web应用程序。\n",
    "- 用于展示模型的输入和输出，不仅有助于模型开发者更好地理解模型的行为，也为最终用户提供了一个可以更直观地输入数据并查看模型的预测结果。\n",
    "- 可以利用Gradio内置的分享功能在几秒钟内部署Web应用程序，且无需JavaScript、CSS或Web托管经验！\n",
    "- **安装**\n",
    "  - 先决条件：Gradio需要Python 3.8或更高版本。\n",
    "  - 推荐使用pip来安装Gradio，它默认包含在Python中。在终端或命令提示符中运行以下命令：\n",
    "```python\n",
    "pip install gradio\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f66dd81-fc44-45d7-9b6a-5e257047d8b7",
   "metadata": {},
   "source": [
    "- 为了代码更易读，在Python程序中导入Gradio时，将其名字简化为gr。这是一个大家应该遵循的约定，便于他人理解代码。\n",
    "- 运行你的代码。\n",
    "  - 如果将Python代码写在一个名为app.py的文件中，那么可以从终端运行python app.py。\n",
    "  - 如果从文件运行，可在浏览器中打开http://localhost:7860。\n",
    "  - 如果上述程序在Jupyter或JupyterLab内运行，演示将嵌入在notebook内。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da8f03a1-3620-48fa-9bb6-01e52b830852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT: You are using gradio version 4.31.5, however version 4.44.1 is available, please upgrade.\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def greet(name, intensity):\n",
    "    return \"Hello, \" + name + \"!\" * int(intensity)\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=greet,\n",
    "    inputs=[\"text\", \"slider\"],\n",
    "    outputs=[\"text\"],\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee4d4bf-6405-4a90-a4a9-90e0bd97fa2e",
   "metadata": {},
   "source": [
    "- 理解Interface类\n",
    "  - 通过创建了一个gr.Interface类实例可以创建演示。\n",
    "  - Interface类旨在为接受一个或多个输入并返回一个或多个输出的机器学习模型创建演示。\n",
    "- Interface类有三个核心参数：\n",
    "  - fn：要围绕用户界面(UI)包装的函数，此参数非常灵活--可以传递任何的想用UI包装Python函数。\n",
    "  - inputs：用于输入的Gradio组件。组件数量应与函数的参数数量相匹配。\n",
    "  - outputs：用于输出的Gradio组件。组件数量应与函数的返回值数量相匹配。\n",
    "  - input和output参数接受一个或多个Gradio组件。\n",
    "    - Gradio包括30多个内置组件（如gr.Textbox()、gr.Image()和gr.HTML()组件），这些组件专为机器学习应用程序设计。\n",
    "    - 对于inputs和outputs参数，你可以将这些组件的名称作为字符串(\"textbox\")传递，或者传递类的实例(gr.Textbox())。\n",
    "    - 如果编写的函数接受不止一个参数，如上文所述，向inputs传递输入组件的列表，并使每个输入组件对应于函数的一个参数。\n",
    "    - 如果函数返回不止一个值：简单地传递输出组件的列表给outputs。\n",
    "- 上述各种灵活性使得Interface类功能更加强大。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e3c92d-df1c-4413-96f5-bb1f298a45b0",
   "metadata": {},
   "source": [
    "- Gradio在`launch()`中设置`share=True`，就会为演示创建一个公共可访问的URL。\n",
    "- 将上述代码做一点修改，会生成公共URL，比如 https://376ea772521e6fd04e.gradio.live。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5585af5-f531-42cc-8967-239fd8662949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "IMPORTANT: You are using gradio version 4.31.5, however version 4.44.1 is available, please upgrade.\n",
      "--------\n",
      "Running on public URL: https://51f692adabedf0fd05.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://51f692adabedf0fd05.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def greet(name):\n",
    "    return \"Hello \" + name + \"!\"\n",
    "\n",
    "demo = gr.Interface(fn=greet, inputs=\"textbox\", outputs=\"textbox\")\n",
    "\n",
    "demo.launch(share=True)  # 通过一个额外的参数分，分享你的演示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096896c9-277b-4951-946c-08942a98351b",
   "metadata": {},
   "source": [
    "#### 16.2.1.2 使用Gradio部署基于LLM的聊天机器人\n",
    "\n",
    "- Gradio还包括另一个高级类，gr.ChatInterface，它专门设计用来创建聊天机器人UI。\n",
    "  - 该类与Interface类似，用户提供一个函数，Gradio就会创建一个完整的聊天机器人UI。\n",
    "- 下面是ChatInterface的基本用法："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd899528-0b24-4822-80fa-c33f87e7591d",
   "metadata": {},
   "source": [
    "-  **（1）导入Gradio**：首先在你的Python脚本中导入gradio库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6704975-5b67-4cc2-8623-93eec6583f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87df2940-d526-495b-81eb-53fc507d122f",
   "metadata": {},
   "source": [
    "- **（2）定义回复函数**：需要一个函数，`ChatInterface`将使用它来响应用户输入。例如，这可以是一个接受字符串并返回字符串的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d29fab04-11f2-4b7b-a225-d5ecd2d79b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "def echo(message, history):\n",
    "    return '你好， ' + message[\"text\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c8f3e6-2311-4622-92e3-6ebfda6ccf1b",
   "metadata": {},
   "source": [
    "  -  **（3）创建ChatInterface**：接下来，通过将回复函数传递给参数fn，然后定义对话所需的其他参数来创建`ChatInterface`的实例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7087470-b664-4716-9398-66b9aee16ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = gr.ChatInterface(\n",
    "    fn=echo,\n",
    "    examples=[{\"text\": \"张三\"}, {\"text\": \"李四\"}, {\"text\": \"王五\"}],\n",
    "    title=\"聊天机器人\",\n",
    "    multimodal=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c6a3d2-ecc8-420b-8316-7cbb2e953afe",
   "metadata": {},
   "source": [
    "  - **（4）启动接口**：最后启动聊天接口。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80695327-7977-4c4e-9331-4f24804f570d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7866\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e53fdb-f674-436d-a7cd-6f476da4ee4d",
   "metadata": {},
   "source": [
    "- `ChatInterface`可以连接到更复杂的机器学习模型，这些模型处理用户的输入，根据上下文提供响应，并进行更动态的对话。\n",
    "- 下面是使用Gradio部署开源大语言模型的代码示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f20b82-d2c0-4566-833d-8682c539f6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import gradio as gr\n",
    "# 模型保存路径，模型权重文件和配置文件保存在本程序代码上层目录huggingface中\n",
    "model_path = \"../huggingface/Meta-Llama3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "# 加载模型的权重文件，然后将参数从float32转换成float16（half函数）\n",
    "# 然后使用cuda函数将模型迁移到GPU内存\n",
    "model = AutoModel.from_pretrained(model_path, trust_remote_code=True).half().cuda()\n",
    "# 将模型转换成评估状态\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf44ef1-1db7-46f9-9e9f-fa036c684775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建基于LLM的对话函数\n",
    "response, _ = model.chat(tokenizer, '你好', history=[])\n",
    "def chat(name,history):\n",
    "    response, _ = model.chat(tokenizer, name, history=[])    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00f2d35-a074-411b-90f6-79f69ade9a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造对话框，显示复制按钮\n",
    "chatbot=gr.Chatbot(show_copy_button=True)\n",
    "# 对话框的标签\n",
    "chatbot.label=\"问答机器人\"\n",
    "# 构造输入框以及输入框提示\n",
    "textbox = gr.Textbox(placeholder=\"在此处输入问题...\", autofocus=True, scale=40)\n",
    "# 构造对话界面\n",
    "gr.ChatInterface(fn=chat, title=\"人工智能对话系统(测试版)\",chatbot=chatbot,textbox =textbox,\n",
    "                 submit_btn=\"提交\",stop_btn=\"停止\", retry_btn=\"🔄 重试\",\n",
    "                undo_btn=\"↩️ 撤销最后一个对话\",clear_btn=\"🗑️ 清除历史对话\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96d9276-39ed-48f0-ba4f-094f04eea993",
   "metadata": {},
   "source": [
    "### 16.2.2 Ollama与Open WebUI部署开源大语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ddd0d5-fa04-4d58-886f-6f9a858bf49c",
   "metadata": {},
   "source": [
    "#### 16.2.2.1 Ollama与Open WebUI简介\n",
    "- Ollama作为一个本地大语言模型运行框架，具有简明易用、高效稳定等特点，本节将详细介绍其底层技术、联网步骤以及实例解析。\n",
    "- 其功能和特点如下：\n",
    "  - 简化部署：Ollama旨在简化在Docker容器中部署LLM的过程，使管理和运行这些模型更加便捷。\n",
    "  - 捆绑模型组件：该框架将模型权重、配置和数据捆绑到一个称为Modelfile的包中，这有助于优化设置和配置细节，包括GPU的使用情况。\n",
    "  - 支持多种模型：Ollama支持多种大型语言模型，例如Llama 2、Code Llama、Mistral、Gemma等，并且允许用户根据具体需求定制和创建自己的模型。\n",
    "  - 跨平台支持：Ollama支持macOS和Linux平台，同时Windows平台的预览版也已发布。用户只需访问Ollama的官方网站下载对应平台的安装包即可进行安装。\n",
    "  - 命令行操作：安装完成后，用户可以通过简单的命令行操作来启动和运行大型语言模型。例如，要运行llama3 8B模型，只需执行命令```ollama run llama3```,如果不显式指定模型大小，默认是8B。\n",
    "  - 资源要求：为了流畅运行大型模型，Ollama需要一定的内存或显存。具体来说，至少需要8GB的内存/显存来运行7B模型，至少需要16GB来运行13B模型，而运行34B的模型则至少需要32GB。\n",
    "  - 此外，Ollama还提供了类似OpenAI的简单内容生成接口和类似ChatGPT的聊天界面，无需开发即可直接与模型进行交互。它还支持热切换模型，即可以在不重新启动的情况下切换不同的模型，非常灵活多变。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc08b86-87d9-4d46-8920-4c725dbd8f6e",
   "metadata": {},
   "source": [
    "- **Ollama支持的开源大语言模型**\n",
    "\n",
    "| Model              | Parameters | Size  | Download                       |\n",
    "| ------------------ | ---------- | ----- | ------------------------------ |\n",
    "| Llama 3            | 8B         | 4.7GB | `ollama run llama3`            |\n",
    "| Llama 3            | 70B        | 40GB  | `ollama run llama3:70b`        |\n",
    "| Phi 3 Mini         | 3.8B       | 2.3GB | `ollama run phi3`              |\n",
    "| Phi 3 Medium       | 14B        | 7.9GB | `ollama run phi3:medium`       |\n",
    "| Gemma              | 2B         | 1.4GB | `ollama run gemma:2b`          |\n",
    "| Gemma              | 7B         | 4.8GB | `ollama run gemma:7b`          |\n",
    "| Mistral            | 7B         | 4.1GB | `ollama run mistral`           |\n",
    "| Moondream 2        | 1.4B       | 829MB | `ollama run moondream`         |\n",
    "| Neural Chat        | 7B         | 4.1GB | `ollama run neural-chat`       |\n",
    "| Starling           | 7B         | 4.1GB | `ollama run starling-lm`       |\n",
    "| Code Llama         | 7B         | 3.8GB | `ollama run codellama`         |\n",
    "| Llama 2 Uncensored | 7B         | 3.8GB | `ollama run llama2-uncensored` |\n",
    "| LLaVA              | 7B         | 4.5GB | `ollama run llava`             |\n",
    "| Solar              | 10.7B      | 6.1GB | `ollama run solar`             |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e7a509-a59d-4743-94af-4336d4e1e4fa",
   "metadata": {},
   "source": [
    "- **安装**：\n",
    "  - Windows平台的[下载地址](https://ollama.com/download/OllamaSetup.exe)\n",
    "  - Linux平台下载：```curl -fsSL https://ollama.com/install.sh | sh```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2cdd7b-251b-48f8-9e27-3e360d3ae93b",
   "metadata": {},
   "source": [
    "#### 16.2.2.2 Ollama部署大语言模型的过程\n",
    "\n",
    "- 配置模型下载目录：在安装完成后，需要配置模型下载目录。\n",
    "  - 由于Ollama默认将模型下载到C盘，为了避免C盘空间不足的问题，建议指定一个其他目录作为模型下载目录。\n",
    "  - 可以在系统变量中新建OLLAMA_MODELS变量，并指定一个目录路径作为其值。\n",
    "<center><img src='../img/16_2_19.png' width=400px></center>\n",
    "<center>16.2.1 设置环境变量OLLAMA_MODELS的变量名和值</center>\n",
    "- 启动Ollama服务：\n",
    "  - 配置完成后，可以通过命令行指令启动Ollama服务。在命令行中输入ollama serve命令即可启动服务。\n",
    "  - 此时，Ollama将开始监听指定的端口，等待外部请求的连接。\n",
    "\n",
    "- 连接大模型：\n",
    "  - 当Ollama服务启动后，就可以通过HTTP请求连接到大模型了。\n",
    "  - 可以在自己的应用程序中编写代码，使用HTTP请求调用Ollama提供的API接口，从而实现对大模型的调用和控制。\n",
    "  - 可以使用Python的requests库发送HTTP请求，调用Ollama的推理接口，实现对大模型的推理操作。\n",
    "- **示例**："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f19054-3c2f-456a-995c-95ef5d32ec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'http://localhost:8080/model/inference'\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "data = {'input': 'your input data'}\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60458ef7-b6cb-4049-ad74-6051ac2de108",
   "metadata": {},
   "source": [
    "- 上述代码中，url变量指定了Ollama推理接口的URL地址，headers变量指定了请求头信息，data变量指定了推理请求的数据。通过requests.post()方法发送POST请求，并打印出响应的JSON结果。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
